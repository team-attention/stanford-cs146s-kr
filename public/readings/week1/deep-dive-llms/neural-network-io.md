---
title: "4. Neural Network I/O"
titleKr: "4. 신경망 입출력"
chapter: 4
timestamp: "14:27"
sourceUrl: "https://www.youtube.com/watch?v=7xTGNNLPyMI&t=867s"
translatedAt: "2026-01-10"
---

# 4. 신경망 입출력

[영상 바로가기 (14:27)](https://www.youtube.com/watch?v=7xTGNNLPyMI&t=867s)

## 요약

신경망의 입력과 출력 구조를 설명합니다. 토큰 시퀀스(0~8,000개)가 입력으로 들어가면, 신경망은 다음 토큰에 대한 확률 분포(약 100,000개 토큰 각각의 확률)를 출력합니다. 처음에는 무작위 예측을 하지만, 훈련 과정에서 정답 토큰의 확률을 높이고 다른 토큰의 확률을 낮추도록 신경망의 파라미터를 점진적으로 조정합니다.

**핵심 개념:**
- **컨텍스트 윈도우**: 신경망에 입력되는 토큰 시퀀스로, 최대 8,000개 등으로 제한
- **확률 분포 출력**: 어휘의 모든 토큰(100,277개)에 대해 다음에 올 확률을 예측
- **레이블(정답)**: 훈련 데이터에서 실제로 다음에 오는 토큰
- **파라미터 업데이트**: 정답 토큰의 확률을 높이도록 신경망을 수학적으로 조정
- **배치 학습**: 여러 토큰 윈도우를 동시에 처리하여 효율적으로 학습

---

## 전체 번역

데이터셋에 있는 이 텍스트 시퀀스를 토크나이저를 사용해서 토큰 시퀀스로 다시 표현한 것입니다. 이게 그 모습입니다. 예를 들어 FineWeb 데이터셋으로 돌아가면, 이것이 44테라바이트의 디스크 공간일 뿐만 아니라 이 데이터셋에는 약 15조 개의 토큰 시퀀스가 있다고 언급합니다. 여기 이것들은 이 데이터셋의 처음 수천 개 정도의 토큰이지만 15조 개가 있다는 것을 기억하세요. 그리고 다시 한번 기억하세요, 이것들 모두 작은 텍스트 청크를 나타냅니다. 이 시퀀스들의 원자 같은 것이고, 여기 숫자들은 아무 의미가 없습니다. 그냥 고유 ID입니다.

자, 이제 재미있는 부분인 신경망 훈련에 들어갑니다. 이 신경망을 훈련할 때 계산적으로 많은 무거운 작업이 일어나는 곳입니다. 이 단계에서 우리가 하는 것은 이 토큰들이 시퀀스에서 서로 어떻게 따라오는지의 통계적 관계를 모델링하는 것입니다. 우리가 하는 것은 데이터에 들어가서 토큰의 윈도우를 취하는 것입니다. 데이터셋에서 상당히 무작위로 토큰의 윈도우를 취하고, 윈도우 길이는 실제로 0개 토큰에서 우리가 결정한 최대 크기까지 어디든 될 수 있습니다. 예를 들어 실제로는 8,000 토큰의 윈도우를 볼 수 있습니다. 원칙적으로 임의의 윈도우 길이를 사용할 수 있지만, 매우 긴 윈도우 시퀀스를 처리하는 것은 계산적으로 매우 비쌀 것이므로 8,000이 좋은 숫자라고 결정하거나 4,000이나 16,000으로 정하고 거기서 자릅니다.

이 예시에서는 모든 것이 잘 맞도록 처음 네 개의 토큰을 취하겠습니다. 이 토큰들, 네 개의 토큰 윈도우를 취할 건데 "bar view in" 그리고 "single"이고 이것이 이 토큰 ID들입니다. 우리가 하려는 것은 기본적으로 시퀀스에서 다음에 오는 토큰을 예측하는 것입니다. 3962가 다음에 옵니다. 이제 여기서 하는 것은 이 네 개의 토큰을 컨텍스트라고 부르고 신경망에 입력하는 것입니다. 이것이 신경망의 입력입니다. 잠시 후에 이 신경망 내부에 무엇이 있는지 자세히 들어가겠지만, 지금 중요한 것은 신경망의 입력과 출력을 이해하는 것입니다. 입력은 가변 길이의 토큰 시퀀스로, 0에서 8,000 같은 최대 크기까지입니다. 출력은 다음에 무엇이 오는지에 대한 예측입니다. 우리 어휘에 100,277개의 가능한 토큰이 있으므로, 신경망은 정확히 그만큼의 숫자를 출력할 것이고, 그 모든 숫자는 그 토큰이 시퀀스에서 다음으로 올 확률에 해당합니다. 다음에 무엇이 오는지 추측하는 것입니다.

처음에 이 신경망은 무작위로 초기화되어 있고, 잠시 후에 그게 무엇을 의미하는지 보겠지만, 무작위 변환입니다. 그래서 훈련 초기에 이 확률들도 일종의 무작위입니다. 여기 세 가지 예시가 있지만 100,000개의 숫자가 있다는 것을 기억하세요. "direction"이라는 토큰의 확률을 신경망이 4%로 말하고 있습니다. 11799는 2%이고, 여기 "post"인 3962의 확률은 3%입니다. 물론 우리는 데이터셋에서 이 윈도우를 샘플링했으므로 다음에 무엇이 오는지 알고 있습니다. 이것이 레이블입니다. 정답이 실제로 시퀀스에서 3962가 다음에 온다는 것을 알고 있습니다. 이제 우리에게는 신경망을 업데이트하기 위한 이 수학적 과정이 있습니다.

조정하는 방법이 있고, 조금 자세히 들어가겠지만 기본적으로 여기 3%인 이 확률이 더 높아지기를 원하고 다른 모든 토큰의 확률은 더 낮아지기를 원한다는 것을 알고 있습니다. 그래서 정답이 약간 더 높은 확률을 갖도록 신경망을 조정하고 업데이트하는 방법을 수학적으로 계산하는 방법이 있습니다. 신경망을 업데이트하면 이제 다음번에 이 특정 네 개의 토큰 시퀀스를 신경망에 입력하면 신경망이 이제 약간 조정되어서 "post가 아마 4%이고, case가 이제 아마 1%이고, direction이 2%가 될 수 있다" 같은 식으로 말할 것입니다. 기본적으로 시퀀스에서 다음에 오는 정확한 토큰에 더 높은 확률을 부여하도록 신경망을 살짝 업데이트하는 방법이 있습니다.

이제 이 과정이 이 토큰 하나만이 아니라 전체 데이터셋의 이 모든 토큰들에서 동시에 일어난다는 것을 기억해야 합니다. 네 개가 입력되고 이것을 예측한 곳뿐만 아니라요. 실제로 우리는 작은 윈도우들, 작은 윈도우 배치를 샘플링하고, 이 토큰들 각각에서 우리는 그 토큰의 확률이 약간 더 높아지도록 신경망을 조정하려고 합니다. 이 모든 것이 이 토큰들의 큰 배치에서 병렬로 일어나고, 이것이 신경망을 훈련하는 과정입니다. 예측이 훈련 세트에서 실제로 일어나는 것의 통계와 일치하도록 업데이트하는 시퀀스이고, 확률이 데이터에서 이 토큰들이 서로 따라오는 통계적 패턴과 일관되게 됩니다.

이제 이 신경망들의 내부를 간단히 살펴봐서 안에 무엇이 있는지 감을 드리겠습니다.

---
