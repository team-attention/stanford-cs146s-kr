---
title: "23. Where to Find LLMs"
titleKr: "23. LLM을 찾을 수 있는 곳"
chapter: 23
timestamp: "3:18:34"
sourceUrl: "https://www.youtube.com/watch?v=7xTGNNLPyMI&t=11914s"
translatedAt: "2026-01-10"
---

# 23. LLM을 찾을 수 있는 곳

[영상 바로가기 (3:18:34)](https://www.youtube.com/watch?v=7xTGNNLPyMI&t=11914s)

## 요약

LLM을 실제로 사용할 수 있는 다양한 플랫폼을 소개합니다. OpenAI(ChatGPT), Google(Gemini), Anthropic(Claude)은 각자의 웹사이트에서 독점 모델을 제공합니다. 오픈 웨이트 모델의 경우 together.ai 같은 추론 제공자를 통해 접근하거나, LM Studio를 사용해 로컬에서 실행할 수 있습니다. 기본 모델(Base Model)을 사용하고 싶다면 hyperbolic이 좋은 선택입니다.

**핵심 개념:**
- **독점 모델 플랫폼**: chat.openai.com(ChatGPT), gemini.google.com(Gemini), claude.ai(Claude)
- **추론 제공자**: together.ai 등 오픈 웨이트 모델을 호스팅하는 서비스
- **기본 모델 호스팅**: hyperbolic이 llama 3.1 base 등 SFT 이전의 기본 모델 제공
- **로컬 실행**: LM Studio를 사용해 맥북 등에서 소형 모델을 직접 실행 가능
- **증류 모델(Distilled)**: 대형 모델을 압축한 소형 버전으로, 낮은 정밀도로 로컬 실행 가능

---

## 전체 번역

마지막으로 모델을 어디서 찾을 수 있고 어디서 사용할 수 있는지 몇 마디 하겠습니다. 첫째로, 가장 큰 독점 모델의 경우 그 LLM 제공자의 웹사이트에 가야 합니다. 예를 들어 OpenAI의 경우 chat.openai.com입니다. 이제 작동하는 것 같습니다. 제미니의 경우 gemini.google.com이나 AI Studio입니다. 그들이 두 개를 가지고 있는데, 완전히 이해하지 못하는 이유로요. 아무도 이해하지 못합니다. 딥시크 같은 오픈 웨이트 모델의 경우 어떤 종류의 LLM 추론 제공자에 가야 합니다.

제가 좋아하는 것은 together.ai입니다. together.ai의 플레이그라운드에 가면 많은 다른 모델을 선택할 수 있고, 이것들은 모두 다른 유형의 오픈 모델이고, 여기서 대화할 수 있습니다. 기본 모델을 사용하고 싶다면, 이 추론 제공자에서도 기본 모델을 찾는 것이 흔하지 않습니다. 모두 어시스턴트와 채팅을 대상으로 합니다. 여기서도 기본 모델을 볼 수 없었습니다. 기본 모델의 경우 보통 hyperbolic에 갑니다. 그들이 llama 3.1 base를 서비스하고, 그 모델을 좋아합니다.

여기서 대화할 수 있습니다. 제가 아는 한 이것이 기본 모델을 위한 좋은 곳이고, 더 많은 사람들이 기본 모델을 호스팅했으면 합니다. 일부 경우에 작업하기에 유용하고 흥미롭기 때문입니다. 마지막으로 일부 더 작은 모델을 가져와서 로컬에서 실행할 수도 있습니다. 예를 들어 딥시크의 가장 큰 모델은 맥북에서 로컬로 실행할 수 없지만, 증류된 딥시크 모델의 더 작은 버전이 있습니다. 그리고 이 모델들을 더 낮은 정밀도에서 실행할 수도 있습니다.

딥시크의 네이티브 정밀도인 fp8이나 llama의 bf16이 아니라 훨씬 더 낮게요. 그 세부 사항을 완전히 이해하지 못해도 괜찮지만, 증류된 더 작은 버전을 훨씬 더 낮은 정밀도에서 실행할 수 있고, 컴퓨터에 맞출 수 있습니다. 실제로 노트북에서 꽤 괜찮은 모델을 실행할 수 있습니다. 제가 보통 가는 곳은 LM Studio인데, 기본적으로 얻을 수 있는 앱입니다. 정말 못생겼고, 기본적으로 유용하지 않은 이 모든 모델을 보여주는 것이 마음에 들지 않습니다.

모두가 딥시크를 실행하고 싶은데, 왜 500가지 다른 유형의 모델을 주는지 모르겠습니다. 검색하기 정말 복잡하고, 다른 증류와 다른 정밀도를 선택해야 하고, 모두 정말 혼란스럽습니다. 하지만 실제로 어떻게 작동하는지 이해하면, 그것은 별개의 비디오이지만, 모델을 로드할 수 있습니다. 여기서 llama 3 2 instruct 10억을 로드했고, 대화할 수 있습니다. 펠리컨 농담을 요청하고 다른 것을 요청할 수 있고, 등등. 여기서 일어나는 모든 것은 컴퓨터에서 로컬로 일어납니다.

실제로 다른 곳에 가는 것이 아니고, 맥북 프로의 GPU에서 실행되고 있습니다. 매우 좋고, 끝나면 모델을 이젝트하면 RAM이 해제됩니다. LM Studio가 아마 제가 좋아하는 것인데, UI/UX 문제가 많고 거의 전문가를 대상으로 한다고 생각하지만, 유튜브에서 비디오를 보면 이 인터페이스를 사용하는 방법을 알아낼 수 있다고 생각합니다. 이것들이 모델을 어디서 찾을 수 있는지에 대한 몇 마디입니다.

---
