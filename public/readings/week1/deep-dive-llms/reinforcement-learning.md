---
title: "17. Reinforcement Learning"
titleKr: "17. 강화학습"
chapter: 17
timestamp: "2:14:42"
sourceUrl: "https://www.youtube.com/watch?v=7xTGNNLPyMI&t=8082s"
translatedAt: "2026-01-10"
---

# 17. 강화학습

[영상 바로가기 (2:14:42)](https://www.youtube.com/watch?v=7xTGNNLPyMI&t=8082s)

## 요약

강화학습(RL)은 LLM 훈련의 세 번째이자 가장 혁신적인 단계입니다. 사람이 정답을 알려주는 대신, 모델이 스스로 다양한 풀이를 시도하고 정답에 도달하는 방법을 발견합니다. 수학이나 코딩처럼 정답을 자동으로 검증할 수 있는 영역에서 특히 강력하며, 인간 전문가를 모방하는 것만으로는 도달할 수 없는 수준의 성능을 달성할 수 있습니다.

**핵심 개념:**
- **시행착오 학습**: 하나의 문제에 수천 개의 풀이를 시도하고, 정답에 도달한 풀이를 강화하는 방식
- **보상 함수(Reward Function)**: 풀이가 얼마나 좋은지 점수를 매기는 함수로, 검증 가능한 영역에서는 자동화 가능
- **자기 발견(Self-Discovery)**: 모델이 인간이 알려주지 않은 문제 해결 전략을 스스로 발견
- **SFT의 한계**: 지도학습 미세조정은 인간 전문가를 모방할 뿐, 인간의 인지 방식이 LLM에게 최적이 아닐 수 있음
- **토큰 효율성**: 인간에게 쉬운 도약이 LLM에게는 어려울 수 있고, 그 반대도 마찬가지

---

## 전체 번역

1차원 토큰 시퀀스입니다. 저는 사실 이 관점을 더 좋아하는데, 이게 LLM의 본래 관점이기 때문입니다. LLM이 실제로 보는 것은 토큰 ID입니다. 자, 에밀리는 사과 3개와 오렌지 2개를 삽니다. 각 오렌지는 2달러이고, 모든 과일의 총 가격은 13달러입니다. 사과 하나의 가격은 얼마일까요? 여기서 이해해 주셨으면 하는 것이 있습니다. 이것들은 예시로 든 네 가지 후보 풀이인데, 모두 정답 3에 도달합니다. 여기서 이해해 주셨으면 하는 점은, 제가 훈련 세트에 넣을 대화를 만드는 사람 데이터 라벨러라면, 이 대화들 중 어떤 것을 데이터셋에 추가해야 할지 실제로 잘 모른다는 것입니다. 어떤 풀이는 연립방정식을 세우고, 어떤 것은 영어로 설명하며, 어떤 것은 바로 답으로 건너뜁니다. ChatGPT를 보면 이 문제를 주면 변수들의 시스템을 정의하고 이런 작은 작업을 합니다.

하지만 구분해야 할 것이 있습니다. 풀이의 첫 번째 목적은 당연히 정답에 도달하는 것입니다. 최종 답 3을 얻는 것이 중요한 목적이죠. 하지만 두 번째 목적도 있는데, 사람을 위해 보기 좋게 만드는 것입니다. 사람이 풀이를 보고 싶어한다고 가정하고, 중간 단계를 보여주며, 보기 좋게 제시하려는 것이죠. 여기서 두 가지 별개의 일이 일어납니다. 첫째는 사람을 위한 표현이고, 둘째는 실제로 정답을 얻으려는 것입니다.

일단 최종 답에 도달하는 것에만 집중해 봅시다. 최종 답만 중요하다면, 이 중 어떤 것이 최적의 풀이일까요? 제가 말하고 싶은 것은, 우리는 모른다는 것입니다. 사람 라벨러로서 저는 어떤 것이 최선인지 모릅니다. 예를 들어, 앞서 토큰 시퀀스와 암산, 추론을 볼 때, 각 토큰에 대해 기본적으로 유한하고 그리 크지 않은 양의 연산만 할 수 있다는 것을 봤습니다. 하나의 토큰에서 너무 큰 도약을 할 수 없다는 것이죠. 예를 들어, 이 풀이의 좋은 점은 토큰이 매우 적어서 정답에 빨리 도달한다는 것입니다. 하지만 30 - 4를 3으로 나눌 때, 이 단일 토큰에서 많은 연산을 요구하고 있습니다. 그래서 이 예시는 LLM에게 나쁠 수 있는데, 계산을 빨리 건너뛰도록 유도하여 암산에서 실수를 하게 만들 수 있기 때문입니다. 더 펼쳐서 쓰는 게 나을 수도 있고, 방정식으로 세우는 게 나을 수도 있고, 설명하며 푸는 게 나을 수도 있습니다.

근본적으로 우리는 모릅니다. 왜냐하면 사람 라벨러인 우리에게 쉽거나 어려운 것이 LLM에게 쉽거나 어려운 것과 다르기 때문입니다. 인지 방식이 다릅니다. 저에게 사소한 토큰 시퀀스가 LLM에게는 너무 큰 도약일 수 있습니다. 반대로, 제가 만드는 많은 토큰이 LLM에게는 사소해서 토큰을 낭비하는 것일 수 있습니다. 최종 답만 중요하고 사람을 위한 표현을 분리한다면, 우리는 어떤 풀이를 LLM에게 줘야 할지 모릅니다. 우리는 LLM이 아니기 때문입니다. 수학 예시에서는 명확하지만, 이것은 매우 광범위한 문제입니다. 우리의 지식은 LLM의 지식이 아닙니다. LLM은 실제로 수학, 물리학, 화학 등에서 박사 수준의 엄청난 지식을 가지고 있습니다. 많은 면에서 저보다 더 많이 알고 있어서, 그 지식을 문제 해결에 활용하지 못할 수 있습니다.

반대로, 제가 풀이에 LLM의 매개변수에 없는 지식을 넣으면, 그것은 모델에게 혼란스러운 갑작스러운 도약이 됩니다. 우리의 인지 방식이 다르고, 최종 답을 효율적으로 얻는 것만 중요하다면 무엇을 넣어야 할지 모릅니다. 결론적으로, 우리는 LLM을 위한 토큰 시퀀스를 만들기에 좋은 위치에 있지 않습니다. 모방에 의해 시스템을 초기화하는 데는 유용하지만, 실제로는 LLM이 자신에게 맞는 토큰 시퀀스를 발견하기를 원합니다. 프롬프트가 주어졌을 때 신뢰할 수 있게 정답에 도달하는 토큰 시퀀스를 스스로 찾아야 하며, 이를 강화학습과 시행착오 과정에서 발견해야 합니다.

이 예시가 강화학습에서 어떻게 작동하는지 봅시다. 허깅페이스 추론 플레이그라운드로 돌아왔습니다. 여기서 다양한 모델을 쉽게 호출할 수 있습니다. 예를 들어, 오른쪽 상단에서 Gemma 2 20억 매개변수 모델을 선택했습니다. 20억은 매우 작은 모델이지만 괜찮습니다. 강화학습이 기본적으로 어떻게 작동하는지 봅시다. 실제로 꽤 간단합니다. 다양한 풀이를 시도하고 어떤 풀이가 잘 되는지 안 되는지 봐야 합니다. 프롬프트를 가져와서 모델을 실행하고, 모델이 풀이를 생성합니다. 그런 다음 풀이를 검사하는데, 이 문제의 정답이 3달러라는 것을 압니다. 실제로 모델이 맞췄고, 3달러라고 했습니다. 이것이 한 번의 풀이 시도입니다.

이제 이것을 삭제하고 다시 실행해 봅시다. 두 번째 시도입니다. 모델이 약간 다른 방식으로 풀었습니다. 매번 시도마다 다른 생성이 됩니다. 이 모델들은 확률적 시스템이라는 것을 기억하세요. 매 토큰마다 확률 분포가 있고 그로부터 샘플링합니다. 그래서 약간 다른 경로로 가게 됩니다. 이것도 정답에 도달하는 두 번째 풀이입니다. 삭제하고 세 번째로 가봅시다. 역시 약간 다른 풀이지만 정답을 맞췄습니다. 이것을 여러 번 반복할 수 있고, 실제로는 하나의 프롬프트에 대해 수천 또는 수백만 개의 독립적인 풀이를 샘플링할 수 있습니다. 일부는 정답이고 일부는 아닐 것입니다. 기본적으로 하고 싶은 것은 정답에 도달하는 풀이를 장려하는 것입니다.

어떻게 생겼는지 봅시다. 여기 간단한 다이어그램이 있습니다. 프롬프트가 있고, 여러 다른 풀이를 병렬로 시도했습니다. 일부 풀이는 잘 되어 정답을 얻었고(녹색), 일부는 잘 되지 않아 정답을 얻지 못했습니다(빨간색). 이 문제는 사실 좋은 예시가 아닌데, 사소한 프롬프트라서 20억 매개변수 모델도 항상 맞추기 때문입니다. 하지만 상상력을 발휘해서 녹색은 좋고 빨간색은 나쁘다고 가정합시다. 15개 풀이 중 4개만 정답을 맞췄습니다.

이제 정답에 도달하는 종류의 풀이를 장려하고 싶습니다. 빨간색 풀이에서 일어난 토큰 시퀀스는 어딘가에서 뭔가 잘못되었고, 좋은 경로가 아니었습니다. 녹색 풀이의 토큰 시퀀스는 잘 되었으니, 이런 프롬프트에서 이런 것을 더 하고 싶습니다. 미래에 이런 행동을 장려하는 방법은 기본적으로 이 시퀀스들을 훈련하는 것입니다. 하지만 이 훈련 시퀀스는 전문 사람 주석자에게서 온 것이 아닙니다. 이것이 정확한 풀이라고 결정한 사람이 없습니다. 이 풀이는 모델 자체에서 왔습니다. 모델이 여기서 연습하고 있습니다. 몇 가지 풀이를 시도했고, 네 개가 효과가 있었고, 이제 모델이 그것들을 훈련합니다.

이것은 학생이 자신의 풀이를 보고 "이게 정말 잘 됐으니, 이런 문제는 이렇게 풀어야겠다"고 하는 것과 같습니다. 이 예시에서 방법론을 조정하는 여러 방법이 있지만, 핵심 아이디어를 전달하기 위해 가장 간단하게, 이 네 개 중 가장 좋은 풀이 하나를 선택한다고 생각해 봅시다. 이것이 최고의 풀이입니다. 정답에 도달했을 뿐 아니라 다른 좋은 속성도 있을 수 있습니다. 가장 짧거나 보기 좋거나 다른 기준이 있을 수 있습니다. 이것을 최고 풀이로 결정하고 훈련하면, 매개변수 업데이트 후 모델이 앞으로 이런 상황에서 이 경로를 택할 확률이 약간 높아집니다.

하지만 많은 다양한 프롬프트를 실행한다는 것을 기억하세요. 수많은 수학과 물리학 문제 등, 수만 개의 프롬프트가 있고 프롬프트당 수천 개의 풀이가 있습니다. 이 과정을 반복하면서 모델은 어떤 토큰 시퀀스가 정답에 도달하는지 스스로 발견합니다. 사람 주석자에게서 오는 것이 아닙니다. 모델이 이 놀이터에서 놀면서 목표를 알고 자신에게 맞는 시퀀스를 발견합니다. 정신적 도약을 하지 않고 신뢰할 수 있게 통계적으로 작동하며 모델의 지식을 완전히 활용하는 시퀀스입니다. 이것이 강화학습 과정입니다. 기본적으로 추측하고 확인하는 것입니다. 다양한 유형의 풀이를 추측하고, 확인하고, 효과가 있었던 것을 미래에 더 합니다.

이전 내용과 관련하여, SFT 모델(지도학습 미세조정 모델)은 모델을 올바른 풀이의 근처로 초기화하는 데 여전히 도움이 됩니다. 풀이를 작성하게 하고, 연립방정식을 세우거나 풀이를 설명하는 것을 이해하게 합니다. 올바른 풀이의 근처로 가게 하지만, 강화학습에서 모든 것이 정교해집니다. 모델에게 맞는 풀이를 발견하고, 정답을 얻고, 장려하면 모델이 시간이 지나면서 더 좋아집니다.

이것이 대규모 언어 모델을 훈련하는 고수준 과정입니다. 요약하면, 아이들을 가르치는 방식과 매우 유사하게 훈련합니다. 유일한 차이점은 아이들은 책의 장을 거치며 각 장 내에서 다양한 훈련 연습을 하지만, AI를 훈련할 때는 단계 유형에 따라 단계별로 합니다. 첫째는 사전학습입니다. 이것은 기본적으로 모든 설명 자료를 읽는 것과 같습니다. 모든 교과서를 동시에 보고 모든 설명을 읽으며 지식 기반을 구축합니다. 둘째는 SFT 단계입니다. 모든 교과서에 걸쳐 인간 전문가의 모든 고정된 풀이, 모든 유형의 예제 풀이를 보는 것입니다. SFT 모델은 전문가를 모방할 수 있지만 맹목적으로 합니다. 통계적으로 전문가 행동을 흉내 내려고 최선을 다합니다.

마지막으로 RL 단계에서 모든 교과서의 연습 문제를 풀고, 그것이 RL 모델을 얻는 방법입니다. 고수준에서 LLM 훈련 방식은 아이들 교육 과정과 매우 동등합니다. 다음으로 말하고 싶은 점은, 처음 두 단계인 사전학습과 지도학습 미세조정은 수년간 있었고 매우 표준적이며 모든 LLM 제공자가 합니다. 마지막 단계인 RL 훈련이 개발 과정에서 훨씬 초기 단계이며 아직 분야에서 표준이 아닙니다. 이 단계는 훨씬 초기이고 미성숙합니다. 그 이유는 이 과정에서 수많은 세부 사항을 건너뛰었기 때문입니다. 고수준 아이디어는 시행착오 학습으로 매우 간단하지만, 최선의 풀이를 선택하는 방법, 얼마나 훈련할지, 프롬프트 분포는 무엇인지, 훈련을 어떻게 설정해야 실제로 작동하는지 등 수많은 세부 사항과 수학적 미묘함이 있습니다. 세부 사항을 맞추는 것이 쉽지 않습니다.

OpenAI와 같은 많은 회사들이 LLM을 위한 강화학습 미세조정을 내부적으로 실험해 왔지만 공개적으로 말하지 않았습니다. 모두 회사 내부에서 이루어졌습니다. 그래서 딥시크의 논문이 최근에 나왔을 때 큰 화제가 되었습니다. 중국의 딥시크 AI라는 회사의 논문으로, LLM을 위한 강화학습 미세조정에 대해 매우 공개적으로 이야기하고, 그것이 LLM에 얼마나 중요한지, 모델에 많은 추론 능력을 어떻게 가져오는지 설명했습니다. 이 논문은 LLM에 RL을 사용하는 것에 대한 대중의 관심을 다시 불러일으켰고, 그들의 결과를 재현하고 이 단계를 실제로 작동시키는 데 필요한 많은 세부 사항을 제공했습니다.

딥시크 R1 논문을 간략히 살펴보고, RL을 언어 모델에 올바르게 적용하면 어떤 일이 일어나는지 봅시다. 먼저 스크롤해서 보여드릴 것은 이 그림 2입니다. 모델이 수학 문제를 푸는 것이 어떻게 개선되는지 보여줍니다. 이것은 수학 문제 풀이 정확도이고, 웹페이지에서 실제로 어떤 종류의 수학 문제가 측정되는지 볼 수 있습니다. 간단한 수학 문제들로, 모델들이 풀어야 하는 문제들입니다. 처음에는 잘 못하지만, 수천 번의 단계로 모델을 업데이트하면서 정확도가 계속 올라갑니다. 모델이 개선되고, 수학 문제를 더 높은 정확도로 풀고 있습니다. 다양한 문제의 대규모 데이터셋에서 시행착오를 통해 수학 문제를 푸는 방법을 발견하고 있습니다.

하지만 정량적 결과보다 더 놀라운 것은 모델이 이 결과를 달성하는 정성적 방법입니다. 스크롤을 내려보면, 흥미로운 그림 중 하나는 최적화 후반에 모델이 응답당 평균 길이가 올라간다는 것입니다. 모델이 더 높은 정확도 결과를 얻기 위해 더 많은 토큰을 사용하는 것 같습니다. 왜 이 풀이들이 매우 길까요?

정성적으로 살펴봅시다. 기본적으로 모델 풀이가 매우 길어지는데, 부분적으로는 이렇게 합니다. 여기 질문이 있고 모델의 답이 있습니다. 모델이 배운 것, 그리고 이것은 최적화의 새로운 속성으로 발견된 것인데, 문제 해결에 좋다는 것을 발견합니다. "잠깐, 잠깐, 잠깐, 이건 아닌 것 같아. 이 단계를 다시 평가해서 올바른 합을 찾아보자"라고 합니다. 모델이 여기서 무엇을 하고 있을까요? 모델은 기본적으로 단계를 재평가하고 있습니다. 정확도를 위해 많은 아이디어를 시도하고, 다른 관점에서 시도하고, 되추적하고, 재구성하고, 백트래킹하는 것이 더 낫다는 것을 배웠습니다. 수학 문제에 대한 문제 해결 과정에서 당신과 제가 하는 많은 것들을 하고 있습니다.

하지만 당신의 머릿속에서 일어나는 것을 재발견하고 있지, 풀이에 적는 것이 아닙니다. 어떤 사람도 이것을 이상적인 어시스턴트 응답에 하드코딩할 수 없습니다. 이것은 강화학습 과정에서만 발견될 수 있습니다. 왜냐하면 여기에 무엇을 넣어야 할지 모르기 때문입니다. 이것이 모델에게 효과가 있고 문제 해결 정확도를 높이는 것으로 밝혀졌습니다. 모델은 당신의 머릿속에서 우리가 Chain of Thought라고 부르는 것을 배우고, 이것은 최적화의 창발적 속성입니다. 그것이 응답 길이를 부풀리고 있지만, 문제 해결 정확도도 높이고 있습니다.

놀라운 것은 기본적으로 모델이 생각하는 방법을 발견하고 있다는 것입니다. 제가 인지 전략이라고 부르는 것을 배우고 있습니다. 문제를 어떻게 조작하고 다른 관점에서 접근하는지, 유추를 끌어오거나 다양한 것들을 어떻게 하는지, 시간이 지나면서 많은 다른 것을 시도하고, 다른 관점에서 결과를 확인하고, 문제를 푸는 방법을 발견합니다. 여기서는 RL에 의해 발견됩니다. 어디에도 하드코딩하지 않고 이것이 최적화에서 나타나는 것을 보는 것은 매우 놀랍습니다. 우리가 준 것은 정답뿐이고, 이것이 그냥 정확하게 풀려고 하는 것에서 나옵니다.

이제 우리가 작업해 온 문제로 돌아가서, 추론 또는 사고 모델이라고 부르는 것이 이 문제를 어떻게 풀지 살펴봅시다. 이것이 우리가 작업해 온 문제이고, ChatGPT 4o에 붙여넣으면 이런 종류의 응답을 얻습니다. 같은 쿼리를 추론 또는 사고 모델, 즉 강화학습으로 훈련된 모델에 주면 어떻게 되는지 봅시다. 이 논문에서 설명한 딥시크 R1 모델은 chat.deepseek.com에서 이용 가능합니다. 이 회사가 호스팅하고 있습니다. Deep think 버튼이 켜져 있는지 확인하세요. R1 모델을 얻을 수 있습니다.

여기에 붙여넣고 실행해 봅시다. 이전에는 기본적으로 SFT 접근법, 지도학습 미세조정 접근법에서 이것을 얻습니다. 전문가 풀이를 흉내 내는 것이죠. 이것이 RL 모델에서 얻는 것입니다. "이것을 풀어보자. 에밀리가 사과 3개와 오렌지 2개를 사고..." 등등. 이것을 읽으면서 이 모델이 생각하고 있다는 느낌을 피할 수 없습니다. 풀이를 추구하고 있고, 3달러일 것이라고 추론하고, "잠깐, 내 계산을 다시 확인해 보자"라고 합니다. 약간 다른 관점에서 시도합니다. "맞아, 다 맞아. 이게 답인 것 같아. 실수가 없는지 보자. 문제에 접근하는 다른 방법이 있나? 방정식을 세워보자. 사과 하나의 가격을 A달러라고 하면..." 등등. "맞아, 같은 답이야. 각 사과는 3달러야. 이게 맞는 것 같아." 그런 다음 사고 과정을 마치고 사람을 위해 깔끔한 풀이를 작성합니다. 이것은 정확성 측면이고, 이것은 표현 측면입니다. 깔끔하게 작성하고 맨 아래에 정답을 박스로 표시합니다.

놀라운 것은 모델의 사고 과정을 얻는다는 것이고, 이것이 강화학습 과정에서 나오는 것입니다. 토큰 시퀀스의 길이를 부풀리는 것입니다. 생각하고 다른 방법을 시도합니다. 이것이 문제 해결에서 더 높은 정확도를 주고, "아하" 순간과 다른 전략, 정답을 확실히 얻는 방법에 대한 아이디어를 보는 곳입니다.

마지막으로 말하고 싶은 점은, 일부 사람들이 chat.deepseek.com에 민감한 데이터를 넣는 것에 대해 조금 긴장합니다. 중국 회사이기 때문입니다. 딥시크 R1은 이 회사가 출시한 모델로, 오픈 소스 모델 또는 오픈 웨이트 모델입니다. 누구나 다운로드하고 사용할 수 있습니다. 전체 모델을 전체 정밀도로 맥북이나 로컬 장치에서 실행할 수는 없습니다. 꽤 큰 모델이기 때문입니다. 하지만 많은 회사들이 전체 모델을 호스팅하고 있습니다. 제가 사용하기 좋아하는 회사 중 하나는 together.ai입니다. together.ai에 가서 가입하고 플레이그라운드에 가면 딥시크 R1을 선택할 수 있고, 다른 많은 종류의 모델도 선택할 수 있습니다. 모두 최첨단 모델입니다. 지금까지 사용해 온 허깅페이스 추론 플레이그라운드와 비슷하지만, together.ai는 보통 모든 최첨단 모델을 호스팅합니다. 딥시크 R1을 선택하고, 기본 설정으로 괜찮을 것입니다. 딥시크가 모델을 출시했기 때문에 여기서 얻는 것은 기본적으로 여기서 얻는 것과 동등해야 합니다. 샘플링의 무작위성 때문에 약간 다른 것을 얻겠지만, 원칙적으로 모델의 성능 면에서는 동일해야 합니다. 같은 것을 정량적으로나 정성적으로 볼 수 있어야 합니다. 하지만 이 모델은 미국 회사에서 오는 것이니까요. 이것이 딥시크와 추론 모델이라는 것입니다.

ChatGPT로 돌아가면, 드롭다운에서 보이는 모델 중 일부, 예를 들어 o1, o3 mini, o3 mini high 등은 "고급 추론 사용"이라고 말합니다. 이것이 의미하는 것은 딥시크 R1과 매우 유사한 기술로 강화학습으로 훈련되었다는 것입니다. OpenAI 직원들의 공식 발표에 따르면요. 이것들은 RL로 훈련된 사고 모델이고, GPT-4o나 GPT-4o mini 같은 모델은 무료 티어에서 얻는 것인데, 대부분 SFT 모델로 생각해야 합니다. RL 모델에서 보는 것처럼 사고를 하지 않습니다. 이 모델들에도 약간의 강화학습이 관여되어 있지만, 잠시 후 그것에 대해 다루겠습니다. 대부분 SFT 모델이라고 생각해야 합니다.

---
