---
title: "2. Pretraining Data (Internet)"
titleKr: "2. 사전학습 데이터 - 인터넷"
chapter: 2
timestamp: "1:00"
sourceUrl: "https://www.youtube.com/watch?v=7xTGNNLPyMI&t=60s"
translatedAt: "2026-01-10"
---

# 2. 사전학습 데이터 - 인터넷

[영상 바로가기 (1:00)](https://www.youtube.com/watch?v=7xTGNNLPyMI&t=60s)

## 요약

LLM 사전학습의 첫 단계인 인터넷 데이터 수집과 처리 과정을 설명합니다. Common Crawl에서 수집된 원시 웹 데이터는 URL 필터링, 텍스트 추출, 언어 필터링, 중복 제거, PII 제거 등 여러 단계를 거쳐 약 44TB의 고품질 텍스트 데이터셋(예: FineWeb)으로 정제됩니다. 이 데이터셋은 신경망이 학습할 "인터넷의 거대한 텍스트 직물"이 됩니다.

**핵심 개념:**
- **Common Crawl**: 2007년부터 인터넷을 수집해온 조직으로, 27억 개 이상의 웹페이지를 색인
- **URL 필터링**: 악성코드, 스팸, 성인 사이트 등 원치 않는 도메인 제거
- **텍스트 추출**: HTML에서 순수 텍스트 콘텐츠만 추출하는 과정
- **언어 필터링**: 특정 언어(예: 영어 65% 이상) 문서만 선별
- **PII 제거**: 개인 식별 정보(주소, 사회보장번호 등) 필터링
- **FineWeb 데이터셋**: Hugging Face가 공개한 약 44TB, 15조 토큰 규모의 대표적 학습 데이터셋

---

## 전체 번역

자, ChatGPT를 만들어 봅시다. 순차적으로 배열된 여러 단계가 있는데, 첫 번째 단계는 사전학습 단계라고 합니다. 사전학습 단계의 첫 번째 과정은 인터넷을 다운로드하고 처리하는 것입니다. 이게 대략 어떤 모습인지 감을 잡으려면 이 URL을 살펴보시길 권합니다. 허깅페이스라는 회사가 FineWeb이라는 데이터셋을 수집하고 만들고 큐레이션했는데, 이 블로그 포스트에서 FineWeb 데이터셋을 어떻게 구축했는지 자세히 설명하고 있습니다.

OpenAI, Anthropic, Google 같은 모든 주요 LLM 제공업체들은 내부적으로 FineWeb 데이터셋과 비슷한 것을 가지고 있을 겁니다. 대략적으로 우리가 달성하려는 것은 이겁니다. 공개적으로 이용 가능한 소스에서 인터넷의 엄청난 양의 텍스트를 얻으려는 거죠. 매우 고품질의 문서를 대량으로 확보하고, 또한 매우 다양한 문서를 원합니다. 왜냐하면 이 모델들 안에 많은 지식을 담고 싶기 때문입니다. 그래서 고품질 문서의 큰 다양성과 많은 양이 필요합니다. 이를 달성하는 것은 상당히 복잡하고, 보시다시피 여러 단계를 거쳐야 잘 수행됩니다. 이 단계들이 어떻게 생겼는지 조금 살펴보겠습니다. 지금은 예를 들어 FineWeb 데이터셋이 실제 프로덕션급 애플리케이션에서 볼 수 있는 것을 상당히 잘 대표하며, 실제로는 약 44테라바이트의 디스크 공간만 차지한다는 점을 말씀드리고 싶습니다. 1테라바이트 USB 스틱은 매우 쉽게 구할 수 있고, 아마 오늘날에는 하나의 하드 드라이브에 거의 들어갈 수 있을 겁니다. 그래서 인터넷이 매우 방대하지만, 결국 그렇게 엄청난 양의 데이터는 아닙니다. 우리는 텍스트를 다루고 있고 공격적으로 필터링하기 때문에 이 예시에서는 약 44테라바이트로 끝납니다.

이 데이터가 어떻게 생겼는지, 그리고 이 단계들이 무엇인지 살펴봅시다. 이런 노력의 출발점이 되고 최종적으로 대부분의 데이터를 기여하는 것은 Common Crawl의 데이터입니다. Common Crawl은 기본적으로 2007년부터 인터넷을 수집해온 조직입니다. 예를 들어 2024년 기준으로 Common Crawl은 27억 개의 웹페이지를 색인했습니다. 인터넷을 돌아다니는 크롤러들이 있고, 기본적으로 몇 개의 시드 웹페이지에서 시작해서 모든 링크를 따라가고, 계속 링크를 따라가면서 모든 정보를 색인하다 보면 시간이 지나면서 인터넷의 엄청난 양의 데이터를 얻게 됩니다. 이것이 보통 이런 노력들의 출발점입니다.

이 Common Crawl 데이터는 상당히 원시적이고 여러 가지 방식으로 필터링됩니다. 여기서 그들은 이 같은 다이어그램을 문서화했는데, 이 단계들에서 어떤 종류의 처리가 일어나는지 조금 설명합니다. 첫 번째는 URL 필터링이라는 것입니다. 이것은 기본적으로 데이터를 얻고 싶지 않은 URL이나 도메인의 목록을 말합니다. 보통 여기에는 악성코드 웹사이트, 스팸 웹사이트, 마케팅 웹사이트, 인종차별 웹사이트, 성인 사이트 같은 것들이 포함됩니다. 데이터셋에 원하지 않기 때문에 이 단계에서 제거되는 다양한 유형의 웹사이트들이 정말 많습니다. 두 번째 부분은 텍스트 추출입니다. 이 웹페이지들이 크롤러에 의해 저장되는 원시 HTML이라는 것을 기억해야 합니다. 여기서 검사를 클릭하면 실제 원시 HTML이 이렇게 생겼습니다. 리스트 같은 마크업이 있고 CSS와 이런 것들이 있다는 걸 알 수 있습니다. 이것은 거의 웹페이지를 위한 컴퓨터 코드인데, 우리가 정말 원하는 것은 이 텍스트입니다. 웹페이지의 텍스트만 원하고 내비게이션 같은 것은 원하지 않습니다. 그래서 이 웹페이지들의 좋은 콘텐츠만 적절히 필터링하기 위한 많은 필터링, 처리, 휴리스틱이 들어갑니다.

다음 단계는 언어 필터링입니다. 예를 들어 FineWeb은 언어 분류기를 사용하여 모든 웹페이지가 어떤 언어인지 추측하고, 예를 들어 65% 이상이 영어인 웹페이지만 유지합니다. 이것이 설계 결정이라는 것을 알 수 있는데, 다른 회사들은 스스로 결정할 수 있습니다. 다양한 유형의 언어를 데이터셋에 어느 정도 포함시킬 것인가를요. 예를 들어 스페인어를 모두 필터링하면 나중에 우리 모델이 스페인어를 잘 못할 수 있습니다. 그 언어의 데이터를 많이 보지 못했기 때문이죠. 그래서 다른 회사들은 다국어 성능에 다른 정도로 집중할 수 있습니다. FineWeb은 영어에 상당히 집중되어 있어서 나중에 훈련하는 언어 모델은 영어는 매우 잘하지만 다른 언어는 그다지 잘 못할 수 있습니다. 언어 필터링 후에는 몇 가지 다른 필터링 단계와 중복 제거 등이 있고, 마지막으로 예를 들어 PII 제거가 있습니다. 이것은 개인 식별 정보로, 예를 들어 주소, 사회보장번호 같은 것을 감지하고 그런 종류의 웹페이지를 데이터셋에서 필터링하려고 합니다.

여기에는 많은 단계가 있고 전체 세부 사항을 다루지는 않겠지만, 전처리의 상당히 광범위한 부분이고 예를 들어 FineWeb 데이터셋으로 끝납니다. 클릭해 들어가면 실제로 어떻게 생겼는지 예시를 볼 수 있고, 누구나 허깅페이스 웹페이지에서 다운로드할 수 있습니다. 훈련 세트에 들어가는 최종 텍스트의 예시들입니다. 이것은 2012년 토네이도에 대한 기사입니다. 2012년에 토네이도가 있었고 무슨 일이 있었는지... 다음 것은 "당신의 몸에 9볼트 배터리 크기의 작은 노란색 부신이 두 개 있다는 걸 알고 계셨나요?" 같은 내용입니다. 이것은 어떤 종류의 이상한 의학 기사죠. 이것들을 기본적으로 다양한 방식으로 텍스트만 필터링된 인터넷의 웹페이지들이라고 생각하세요.

이제 우리는 40테라바이트의 엄청난 양의 텍스트를 가지고 있고, 이것이 다음 단계의 출발점입니다. 지금 우리가 어디에 있는지 직관적인 감각을 드리고 싶었습니다. 여기서 처음 200개의 웹페이지를 가져왔는데, 우리는 엄청나게 많은 웹페이지를 가지고 있다는 것을 기억하세요. 그 모든 텍스트를 그냥 합쳐서 연결했습니다. 이것이 우리가 얻는 것입니다. 그냥 원시 텍스트, 원시 인터넷 텍스트이고 양이 엄청납니다. 200개의 웹페이지만으로도요. 계속 축소해 보면 이런 거대한 텍스트 데이터의 태피스트리가 있습니다. 이 텍스트 데이터에는 이 모든 패턴들이 있고, 우리가 하고 싶은 것은 이 데이터로 신경망을 훈련시켜서 신경망이 이 텍스트가 어떻게 흘러가는지 내재화하고 모델링하게 하는 것입니다. 이 거대한 텍스트 직물이 있고, 이제 이것을 모방하는 신경망을 얻으려고 합니다. 자, 텍스트를 신경망에 넣기 전에 이 텍스트를 어떻게 표현하고 어떻게 입력할지 결정해야 합니다.

---
