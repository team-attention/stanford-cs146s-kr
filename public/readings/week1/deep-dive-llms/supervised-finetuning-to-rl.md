---
title: "16. Supervised Finetuning to RL"
titleKr: "16. SFT에서 강화학습으로"
chapter: 16
timestamp: "2:07:28"
sourceUrl: "https://www.youtube.com/watch?v=7xTGNNLPyMI&t=7648s"
translatedAt: "2026-01-10"
---

# 16. SFT에서 강화학습으로

[영상 바로가기 (2:07:28)](https://www.youtube.com/watch?v=7xTGNNLPyMI&t=7648s)

## 요약

지금까지 사전학습(인터넷 문서로 베이스 모델 생성)과 SFT(대화 데이터로 어시스턴트 생성)의 두 단계를 다뤘습니다. SFT의 핵심은 인간 라벨러가 만든 이상적인 응답을 모방하는 것입니다. 하지만 SFT만으로는 한계가 있습니다. 다음 단계인 강화학습(RL)에서는 모델이 인간 전문가를 단순히 모방하는 것을 넘어, 스스로 더 나은 해결책을 발견하도록 학습합니다. 코드 인터프리터 같은 도구도 환각을 줄이는 데 활용됩니다.

**핵심 개념:**
- **학습 3단계 구조**: 사전학습(인터넷) -> SFT(대화) -> RL(강화학습)의 순서로 모델 발전
- **SFT의 본질**: 인간 라벨러가 만든 이상적인 응답의 통계적 모방
- **SFT의 한계**: 모방만으로는 인간 전문가 수준을 넘어서기 어려움
- **RL의 역할**: 모델이 스스로 탐색하며 더 나은 해결책을 발견하도록 유도
- **도구 활용**: 코드 인터프리터 등으로 환각 감소 및 정확도 향상

---

## 전체 번역

이제 대규모 언어 모델 학습의 두 가지 주요 단계를 다루었습니다. 첫 번째 단계는 사전학습 단계라고 불리고, 기본적으로 인터넷 문서에서 학습합니다. 인터넷 문서에서 언어 모델을 학습시키면 베이스 모델이라고 불리는 것을 얻고, 기본적으로 인터넷 문서 시뮬레이터입니다. 이것이 흥미로운 결과물이고 수천 대의 컴퓨터에서 여러 달 학습이 필요하고, 인터넷의 손실 압축이고, 매우 흥미롭지만 직접적으로 유용하지 않습니다. 인터넷 문서를 샘플링하고 싶지 않고, AI에게 질문하고 우리 질문에 응답하기를 원하기 때문입니다. 그것을 위해 어시스턴트가 필요하고, 후속학습 과정에서 어시스턴트를 실제로 구성할 수 있다는 것을 봤습니다. 특히 지도학습 미세조정이라고 부르는 과정에서요. 이 단계에서 알고리즘적으로 사전학습과 동일합니다. 바뀌는 것은 없습니다. 바뀌는 유일한 것은 데이터셋입니다. 인터넷 문서 대신 이제 대화의 매우 좋은 데이터셋을 만들고 큐레이션하고 싶습니다. 모든 종류의 다양한 주제에 대해 인간과 어시스턴트 사이의 수백만 대화를 원합니다. 근본적으로 이 대화들은 인간이 만듭니다. 인간이 프롬프트를 작성하고 인간이 이상적인 응답을 작성하고, 라벨링 문서를 기반으로 합니다.

현대 스택에서 실제로 완전히 인간이 수동으로 하지 않습니다. 이제 이 도구들에서 많은 도움을 받습니다. 언어 모델을 사용해서 이 데이터셋을 만드는 것을 돕고, 광범위하게 이루어지지만, 근본적으로 모든 것은 여전히 끝에서 인간 큐레이션에서 옵니다. 이 대화들을 만들고, 그것이 데이터셋이 되고, 파인튜닝하거나 계속 학습시키면 어시스턴트를 얻습니다. 그런 다음 기어를 바꿔서 이 어시스턴트가 어떤지에 대한 인지적 함의에 대해 이야기하기 시작했습니다.

어떤 종류의 완화를 취하지 않으면 어시스턴트가 환각을 일으킬 것이라는 것을 봤습니다. 환각이 흔할 것이고, 그 환각의 완화 중 일부를 봤습니다. 모델이 꽤 인상적이고 머릿속에서 많은 것을 할 수 있지만, 더 나아지기 위해 도구에 의존할 수 있다는 것을 봤습니다. 예를 들어 웹 검색에 의존해 환각을 줄이고 아마 더 최신 정보 등을 가져올 수 있습니다. 또는 코드 인터프리터를 사용할 수 있습니다.

---
