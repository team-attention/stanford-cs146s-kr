---
title: "5. Neural Network Internals"
titleKr: "5. 신경망 내부 구조"
chapter: 5
timestamp: "20:11"
sourceUrl: "https://www.youtube.com/watch?v=7xTGNNLPyMI&t=1211s"
translatedAt: "2026-01-10"
---

# 5. 신경망 내부 구조

[영상 바로가기 (20:11)](https://www.youtube.com/watch?v=7xTGNNLPyMI&t=1211s)

## 요약

트랜스포머 신경망의 내부 구조를 설명합니다. 신경망은 수십억 개의 파라미터(가중치)로 구성되며, 이 파라미터들이 입력 토큰과 수학적으로 결합되어 다음 토큰을 예측합니다. 파라미터는 처음에 무작위로 설정되지만, 훈련을 통해 훈련 데이터의 패턴과 일치하는 예측을 하도록 조정됩니다. 트랜스포머는 어텐션, MLP 블록 등의 레이어로 구성된 고정된 수학적 함수입니다.

**핵심 개념:**
- **파라미터(가중치)**: 신경망을 구성하는 조절 가능한 숫자들로, 현대 모델은 수십억~수조 개 보유
- **트랜스포머(Transformer)**: 현대 LLM의 핵심 아키텍처로, 어텐션과 MLP 블록으로 구성
- **어텐션(Attention)**: 입력 토큰들 간의 관계를 학습하는 트랜스포머의 핵심 메커니즘
- **MLP(다층 퍼셉트론)**: 정보를 처리하는 피드포워드 신경망 블록
- **임베딩(Embedding)**: 토큰을 신경망 내부에서 표현하는 벡터
- **고정된 수학적 함수**: 상태나 기억 없이 입력에서 출력으로 변환하는 결정론적 표현식

---

## 전체 번역

신경망 내부입니다. 말씀드렸듯이 토큰 시퀀스인 입력들이 있는데, 이 경우 네 개의 입력 토큰이지만 0에서 8,000 토큰까지 어디든 될 수 있습니다. 원칙적으로 이것은 무한한 수의 토큰이 될 수 있지만, 무한한 수의 토큰을 처리하는 것은 계산적으로 너무 비쌀 것이므로 특정 길이에서 자르고 그것이 그 모델의 최대 컨텍스트 길이가 됩니다. 이 입력 X들은 이 신경망의 파라미터 또는 가중치와 함께 거대한 수학적 표현식에서 혼합됩니다. 여기서 여섯 개의 예시 파라미터와 그 설정을 보여주고 있지만, 실제로 현대 신경망은 수십억 개의 이런 파라미터를 가지고 있고, 처음에 이 파라미터들은 완전히 무작위로 설정됩니다. 무작위 파라미터 설정으로는 이 신경망이 무작위 예측을 할 것이라고 예상할 수 있고, 처음에는 실제로 완전히 무작위 예측입니다.

하지만 네트워크를 반복적으로 업데이트하는 이 과정을 통해, 이것을 신경망 훈련이라고 부르는데, 이 파라미터들의 설정이 조정되어 신경망의 출력이 훈련 세트에서 본 패턴과 일관되게 됩니다. 이 파라미터들을 DJ 세트의 노브들처럼 생각하세요. 이 노브들을 돌리면서 가능한 모든 토큰 시퀀스 입력에 대해 다른 예측을 얻게 됩니다. 신경망을 훈련한다는 것은 훈련 세트의 통계와 일관되어 보이는 파라미터 설정을 발견하는 것을 의미합니다.

이 거대한 수학적 표현식이 어떻게 생겼는지 예를 들어드리겠습니다. 현대 네트워크는 아마 수조 개의 항을 가진 거대한 표현식이지만, 간단한 예를 보여드리겠습니다. 이런 종류의 표현식입니다. 그다지 무섭지 않다는 것을 보여드리려고요. 입력 x가 있고, 이 경우 두 개의 예시 입력 x1, x2가 있고, 네트워크의 가중치 w0, w1, w2, w3 등과 혼합됩니다. 이 혼합은 곱셈, 덧셈, 지수화, 나눗셈 같은 간단한 것들입니다. 효과적인 수학적 표현식을 설계하는 것이 신경망 아키텍처 연구의 주제입니다. 표현력이 있고, 최적화 가능하고, 병렬화 가능하고 등등 편리한 특성을 많이 가진 것을요. 하지만 결국 이것들은 복잡한 표현식이 아니고 기본적으로 입력을 파라미터와 혼합해서 예측을 만들고, 우리는 신경망의 파라미터를 최적화해서 예측이 훈련 세트와 일관되게 나오도록 합니다.

이 신경망들이 실제로 어떻게 생겼는지 프로덕션급 예시를 보여드리고 싶습니다. 이를 위해 이 네트워크 중 하나의 매우 좋은 시각화가 있는 이 웹사이트를 방문하시길 권합니다. 이 웹사이트에서 이것을 찾을 수 있고, 프로덕션 환경에서 사용되는 이 신경망은 이런 특별한 종류의 구조를 가지고 있습니다. 이 네트워크는 트랜스포머라고 불리고, 이 특정 예시는 대략 85,000개의 파라미터를 가지고 있습니다. 상단에서 토큰 시퀀스인 입력을 받고, 정보가 신경망을 통해 흐르고 출력인 logit softmax까지 가는데, 이것이 다음에 어떤 토큰이 오는지에 대한 예측입니다. 여기에 일련의 변환들이 있고 이 수학적 표현식 안에서 생성되는 모든 중간 값들이 다음에 무엇이 오는지 예측하는 것입니다.

예를 들어 이 토큰들은 소위 분산 표현으로 임베딩됩니다. 가능한 모든 토큰은 신경망 내부에서 그것을 나타내는 일종의 벡터를 가지고 있습니다. 먼저 토큰을 임베딩하고, 그 값들이 이 다이어그램을 통해 흐릅니다. 이것들은 개별적으로 모두 매우 간단한 수학적 표현식입니다. 레이어 놈이 있고, 행렬 곱셈이 있고, 소프트맥스 등이 있습니다. 여기가 이 트랜스포머의 어텐션 블록이고, 정보가 다층 퍼셉트론 블록으로 흐르고 등등... 여기 이 모든 숫자들은 표현식의 중간 값들이고, 이것들을 이 합성 뉴런들의 발화율처럼 생각할 수 있습니다. 하지만 뇌에서 찾을 수 있는 뉴런처럼 너무 많이 생각하지 않도록 주의하시길 권합니다. 생물학적 뉴런은 기억 등을 가진 매우 복잡한 동적 과정입니다. 이 표현식에는 기억이 없습니다. 입력에서 출력까지 기억 없이 고정된 수학적 표현식입니다. 상태가 없습니다. 그래서 생물학적 뉴런에 비해 매우 단순한 뉴런입니다. 하지만 여전히 느슨하게 이것을 합성 뇌 조직처럼 생각할 수 있습니다. 그렇게 생각하고 싶다면요. 정보가 이 모든 뉴런들을 통해 흐르고 발화하면서 예측에 도달합니다.

이 모든 변환들의 정확한 수학적 세부 사항에 대해 너무 자세히 다루지는 않겠습니다. 솔직히 그렇게 중요하지 않다고 생각합니다. 정말 중요한 것은 이것이 수학적 함수라는 것을 이해하는 것입니다. 85,000개 같은 고정된 파라미터 집합으로 파라미터화되어 있고, 입력을 출력으로 변환하는 방법이고, 파라미터를 조정하면서 다른 종류의 예측을 얻고, 예측이 훈련 세트에서 본 패턴과 일치하도록 좋은 파라미터 설정을 찾아야 합니다. 이것이 트랜스포머입니다.

신경망의 내부를 보여드렸고 훈련 과정에 대해 조금 이야기했습니다. 이 네트워크를 다루는 또 하나의 주요 단계를 다루고 싶은데, 추론이라는 단계입니다. 추론에서 우리가 하는 것은 모델에서 새로운 데이터를 생성하는 것입니다. 기본적으로 네트워크의 파라미터에 어떤 종류의 패턴을 내재화했는지 보고 싶은 것입니다. 모델에서 생성하는 것은 상대적으로 간단합니다.

---
