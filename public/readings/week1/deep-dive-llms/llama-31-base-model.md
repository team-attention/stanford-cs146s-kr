---
title: "8. Llama 3.1 Base Model Inference"
titleKr: "8. Llama 3.1 베이스 모델 추론"
chapter: 8
timestamp: "42:52"
sourceUrl: "https://www.youtube.com/watch?v=7xTGNNLPyMI&t=2572s"
translatedAt: "2026-01-10"
---

# 8. Llama 3.1 베이스 모델 추론

[영상 바로가기 (42:52)](https://www.youtube.com/watch?v=7xTGNNLPyMI&t=2572s)

## 요약

Meta의 Llama 3.1 베이스 모델을 통해 사전학습만 완료된 모델의 특성을 보여줍니다. 베이스 모델은 "인터넷 텍스트 시뮬레이터"로, 질문에 답하는 어시스턴트가 아니라 인터넷 문서를 완성하는 문서 완성기입니다. 질문을 입력하면 답변 대신 비슷한 질문을 더 생성하거나, 인터넷에서 볼 법한 다른 패턴으로 텍스트를 완성합니다. ChatGPT 같은 대화형 어시스턴트가 되려면 후속 학습(파인튜닝)이 필요합니다.

**핵심 개념:**
- **베이스 모델(Base Model)**: 인터넷 데이터로 사전학습만 완료된 상태의 모델
- **인터넷 텍스트 시뮬레이터**: 베이스 모델의 본질로, 인터넷에서 본 패턴을 모방
- **문서 완성기**: 질문에 답하는 것이 아니라 입력된 텍스트를 자연스럽게 이어가는 역할
- **파인튜닝(Fine-tuning)**: 베이스 모델을 특정 용도(예: 어시스턴트)에 맞게 추가 학습
- **Llama 3.1**: Meta가 공개한 오픈소스 LLM 시리즈

---

## 전체 번역

이런 대규모 모델을 직접 학습시키기에는 비용이 너무 커서, 다행히도 대형 테크 기업들이 훈련한 모델을 공개한 것을 사용할 수 있습니다. 그런데 많은 회사가 모델을 훈련하더라도, “베이스 모델”은 잘 공개하지 않습니다. 최적화가 끝난 직후의 모델이 바로 베이스 모델인데, 아직은 어시스턴트가 아니기 때문입니다.

베이스 모델은 기본적으로 “토큰 시뮬레이터”, 즉 인터넷 텍스트 시뮬레이터입니다. 우리가 원하는 것은 질문에 답하는 어시스턴트이지만, 베이스 모델은 인터넷 문서의 패턴을 재조합해 이어 쓰는 역할을 합니다. 그래서 보통 베이스 모델은 전체 파이프라인의 첫 단계로만 쓰입니다.

과거에는 GPT‑2 1.5B 같은 베이스 모델이 공개된 적이 있습니다. 모델을 공개하려면 두 가지가 필요합니다. 첫째는 모델의 순전파를 정의하는 코드(보통 수백 줄 내외), 둘째는 실제 가치가 담긴 파라미터입니다. GPT‑2는 약 15억 개의 파라미터가 있고, 이 수치들이 있어야 같은 동작을 재현할 수 있습니다.

이제 더 확실한 예로 Llama 3를 보겠습니다. GPT‑2는 100B 토큰으로 학습한 1.5B 모델이었지만, Llama 3는 훨씬 큰 45B 모델이고 15T 토큰으로 학습됐습니다. 이 논문에서 Meta는 Llama 3.1 405B 베이스 모델도 공개했는데, 동시에 어시스턴트용 “인스트럭트 모델”도 공개했습니다. 지금은 베이스 모델에만 집중해 보겠습니다.

제가 베이스 모델을 실험할 때 자주 사용하는 곳은 Hyperbolic입니다. 사이트에서 반드시 Llama 3.1 405B **베이스** 모델을 선택해야 합니다. 최대 토큰 수를 줄여서 계산을 아끼고, 이제 입력 프롬프트를 이어 쓰게 해보겠습니다.

먼저 이 모델은 어시스턴트가 아닙니다. 예를 들어 “2+2가 뭐야?”라고 물어도 대화형 답변이 나오지 않습니다. 질문은 단지 프롬프트의 일부가 되고, 모델은 다음 토큰의 확률 분포를 샘플링해서 이어 쓰는 “아주 비싼 자동완성”일 뿐입니다.

같은 프롬프트를 넣어도 결과는 매번 다릅니다. 이유는 모델이 확률 분포에서 샘플링하기 때문이며, 결과가 확률적으로 달라집니다. 이 모델은 인터넷에서 본 통계 패턴을 재생산하고 있을 뿐입니다.

그럼에도 베이스 모델은 유용합니다. 다음 토큰 예측 과정에서 세상의 많은 지식을 학습했기 때문입니다. 이 지식은 파라미터에 저장되어 있습니다. 405B 파라미터는 인터넷의 압축본처럼 볼 수 있지만, 손실 압축입니다. 따라서 프롬프트를 잘 구성하면 이 지식을 끌어낼 수 있습니다.

예를 들어 “파리에서 꼭 봐야 할 랜드마크 Top 10” 같은 목록을 프롬프트로 주면, 모델이 리스트를 이어서 써 줍니다. 다만 정보는 어디까지나 인터넷 문서의 **희미한 회상**입니다. 자주 등장하는 내용은 그럴듯하지만, 드물게 등장하는 정보는 신뢰하기 어렵습니다.

또 다른 예로, 위키피디아 문장을 입력하면 모델이 문서를 거의 그대로 “복기(regurgitation)”할 수 있습니다. 이는 모델이 고품질 문서를 학습할 때 그 소스를 자주 샘플링했기 때문입니다. 너무 많이 본 문서는 거의 외워버리고, 나중에 그대로 읊을 수 있습니다. 이런 회상은 보통 바람직하지 않습니다.

반대로 모델이 **모르는 미래**에 대해 프롬프트를 주면, 모델은 확률적으로 추측하여 이어 씁니다. 예를 들어 2024년 대선 결과 같은 것은 학습 데이터에 없으므로, 모델은 가능한 시나리오를 만들어 냅니다. 이것이 환각입니다.

그럼에도 베이스 모델은 실제 응용에 쓸 수 있습니다. 예를 들어 몇 개의 영어‑한국어 단어 쌍을 보여주고 “teacher”를 넣으면, 모델이 패턴을 학습해 “선생님” 같은 번역을 이어 씁니다. 이것이 인컨텍스트 러닝입니다.

마지막으로, 프롬프트만으로도 **어시스턴트처럼 보이게** 만들 수 있습니다. “AI 어시스턴트와 인간의 대화”처럼 보이는 대화 형식을 만들어 놓고, 마지막에 질문을 넣으면 모델이 그 역할을 따라 이어 씁니다. 물론 계속해서 인간 질문을 만들어 내기도 하지만, 어시스턴트처럼 동작하는 효과를 얻을 수 있습니다. 베이스 모델로도 어느 정도 목적을 달성할 수 있다는 점을 보여줍니다.

---
