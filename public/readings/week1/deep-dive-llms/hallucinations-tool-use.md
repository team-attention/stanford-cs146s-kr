---
title: "11. Hallucinations, Tool Use, Knowledge/Working Memory"
titleKr: "11. 환각, 도구 사용, 지식/작업 메모리"
chapter: 11
timestamp: "1:20:32"
sourceUrl: "https://www.youtube.com/watch?v=7xTGNNLPyMI&t=4832s"
translatedAt: "2026-01-10"
---

# 11. 환각, 도구 사용, 지식/작업 메모리

[영상 바로가기 (1:20:32)](https://www.youtube.com/watch?v=7xTGNNLPyMI&t=4832s)

## 요약

LLM은 학습 데이터에서 "자신있게 답하는" 패턴을 배웠기 때문에, 모르는 것도 확신있게 답하는 환각(hallucination) 문제가 발생합니다. 이를 완화하기 위해 두 가지 방법이 사용됩니다. 첫째, 모델을 심문하여 무엇을 모르는지 파악하고 "모르겠습니다"라고 답하도록 학습시킵니다. 둘째, 웹 검색이나 코드 실행 같은 도구를 사용하여 외부 정보를 컨텍스트 윈도우(작업 메모리)에 가져와 정확한 답변을 생성합니다.

**핵심 개념:**
- **환각(Hallucination)**: 모델이 모르는 정보를 자신있게 지어내는 현상
- **지식 기반 거부**: 모델이 모르는 것에 대해 "모르겠습니다"라고 답하도록 학습
- **도구 사용(Tool Use)**: 웹 검색, 코드 실행 등 외부 도구를 호출하는 특수 토큰 메커니즘
- **지식 메모리**: 파라미터에 저장된 희미한 기억 (사전학습에서 습득)
- **작업 메모리**: 컨텍스트 윈도우에 있는 직접 접근 가능한 데이터

---

## 전체 번역

이 문제는 몇 년 전 초기 모델들에서 상당히 존재했고, 잠시 후에 설명할 몇 가지 완화 방법이 있어서 문제가 조금 나아졌다고 생각합니다. 지금은 이 환각이 어디서 오는지 이해해봅시다. 여기 학습 세트에 있을 수 있다고 생각할 수 있는 세 가지 대화의 구체적인 예시가 있습니다. 학습 세트에 있을 수 있는 꽤 합리적인 대화입니다. 예를 들어, 톰 크루즈가 누구냐, 톰 크루즈는 유명한 미국 배우이고 프로듀서입니다. 존 바라소가 누구냐, 예를 들어 미국 상원의원입니다. 칭기즈 칸이 누구냐, 칭기즈 칸은 어쩌고저쩌고였습니다. 학습 시 대화가 이렇게 생겼을 수 있습니다. 이것의 문제는 인간이 각 경우에 어시스턴트의 정답을 작성할 때, 인간이 이 사람이 누군지 알거나 인터넷에서 조사하고 와서 이런 확신 있는 톤의 응답을 작성합니다.

테스트 시에 이 사람이 누구냐고 물으면 - 이것은 제가 완전히 무작위로 만든 이름이고, 제가 알기로는 존재하지 않습니다, 무작위로 생성하려 했습니다 - 오슨 코바츠가 누구냐고 물으면 문제는 어시스턴트가 "모르겠습니다"라고 말하지 않을 것입니다. 어시스턴트와 언어 모델 자체가 내부 특성, 활성화, 일종의 두뇌 안에서 이 사람이 익숙하지 않다는 것을 어떤 의미에서 알 수 있더라도, 네트워크의 어떤 부분이 어떤 의미에서 그것을 안다고 해도, "이 사람이 누군지 모르겠습니다"라고 말하는 것은 일어나지 않습니다. 모델이 통계적으로 학습 세트를 모방하기 때문입니다. 학습 세트에서 "누구누구"라는 형식의 질문은 정답으로 확신 있게 답변됩니다. 그래서 답변의 스타일을 취하고 최선을 다할 것입니다. 통계적으로 가장 가능성 높은 추측을 주고, 기본적으로 지어냅니다.

다시 말하지만 방금 이야기했듯이 이 모델들은 인터넷에 접속하지 않고 조사하지 않습니다. 제가 통계적 토큰 텀블러라고 부르는 것입니다. 시퀀스에서 다음 토큰을 샘플링하려고 하고, 기본적으로 지어냅니다. 이것이 어떻게 보이는지 봅시다. 여기 Hugging Face의 추론 플레이그라운드가 있고, 의도적으로 Falcon 7B라는 모델을 선택했습니다. 오래된 모델이고, 몇 년 전입니다. 그래서 환각을 겪고, 언급했듯이 최근에는 시간이 지나면서 개선되었지만, 오슨 코바츠가 누구냐고 Falcon 7B instruct에게 물어봅시다. 아, 네, 오슨 코바츠는 미국 작가이자 SF 작가입니다. 완전히 거짓입니다. 환각이죠. 다시 해봅시다. 통계적 시스템이니까요. 재샘플링할 수 있습니다. 이번에는 오슨 코바츠가 1950년대 TV 쇼의 가상 캐릭터라고 합니다. 완전히 헛소리죠. 다시 해봅시다. 전직 마이너리그 야구 선수라고 합니다. 기본적으로 모델은 모르고, 모르기 때문에 많은 다른 답을 줍니다. 확률에서 샘플링하는 것입니다. 모델은 "오슨 코바츠가 누구냐 어시스턴트"라는 토큰으로 시작하고, 여기로 와서 확률을 얻고 확률에서 샘플링하고 무언가를 만들어냅니다. 그 내용은 학습 세트의 답변 스타일과 통계적으로 일치하고 그냥 그렇게 합니다. 우리는 이것을 지어낸 사실적 지식으로 경험하지만, 모델은 기본적으로 모르고 답변 형식을 모방하고 있으며, 찾아보러 가지 않을 것입니다. 그냥 답변을 모방하기 때문입니다.

그래서 이것을 어떻게 완화할 수 있을까요? 예를 들어 ChatGPT에 가서 오슨 코바츠가 누구냐고 물으면 OpenAI의 최첨단 모델에게 물으면, 이 모델은 실제로 더 똑똑합니다. 잠깐 웹 검색 중이라고 나왔습니다. 나중에 다룰 것인데, 실제로 도구 사용을 하려고 하고 어떤 이야기를 만들어냈지만, "오슨 코바츠 도구 사용 안 함"이라고 말하고 싶습니다. 웹 검색을 원하지 않습니다. 오슨 코바츠라는 잘 알려진 역사적 또는 공인이 없어서 이 모델은 지어내지 않을 것입니다. 이 모델은 모른다는 것을 알고 말해줍니다. 이 모델이 아는 사람이 아닌 것 같습니다. 그래서 우리는 어떻게든 환각을 개선했습니다. 오래된 모델에서 분명히 문제였지만, 학습 세트가 이렇게 생겼다면 이런 종류의 답을 얻는 것이 완전히 이해됩니다.

그래서 이것을 어떻게 고칠까요? 분명히 데이터셋에 어시스턴트의 정답이 모델이 모른다는 것인 예시가 필요합니다. 하지만 그 답이 모델이 실제로 모르는 경우에만 나오도록 해야 합니다. 그래서 질문은 모델이 무엇을 알고 모르는지 어떻게 아느냐는 것입니다. 경험적으로 모델을 조사해서 알아낼 수 있습니다. 예를 들어 Meta가 Llama 3 시리즈 모델에서 환각을 어떻게 다루었는지 살펴봅시다. Meta에서 발표한 이 논문에서 사실성이라고 부르는 부분으로 가면, 기본적으로 모델을 심문해서 무엇을 알고 모르는지, 지식의 경계를 파악하는 절차를 설명합니다. 그런 다음 모델이 모르는 것에 대해 정답이 모델이 모른다는 것인 예시를 학습 세트에 추가합니다. 원칙적으로는 매우 쉬워 보이지만 대략적으로 이것이 문제를 해결합니다.

문제를 해결하는 이유는 모델이 실제로 네트워크 내부에 자기 지식에 대한 꽤 좋은 모델을 가지고 있을 수 있기 때문입니다. 네트워크와 내부의 모든 뉴런을 봤을 때, 모델이 불확실할 때 켜지는 뉴런이 네트워크 어딘가에 있다고 상상할 수 있습니다. 하지만 문제는 그 뉴런의 활성화가 현재 모델이 실제로 모른다고 말로 말하는 것과 연결되어 있지 않다는 것입니다. 신경망의 내부가 알아도 - 그것을 나타내는 뉴런이 있기 때문에 - 모델은 그것을 표면화하지 않고, 대신 학습 세트에서 보는 것처럼 확신 있게 들리도록 최선의 추측을 합니다. 그래서 기본적으로 모델을 심문하고 모르는 경우에 모른다고 말할 수 있도록 해야 합니다.

Meta가 대략 무엇을 하는지 보여드리겠습니다. 여기 예시가 있습니다. 도미니크 하셰크가 오늘의 특집 기사입니다. 무작위로 거기 갔고, 기본적으로 하는 일은 학습 세트의 랜덤 문서를 가져와서 문단을 가져온 다음, LLM을 사용해 그 문단에 대한 질문을 구성합니다. 예를 들어 ChatGPT로 이것을 했습니다. "여기 이 문서의 문단이 있어. 이 문단을 기반으로 세 가지 구체적인 사실 질문을 생성하고 질문과 답을 줘"라고 했습니다. LLM은 이미 이 정보를 재구성할 만큼 충분히 좋습니다. 정보가 이 LLM의 컨텍스트 윈도우에 있으면 꽤 잘 작동합니다. 메모리에 의존할 필요가 없습니다. 바로 컨텍스트 윈도우에 있고, 꽤 높은 정확도로 그 정보를 재구성할 수 있습니다. 예를 들어 "어느 팀에서 뛰었나요? 여기 답이 있습니다. 컵을 몇 개 이겼나요?" 등의 질문을 생성할 수 있습니다.

이제 질문과 답이 있고, 모델을 심문해야 합니다. 대략적으로 하는 일은 질문을 가져와서 모델에 갑니다. Meta의 경우 Llama가 될 것이지만 여기서는 Mistral 7B를 예시로 심문해봅시다. 다른 모델입니다. 이 모델이 이 답을 알까요? 봅시다. 버팔로 세이버스에서 뛰었죠. 모델이 알고, 프로그래밍적으로 결정하는 방법은 기본적으로 모델의 이 답을 정답과 비교하는 것입니다. 모델이 이것을 자동으로 할 만큼 충분히 좋습니다. 인간이 관여하지 않습니다. 모델의 답을 가져와서 다른 LLM 심판을 사용해 이 답에 따라 맞는지 확인할 수 있습니다. 맞으면 모델이 아마 안다는 뜻입니다. 하는 일은 몇 번 이것을 합니다. 알아요, 버팔로 세이버스입니다. 한 번 더 해봅시다. 버팔로 세이버스. 이 사실 질문에 대해 세 번 물었고 모델이 아는 것 같습니다. 좋습니다.

이제 두 번째 질문을 해봅시다. 스탠리 컵을 몇 개 이겼나요? 다시 모델을 심문하고 정답은 2입니다. 여기서 모델은 네 번 이겼다고 주장하는데 맞지 않죠. 2와 맞지 않습니다. 모델이 모르고, 지어내고 있습니다. 여기서 모델이 또 - 좀 느리네요 - 선수 생활 동안 이기지 못했다고 합니다. 분명히 모델이 모르고, 프로그래밍적으로 알 수 있는 방법은 모델을 세 번 심문하고 답을 정답과 비교하는 것입니다. 세 번, 다섯 번 등. 모델이 모르면 이 질문에 대해 모델이 모른다는 것을 알고, 이 질문을 가져와서 새 대화를 학습 세트에 만듭니다. 새 대화를 학습 세트에 추가합니다. "스탠리 컵을 몇 개 이겼나요?"라는 질문에 답은 "죄송합니다, 모르겠습니다" 또는 "기억이 안 납니다"입니다. 모델을 심문해서 그것이 사실임을 봤기 때문에 이것이 이 질문의 정답입니다.

많은 다른 유형의 질문, 많은 다른 유형의 문서에 대해 이것을 하면, 학습 세트에서 모델에게 지식에 기반해 거부할 기회를 주는 것이고, 이런 예시가 학습 세트에 몇 개만 있으면 모델이 알게 됩니다. 존재한다고 추정하는 네트워크 어딘가의 불확실성의 내부 뉴런에 이 지식 기반 거부의 연관을 배울 기회가 있습니다. 경험적으로 이것이 아마 사실인 것 같고, 그 연관을 배울 수 있습니다. "이 불확실성의 뉴런이 높으면 실제로 모르고 '죄송하지만 이것은 기억이 안 납니다' 등이라고 말해도 된다." 이런 예시가 학습 세트에 있으면 이것은 환각에 대한 큰 완화이고, 대략적으로 ChatGPT도 이런 것을 할 수 있는 이유입니다. 이것이 사람들이 구현한 완화의 종류이고 시간이 지나면서 사실성 문제를 개선했습니다.

자, 환각 문제를 완화하기 위한 완화 방법 1번을 설명했습니다. 이제 실제로 훨씬 더 나아질 수 있습니다. 그냥 모른다고 말하는 대신 추가 완화 방법 2번을 도입해서 LLM에게 사실적으로 질문에 답할 기회를 줄 수 있습니다. 내가 사실적인 질문을 하고 당신이 모르면 답하기 위해 무엇을 할까요? 나가서 검색하고 인터넷을 사용해서 답을 찾고 그 답이 무엇인지 말해줄 수 있습니다. 이 모델들로도 정확히 같은 일을 할 수 있습니다. 신경망 내부의 지식, 수십억 파라미터 안의 지식을 오래 전 사전학습 단계에서 모델이 본 것들의 희미한 기억이라고 생각하세요. 파라미터의 지식을 한 달 전에 읽은 것이라고 생각하세요. 계속 읽으면 기억할 것이고 모델은 기억하지만, 드문 것이라면 그 정보에 대한 좋은 기억이 없을 것입니다. 하지만 우리는 그냥 가서 찾아봅니다. 찾아볼 때 기본적으로 하는 일은 정보로 작업 메모리를 새로 고침하고, 그것을 검색하고 이야기할 수 있습니다.

모델이 메모리나 기억을 새로 고침할 수 있는 동등한 것이 필요하고, 모델에 도구를 도입해서 그렇게 할 수 있습니다. 이것에 접근하는 방식은, 모른다고 말하는 대신 도구를 사용하려고 시도할 수 있습니다. 언어 모델이 특수 토큰을 방출할 수 있는 메커니즘을 만들 수 있고, 이것은 우리가 도입할 새 토큰입니다. 예를 들어 여기서 두 개의 토큰을 도입했고, 모델이 이 토큰을 사용할 수 있는 방법에 대한 형식이나 프로토콜을 도입했습니다. 예를 들어 모델이 모를 때 질문에 답하는 대신, 모른다고 말하는 대신, 모델은 이제 특수 토큰 search_start를 방출할 수 있습니다. 이것이 OpenAI의 경우 Bing.com이나 Google 검색 등으로 갈 쿼리입니다. 쿼리를 방출하고 search_end를 방출합니다. 여기서 일어나는 일은 모델에서 샘플링하고 추론을 실행하는 프로그램이 특수 토큰 search_end를 보면, 시퀀스에서 다음 토큰을 샘플링하는 대신 모델에서 생성을 멈추고, 나가서 Bing.com과 세션을 열고 검색 쿼리를 Bing에 붙여넣고 검색된 모든 텍스트를 가져옵니다.

그 텍스트를 가져와서 아마 다른 특수 토큰 등으로 다시 표현하고, 그 텍스트를 가져와서 괄호로 보여드리려 한 것처럼 여기에 복사 붙여넣기합니다. 모든 텍스트가 여기로 오면 컨텍스트 윈도우에 들어갑니다. 모델에게요. 웹 검색의 텍스트가 이제 신경망에 공급될 컨텍스트 윈도우 안에 있고, 컨텍스트 윈도우를 모델의 작업 메모리라고 생각해야 합니다. 컨텍스트 윈도우에 있는 데이터는 모델이 직접 접근할 수 있습니다. 신경망에 직접 공급됩니다. 더 이상 희미한 기억이 아니라 컨텍스트 윈도우에 있고 그 모델에 직접 사용 가능한 데이터입니다. 이제 새 토큰을 샘플링할 때 거기에 복사 붙여넣기된 데이터를 매우 쉽게 참조할 수 있습니다.

대략 이것이 이 도구들이 어떻게 작동하는지입니다. 웹 검색은 도구 중 하나이고 다른 도구들도 살펴볼 것입니다. 기본적으로 새 토큰을 도입하고, 모델이 이 토큰을 활용하고 웹 검색 함수 같은 특수 함수를 호출할 수 있는 스키마를 도입합니다. 웹 검색, search_start, search_end 등의 도구를 모델이 올바르게 사용하도록 어떻게 가르칠까요? 다시 학습 세트를 통해 합니다. 이제 데이터와 모델에게 예시로 웹 검색을 어떻게 사용하는지 보여주는 대화가 많이 필요합니다. 검색을 사용하는 설정이 무엇이고 어떻게 생겼는지, 예시로 검색을 시작하는 방법과 검색 등. 이런 예시가 몇 천 개 있으면 모델이 실제로 이 도구가 어떻게 작동하는지 이해하고, 쿼리를 어떻게 구조화하는지 알게 됩니다. 물론 사전학습 데이터셋과 세계에 대한 이해 때문에 실제로 웹 검색이 무엇인지 어느 정도 이해하고, 어떤 종류의 것이 좋은 검색 쿼리인지 꽤 좋은 네이티브 이해가 있습니다. 그래서 모두 작동합니다. 새 도구를 어떻게 사용하는지 보여주는 몇 가지 예시만 필요하고, 그것에 의존해 정보를 검색하고 컨텍스트 윈도우에 넣을 수 있습니다.

이것은 우리가 무언가를 찾아보는 것과 같습니다. 컨텍스트에 있으면 작업 메모리에 있고 조작하고 접근하기가 매우 쉽습니다. 이것이 몇 분 전에 ChatGPT에서 오슨 코바츠가 누구냐고 검색했을 때 본 것입니다. ChatGPT 언어 모델은 이것이 드문 개인이나 뭔가라고 결정하고 메모리에서 답을 주는 대신 웹 검색을 할 특수 토큰을 샘플링하기로 결정했습니다. "웹 도구 사용 중" 같은 것이 잠깐 나왔고, 2초 기다린 다음 이것이 생성되었고, 여기서 참조를 만들고 있습니다. 출처를 인용하고 있죠. 여기서 일어난 일은 나가서 웹 검색을 하고 이 출처들과 URL들을 찾았고 이 웹 페이지의 텍스트가 모두 여기 사이에 채워졌습니다. 여기 보이지 않지만 기본적으로 텍스트로 여기 사이에 채워져 있습니다. 이제 그 텍스트를 보고 참조하면서 "이 사람들일 수 있습니다 인용, 저 사람들일 수 있습니다 인용" 등이라고 말합니다. 이것이 여기서 일어난 일이고, 오슨 코바츠가 누구냐고 물었을 때 "도구 사용 안 함"이라고 말할 수도 있고, 그러면 ChatGPT가 도구를 사용하지 않고 메모리와 기억만 사용하도록 설득하기에 충분합니다.

ChatGPT에게 이 질문도 해봤습니다. "도미니크 하셰크가 스탠리 컵을 몇 개 이겼나요?" ChatGPT는 실제로 답을 안다고 결정하고 두 번 이겼다고 말할 자신감이 있습니다. 메모리에 의존했는데, 아마도 가중치, 파라미터, 활성화에 충분한 자신감이 있어서 메모리에서만 검색 가능하기 때문입니다. 반대로 웹 검색을 사용해 확인할 수도 있습니다. 같은 쿼리에 대해 실제로 나가서 검색하고 많은 출처를 찾고 이 모든 것이 거기에 복사 붙여넣기되고, 다시 말해주고 인용하고, 실제로 이 정보의 출처인 Wikipedia 기사도 말해줍니다. 이것이 도구, 웹 검색입니다. 모델이 언제 검색할지 결정하고, 대략 이것이 도구가 작동하는 방식이고, 환각과 사실성에 대한 추가적인 완화입니다.

매우 중요한 심리적 포인트를 다시 강조하고 싶습니다. 신경망의 파라미터에 있는 지식은 희미한 기억입니다. 컨텍스트 윈도우를 구성하는 토큰에 있는 지식은 작업 메모리입니다. 대략적으로 우리 뇌에서 작동하는 방식과 같습니다. 우리가 기억하는 것은 파라미터이고, 몇 초나 몇 분 전에 경험한 것 등은 컨텍스트 윈도우에 있다고 상상할 수 있습니다. 이 컨텍스트 윈도우는 주변에서 의식적 경험을 할 때 쌓입니다.

이것은 실제로 LLM 사용에도 많은 함의가 있습니다. 예를 들어 ChatGPT에 가서 이런 것을 할 수 있습니다. "제인 오스틴의 오만과 편견 1장을 요약해줄 수 있어?"라고 말할 수 있고, 완전히 괜찮은 프롬프트이고 ChatGPT는 실제로 여기서 비교적 합리적인 일을 합니다. 그 이유는 ChatGPT가 오만과 편견 같은 유명한 작품에 대해 꽤 좋은 기억이 있기 때문입니다. 아마 많은 것을 봤고, 이 책에 대한 포럼이 있고, 이 책의 버전을 읽었을 것이고, 기억하기 때문입니다. 이것을 읽었거나 관련 기사를 읽었다면 이 모든 것을 말할 수 있을 만큼 기억할 것입니다.

하지만 보통 실제로 LLM과 상호작용하고 특정 것을 기억하길 원할 때는 그냥 주는 것이 항상 더 잘 작동합니다. 훨씬 더 나은 프롬프트는 이런 것입니다. "오만과 편견 1장을 요약해줄 수 있어? 참고용으로 아래에 첨부했어." 그런 다음 여기서 구분자를 넣고 붙여넣습니다. 어떤 웹사이트에서 찾아서 여기에 1장을 복사 붙여넣기했습니다. 그렇게 하는 이유는 컨텍스트 윈도우에 있으면 모델이 직접 접근할 수 있고, 기억할 필요 없이 그냥 접근할 수 있기 때문입니다. 이 요약은 이 요약보다 상당히 더 높은 품질이 예상됩니다. 모델에 직접 사용 가능하기 때문입니다. 우리도 같은 방식으로 작동할 것입니다. 요약해야 하기 전에 이 장을 다시 읽었다면 훨씬 더 나은 요약을 만들 것이고, 기본적으로 그것이 여기서 일어나는 일이거나 그것과 동등한 것입니다.

다음으로 간략히 이야기하고 싶은 심리적 특이점은 자기 지식입니다. 인터넷에서 매우 자주 보는 것은 사람들이 LLM에게 "어떤 모델이야? 누가 만들었어?"라고 묻는 것입니다.

---
