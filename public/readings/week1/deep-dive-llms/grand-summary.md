---
title: "24. Grand Summary"
titleKr: "24. 전체 요약"
chapter: 24
timestamp: "3:21:46"
sourceUrl: "https://www.youtube.com/watch?v=7xTGNNLPyMI&t=12106s"
translatedAt: "2026-01-10"
---

# 24. 전체 요약

[영상 바로가기 (3:21:46)](https://www.youtube.com/watch?v=7xTGNNLPyMI&t=12106s)

## 요약

강의 전체 내용 정리입니다.

## 핵심 포인트

- 3단계: 사전학습 → SFT → RL
- ChatGPT = 라벨러의 통계적 시뮬레이션
- 도구로 사용하고 결과 검증하기

---

## 전체 번역

**요약**: 전체 강의 내용을 요약합니다. LLM은 인터넷 데이터로 사전학습된 후, 대화 데이터로 미세조정되고, 강화학습으로 개선됩니다. 이들은 강력하지만 완벽하지 않은 도구이며, 신뢰하되 검증하는 자세로 활용해야 합니다.

[3:39:00] 이제 우리가 시작한 곳으로 돌아가 봅시다. 질문은 chat.openai.com에 가서 어떤 쿼리를 입력하고 가기를 누르면 정확히 무엇이 일어나는가였습니다. 무엇을 보고 있고, 무엇과 대화하고 있고, 이것이 어떻게 작동하는지.

[3:39:30] 이 비디오가 이 모델들이 어떻게 훈련되고 무엇이 돌아오는지에 대한 내부 세부 사항에 대해 약간의 이해를 주었기를 바랍니다. 특히 이제 우리는 쿼리가 먼저 토큰으로 잘린다는 것을 압니다. tiktokenizer로 가면 사용자 쿼리가 있는 형식에서

[3:40:00] 기본적으로 쿼리를 거기에 넣습니다. 쿼리가 여기서 논의한 대화 프로토콜 형식에 삽입되는데, 이것이 대화 객체를 유지하는 방식입니다. 거기에 삽입되고, 이 모든 것이 결국 토큰 시퀀스, 내부적으로 1차원 토큰 시퀀스가 됩니다.

[3:40:30] ChatGPT가 이 토큰 시퀀스를 보고, 가기를 누르면 기본적으로 이 목록에 토큰을 계속 추가합니다. 시퀀스를 계속합니다. 토큰 자동완성처럼 작동합니다. 특히 이 응답을 줬습니다.

[3:41:00] 기본적으로 여기에 넣으면 계속한 토큰을 볼 수 있습니다. 대략적으로요. 이제 질문은 왜 이것들이 모델이 응답한 토큰인가, 이 토큰들이 무엇이고, 어디서 오고, 무엇과 대화하고, 이 시스템을 어떻게 프로그래밍하는가입니다.

[3:41:30] 그래서 기어를 바꾸고 내부 부분에 대해 이야기했습니다. 이 과정의 첫 번째 단계, 세 단계가 있는데, 사전학습 단계입니다. 이것은 근본적으로 인터넷에서 이 신경망의 매개변수로 지식을 습득하는 것과 관계가 있습니다.

[3:42:00] 신경망이 인터넷에서 많은 지식을 내재화하지만, 성격이 정말로 나오는 곳은 지도학습 미세조정 과정입니다. 여기서 기본적으로 OpenAI 같은 회사가 매우 다양한 주제에 걸쳐 100만 개의 대화 같은 대규모 대화 데이터셋을 큐레이션합니다.

[3:42:30] 인간과 어시스턴트 사이의 대화이고, 이 전체 과정에서 많은 합성 데이터 생성과 많은 LLM 도움 등이 사용되지만, 근본적으로 이것은 많은 인간이 관여된 인간 데이터 큐레이션 작업입니다. 특히 이 인간들은 OpenAI에 고용된 데이터 라벨러로

[3:43:00] 라벨링 지침을 배우고 임의의 프롬프트에 대해 이상적인 어시스턴트 응답을 만드는 것이 그들의 작업입니다. 예시로 신경망에 프롬프트에 어떻게 응답하는지 가르칩니다. 여기서 돌아온 것을 어떻게 생각해야 할까요? 이것이 무엇일까요?

[3:43:30] 제가 생각하기에 올바른 방법은 이것이 OpenAI의 데이터 라벨러의 신경망 시뮬레이션이라는 것입니다. 마치 이 쿼리를 OpenAI의 데이터 라벨러에게 주고, 이 데이터 라벨러가 먼저 OpenAI의 모든 라벨링 지침을 읽고, 2시간 동안

[3:44:00] 이 쿼리에 대한 이상적인 어시스턴트 응답을 작성하고 저
