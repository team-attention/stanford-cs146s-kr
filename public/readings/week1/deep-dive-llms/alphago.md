---
title: "19. AlphaGo"
titleKr: "19. 알파고"
chapter: 19
timestamp: "2:48:47"
sourceUrl: "https://www.youtube.com/watch?v=7xTGNNLPyMI&t=10127s"
translatedAt: "2026-01-10"
---

# 19. 알파고

[영상 바로가기 (2:48:47)](https://www.youtube.com/watch?v=7xTGNNLPyMI&t=10127s)

## 요약

딥마인드의 알파고는 강화학습이 인간 전문가를 뛰어넘을 수 있음을 입증한 역사적 사례입니다. 지도학습만으로는 인간 최고 수준에 도달할 수 없지만, 자기 대국을 통한 강화학습으로 이세돌과 같은 세계 최고 선수를 이길 수 있었습니다. 특히 "37수"는 인간이 두지 않을 창의적인 수로, RL이 인간 지식의 한계를 넘어설 수 있음을 상징합니다.

**핵심 개념:**
- **지도학습의 한계**: 인간 전문가를 모방하는 것만으로는 인간 수준을 절대 넘을 수 없음
- **자기 대국(Self-Play)**: 알파고가 자기 자신과 대국하며 승리하는 전략을 스스로 발견
- **37수(Move 37)**: 인간 선수가 둘 확률 1만 분의 1인 수로, RL이 발견한 혁신적 전략
- **엘로 레이팅**: RL 모델이 지도학습 모델보다 훨씬 높은 실력에 도달하는 것을 보여주는 지표
- **LLM에의 적용**: 바둑에서 입증된 RL의 힘을 열린 사고 영역으로 확장하려는 현재의 연구 방향

---

## 전체 번역

우리가 구성하는 이 보상 모델은 점수를 출력하는데, 이 모델들은 트랜스포머이고, 수십억 개의 매개변수를 가진 거대한 신경망이고, 인간을 모방하지만 시뮬레이션 방식으로 합니다. 문제는 이것들이 거대하고 복잡한 시스템이라는 것입니다. 여기에 10억 개의 매개변수가 단일 점수를 출력합니다. 이 모델들을 게임하는 방법이 있다는 것이 밝혀졌습니다. 훈련 세트의 일부가 아니었던 종류의 입력을 찾을 수 있고, 이 입력들은 설명할 수 없이 매우 높은 점수를 받지만 가짜 방식으로요. RLHF를 매우 오래 실행하면, 예를 들어 많은 업데이트인 1,000번의 업데이트를 하면, 농담이 더 좋아지고 펠리컨에 대한 진짜 명작을 얻을 것이라고 기대할 수 있지만, 정확히 그렇게 되지 않습니다. 처음 몇 백 단계에서 펠리컨에 대한 농담이 아마 약간 개선되다가, 실제로 극적으로 절벽에서 떨어지고 매우 말도 안 되는 결과를 얻기 시작합니다.

예를 들어 펠리컨에 대한 최고의 농담이 "the the the"가 되기 시작합니다. 이것은 말이 안 됩니다. 보면 왜 이것이 최고의 농담이어야 할까요? 하지만 "the the the"를 보상 모델에 넣으면 0의 점수를 예상하지만, 실제로 보상 모델은 이것을 농담으로 좋아합니다. 1.0의 점수, 최고의 농담이라고 말합니다. 말이 안 되죠. 하지만 이 모델들은 인간의 시뮬레이션일 뿐이고, 거대한 신경망이고, 말도 안 되는 결과를 주는 입력 공간의 부분으로 들어가는 입력을 찾을 수 있습니다. 이 예시들이 적대적 예시라고 불리는 것이고, 너무 깊이 들어가지는 않겠지만, 모델에 대한 적대적 입력입니다. 모델의 틈새 사이로 들어가서 맨 위에서 말도 안 되는 결과를 주는 특정한 작은 입력입니다. 이렇게 상상할 수 있습니다. "the the the"는 분명히 1점이 아니고, 분명히 낮은 점수이니, "the the the"를 데이터셋에 추가하고 매우 나쁜, 5점의 순서를 주자. 실제로 모델은 "the the the"가 매우 낮은 점수여야 한다는 것을 배우고 0점을 줄 것입니다. 문제는 모델에 숨어 있는 말도 안 되는 적대적 예시가 기본적으로 무한하다는 것입니다. 이 과정을 여러 번 반복하고 말도 안 되는 것을 보상 모델에 계속 추가하고 매우 낮은 점수를 주면, 게임에서 절대 이길 수 없습니다. 많은 라운드를 할 수 있고, 강화학습을 충분히 오래 실행하면 항상 모델을 게임하는 방법을 찾습니다.

적대적 예시를 발견하고, 말도 안 되는 결과로 정말 높은 점수를 얻습니다. 근본적으로 점수 함수가 거대한 신경망이고, RL은 그것을 속이는 방법을 찾는 데 매우 뛰어나기 때문입니다. 결론적으로 항상 RLHF를 몇 백 번의 업데이트 정도 실행하고, 모델이 더 좋아지다가, 자르고 끝내고 출시합니다. 이 보상 모델에 대해 너무 많이 실행할 수 없습니다. 최적화가 그것을 게임하기 시작하고, 기본적으로 자르고 출시합니다. 보상 모델을 개선할 수 있지만, 어느 시점에서 결국 이런 상황에 도달합니다.

RLHF에 대해 제가 보통 말하는 것은 RLHF는 RL이 아니라는 것입니다. 물론 RLHF는 RL이지만, 마법 같은 의미에서 RL이 아닙니다. 무한정 실행할 수 있는 RL이 아닙니다. 정답을 얻는 이런 종류의 문제에서는 이것을 쉽게 게임할 수 없습니다. 정답을 얻었거나 얻지 못했거나 둘 중 하나이고, 점수 함수가 훨씬 더 간단합니다. 박스 영역을 보고 결과가 맞는지 보는 것입니다. 이 함수들을 게임하기 매우 어렵지만, 보상 모델을 게임하는 것은 가능합니다.

이 검증 가능한 영역에서는 RL을 무한정 실행할 수 있습니다. 수만, 수십만 단계를 실행하고 우리가 생각하지도 못할 모든 종류의 미친 전략을 발견할 수 있습니다. 바둑 게임에서 게임의 승패를 게임하는 방법은 없습니다. 완벽한 시뮬레이터가 있고, 모든 돌이 어디에 놓여 있는지 알고, 누가 이겼는지 계산할 수 있습니다. 그것을 게임하는 방법이 없으니 RL을 무한정 할 수 있고, 결국 이세돌도 이길 수 있습니다. 하지만 게임할 수 있는 이런 모델들은 이 과정을 무한정 반복할 수 없습니다.

RLHF를 진짜 RL이 아닌 것으로 봅니다. 보상 함수가 게임할 수 있기 때문입니다. 약간의 미세조정, 약간의 개선이지만, 더 많은 연산을 넣고 더 오래 실행하면 훨씬 더 좋고 마법 같은 결과를 얻을 수 있는 것이 근본적으로 올바르게 설정되어 있지 않습니다. 마법이 없다는 의미에서 RL이 아닙니다. 모델을 미세조정하고 더 나은 성능을 얻을 수 있고, 실제로 ChatGPT로 돌아가면 GPT-4o 모델은 RLHF를 거쳤습니다. 효과가 있기 때문입니다. 하지만 같은 의미의 RL은 아닙니다. RLHF는 모델을 약간 개선하는 약간의 미세조정입니다.

---
