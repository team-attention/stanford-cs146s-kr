---
title: "Warp로 Ollama 설정하기"
originalTitle: "How to Setup Ollama using Warp"
category: "developer-workflows"
sourceUrl: "https://youtu.be/Aq8vDxUg4VE"
translatedAt: "2026-01-20"
status: "draft"
---

# Warp로 Ollama 설정하기

[영상 바로가기](https://youtu.be/Aq8vDxUg4VE)

## 핵심 요약

- **Ollama**: 로컬에서 AI 모델을 실행할 수 있는 무료 커맨드 라인 도구
- **시스템 요구사항**: LLM 파라미터 수(B)와 비슷한 양의 VRAM 필요 (예: 10억 파라미터 ≈ 1GB VRAM)
- **모델 선택 기준**:
  - Thinking: 복잡한 문제 해결용
  - Tools: 웹 검색 등 도구 호출 기능
  - Vision: 이미지 분석 기능
  - Embedding: 자연어 검색용
  - Quantization: 메모리 최적화 (4비트 권장)
- **애플리케이션 통합**: OpenAI 호환 엔드포인트 사용, base URL만 변경하면 됨
- **모델 커스터마이징**: Modelfile로 system prompt, temperature 등 조정 가능
- **Warp 활용**: AI 기반 명령어 검색, 모델 파일 생성 등 생산성 향상

---

## 전체 번역

로컬에서 AI 모델을 실행하고 싶다면 Ollama가 가장 빠르게 시작할 수 있는 방법 중 하나입니다. 이 영상에서는 Warp를 사용하여 가장 쉬운 방법으로 Ollama를 설정하는 방법을 보여드리겠습니다. 여러분의 디바이스를 프로파일링하여 무엇을 실행할 수 있는지 확인하고, 모델 간의 차이를 살펴보고, 디바이스에서 실행한 다음 Ollama를 애플리케이션에 통합하는 방법을 다룹니다.

로컬에서 LLM을 실행하는 것은 어떤 시스템에든 부담이 큽니다. 그래서 Ollama를 시작하기 전에 우리가 어떤 환경을 가지고 있는지 확인해봅시다. LLM을 로컬에서 실행하려면 상당한 사양이 필요합니다. Mac에서는 통합 메모리를 사용하는데, 제 머신에는 64GB가 있습니다. Windows에서는 32GB VRAM을 가진 Nvidia RTX 590을 사용합니다. 여기서 차이점은 Mac은 더 큰 모델을 담을 수 있지만 성능이 약한 반면, Nvidia 카드는 극강의 성능을 가지지만 모든 것을 VRAM에 담을 수 없다는 제약이 있습니다.

사양을 이해했다면 이제 Ollama를 설치해봅시다. Ollama는 로컬에서 AI 모델을 실행할 수 있는 가장 쉬운 방법 중 하나입니다. 무료이고 완전히 커맨드 라인에서 실행할 수 있습니다. 다운로드한 후 Warp를 열어서 제대로 설치되었는지 확인해봅시다. Docker나 NPM 같은 다른 레지스트리 기반 커맨드 라인 도구에 익숙하다면 매우 친숙하게 느껴질 것입니다. `ollama run model` 명령어로 간단하게 AI 모델을 실행할 수 있습니다.

하지만 어떤 AI 모델을 사용해야 할까요? 시스템 사양을 참고하여 무엇이 가능한지 확인하세요. Ollama 웹사이트를 보면 모델의 파라미터 크기를 확인할 수 있습니다. 두 가지 모델을 시도해봅시다.

먼저 OpenAI GPT OSS 200억 파라미터 모델을 시도해봅시다. 이 모델은 매우 우수하고 유능합니다. 최소 16GB의 VRAM이 필요하고 컨텍스트를 위한 추가 메모리도 필요합니다. 하지만 이 모델의 정말 멋진 점은 도구 호출(tool calls) 기능입니다. 이 기능을 보여드리기 위해 Ollama는 데스크톱 앱에 내장된 웹 검색 기능을 제공합니다. 터보 모드가 활성화되지 않은 한 남용을 방지하기 위해 Ollama 로그인이 필요하지만, 모델 자체는 여전히 로컬에서 실행됩니다.

이제 다른 모델을 시도해봅시다. Qwen 3 80억 파라미터 모델입니다.

이건 훨씬 빠릅니다. 하지만 출력 품질이 완전히 동일하지는 않다는 것을 알 수 있습니다. 여기서 무엇이 효과적이고 무엇이 그렇지 않은지 테스트하는 것이 매우 중요해집니다. 어떤 LLM은 그렇게 똑똑할 필요가 없습니다. 빠르고 어느 정도 정확하기만 하면 됩니다. 다른 LLM들은 더 특화된 기능을 가지고 있습니다. 예를 들어 코드에 강한 모델이 있고, 창작 글쓰기에 강한 모델도 있습니다. 온라인에 벤치마크가 있지만 물론 회의적으로 봐야 합니다. 무엇이 최선인지 판단하는 진짜 방법은 직접 시도해보는 것입니다.

LLM은 다양한 타입과 특성을 가지고 있습니다. 로컬 모델을 선택할 때 훨씬 쉽게 고를 수 있도록 몇 가지 용어를 간단하게 정리해보겠습니다.

**Thinking(사고)**은 모델이 답변을 제공하기 전에 긴 텍스트 블록으로 스스로 생각하는 것입니다. 이것은 복잡한 문제를 해결하려고 할 때 조금 더 시간을 들여 생각할 필요가 있을 때 가장 효과적입니다.

**Tools(도구)**는 LLM이 답변을 찾는 과정에서 사용할 수 있도록 제공하는 것들입니다. 예를 들어 웹 검색 도구를 제공하면, 쿼리를 추측하는 대신 그 도구를 사용하기로 결정할 수 있습니다.

**Vision(비전)**은 이미지를 보고 응답에 포함시킬 수 있는 능력입니다.

**Embedding(임베딩)**은 LLM 자체와는 별개입니다. 텍스트를 수학적 형식으로 변환하여 자연어 검색을 사용하는 애플리케이션을 훨씬 쉽게 만들 수 있게 합니다.

**숫자 B 부분**은 수십억 개의 파라미터 수를 나타냅니다. 숫자가 높을수록 모델이 더 똑똑하지만, 훨씬 더 많은 처리 능력이 필요합니다. VRAM의 양은 파라미터 수(십억 단위)와 같아야 한다고 보면 됩니다. 예를 들어 10억 파라미터는 대략 1GB의 VRAM이 필요합니다.

**Quantization(양자화)**은 품질을 희생하면서 LLM의 메모리를 줄이는 방법입니다. 기본적으로 Ollama는 4비트 양자화를 사용하여 모델을 다운로드하는데, 이는 최적화와 성능 저하 사이의 적절한 균형점입니다.

일반적으로 원하는 기능을 제공하는 LLM을 사용하되 그 이상은 사용하지 마세요. 이제 커맨드 라인에서 실행하는 것도 좋지만, 실제로는 이것을 다른 것에 통합하고 싶을 것입니다. 좋은 소식이 있습니다. 실제로 매우 간단하고 리팩토링이 거의 필요하지 않을 것입니다.

대부분의 LLM은 OpenAI 호환 엔드포인트를 사용합니다. 여러분의 애플리케이션도 아마 그럴 것입니다. 저는 Warp AI에게 제 코드 파일로 가서 OpenAI 클라이언트를 초기화한 위치를 검색하도록 요청하겠습니다. 그런 다음 base URL을 기본값인 `http://localhost:11434`로 바꾸고, API 키로 아무거나 넣습니다. 그리고 채팅 완성(chat completion) 엔드포인트에서 사용하고 싶은 모델을 입력하면 됩니다.

Ollama의 장점은 초기화되면 자동으로 LLM을 구동하고 비활성 상태가 되면 백그라운드로 보낸다는 것입니다. 앱이 로컬 우선이라면 리소스를 절약할 수 있는 훌륭한 방법입니다. 물론 폴링(polling) 기능을 통합하거나 TTL(time to live)을 완전히 비활성화하여 이를 완전히 우회할 수도 있습니다.

이제 사용하는 모든 채팅 애플리케이션에는 temperature, system prompt, top P와 같이 조작할 수 있는 커스터마이징 기능이 있으며, 이것은 Ollama에서도 가능합니다. 먼저 편집하고 싶은 모델을 pull해서 컴퓨터에 있는지 확인합니다.

그런 다음 이것을 새 모델로 저장하여 나중에 사용할 수 있습니다. 여기 훌륭한 팁이 있습니다. Warp를 사용하여 모델 파일을 만들면 필요한 것에 대해 훨씬 더 상세한 프롬프트와 시스템 메시지를 얻을 수 있습니다. 예를 들어 저는 Warp에게 YouTube 제목 생성기를 위한 모델 파일을 만들어달라고 요청하겠습니다. 그러면 사용할 수 있는 훨씬 더 상세한 시스템 프롬프트가 생성됩니다.

이상입니다. Ollama로 로컬에서 LLM을 실행하는 방법이었습니다.
