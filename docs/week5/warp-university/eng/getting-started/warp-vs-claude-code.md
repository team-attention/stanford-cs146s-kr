---
title: "Warp VS Claude Code"
titleKr: "Warp vs Claude Code 비교"
category: "getting-started"
sourceUrl: "https://youtu.be/NUVftxAqZQo"
---
# Warp VS Claude Code

[영상 바로가기](https://youtu.be/NUVftxAqZQo)

## 전체 자막

Hey there, I'm Ben Holmes, product engineer at Warp, and I want to do a quick comparison video between Claude Code and Warp's built-in coding agent.

Let's talk about Claude Code first.

Claude is a CLI tool, so you'll need a terminal to start like warp, and you'll need to install the Claude CLI. Then you can run Claude to start prompting the coding agent. You can type your prompt in this box and let Claude code do its thing. It's able to read files, search your codebase for key terms, and once it has enough context, it's able to make diffs on your behalf. Here, we're editing the readme file. It presents a diff that I'm able to manually review, or I can hit shift tab to auto accept edits for the rest of the session. You can also enter planning mode to have the agent do some research and present a markdownbased plan before it starts working. Warp's coding agent is built into Warp directly. So, if you already use Warp Day today as a terminal, you can submit an AI query instead. And if it looks like an AI query, we'll smartly kick you over to agent mode, which you can enter manually at any time with the keyboard shortcut or this little agent button right here. Once you submit a query, it will kick off a warp coding task. So, it's able to read files, search your codebase, and create code diffs similar to claw code. Now, one key difference is what you can do with these diffs. You're able to accept or auto approve for the rest of the session, but you can also edit that diff directly. We can give you nice affordances like a built-in editor if you want to edit the diff manually, remove comments that an agent might have added, etc. And when you hit accept, it will notify the agent about those diffs so it doesn't try to overwrite them later in the session.

Another key difference is reviewing your changes. Cloud code requires you to use git cli or a tool like VS Code to review your diffs, but Warp has a built-in button for this. You can hit review changes at the end of a session or hit this charm right here and you'll get a code diff view to see all of the changes the agent has worked on. You can also diff against the main line if you're working on a multi-step PR and you want to look at all the changes that you've worked on in that session. As for planning, Warp has an option to allow the agent to prompt you for a plan if a query sounds intensive enough. For this query here, we decided to prompt for a plan. You can proceed without one or we'll use cloud for opus to do some research and present a similar markdown plan to claude code. Now let's talk about context gathering. In cloud code, it's able to crawl your codebase and figure out what context may be relevant, but you can add explicit context by using the at symbol. This gives you a file picker across all the files in your current directory. So you can reference those and directly ask questions. Warp's coding agent also lets you reference context explicitly. Hit the at symbol and you can reference files in your current project. But you can also reference symbols. This is very powerful. If there's a specific function or object that you want to reference, this will inline the name of that function, the file it belongs to, and the exact line number so you can ask more specific questions. And if you want to locate a file directly, there's a project explorer available. This pulls up a simple file tree to look at all the files in your project. and clicking on it will open an inline editor to either edit the file directly or reference specific parts of that file for context in your LLM queries. This will also insert the line number and the file name so that the agent knows where to look.

Now let's discuss model selection.

Claude has a number of options available from the slash menu including model.

This lets you select between Claude's models. So Claude Forson opus and opus plan mode for researching tasks in warp.

You can hit the model dropown to select your model here. This gives you access to a number of different models including Claude and Claude for Opus along with the Gemini and GBD5 series of models with different reasoning levels for GPT5. Now let's discuss configuration options. In cloud code, you can configure all aspects of the tool from the slash command. This lets you see simple options like the current status, a model switcher to change which model you're going to use, a permissions menu to choose what tools are allowed or denied, and also an agents creator to spin off a sub agent for specific coding tasks. Heading over to warp, there's a couple different ways to configure your setup. You can use a slash command here as well to add MCP tools, prompts, as well as global rules. Cloud code scopes the rules to specific git repos, but in warp you can have a global rules directory to describe how all projects should behave and that's accessible from here. Warp also offers codebase indexing which you can kick off right here to create embeddings for your codebase to improve file search. As for agent configuration, you can edit that using profiles. Here I'm in the warp settings menu and I've set up a default profile with my preferences for base planning and read and write permissions. I've also created another profile that's a little bit more relaxed, always allowing code diffs without my manual review.

Comparing these tools, they both offer extensive configuration over what an agent is allowed to do, but I would give an edge to Claude Code for also allowing sub agents and hooks. Now, let's discuss managing agents over time. Since Claude Code is a CLI tool, you'll need to watch that CLI tab to see if the agent is ever blocked on a coding task. Though, it will update your tab description to see what the agent is currently doing. Since the warp coding agent is built into the tool, we can give you more information about what an agent is doing, like a status indicator to show where the agent is in the process, a more descriptive query in the tool tip, and also toast notifications whenever an agent is blocked, so you can quickly jump over to that tab and take action. You can also choose to receive desktop notifications with or without sound if you prefer.

Now, let's run encoding tasks to compare performance. I have warp on the left and claw code on the right. And I'm going to give each of them a sentry issue. Going to pass off the URL. And I'm not going to give much more direction than that. I ran this task three times across each of the coding agents to see how they perform on code quality and the amount of time it takes. You can see a breakdown here where each agent takes between two and four minutes using Cloud Force Sonnet with a slight edge to claude code on the average time. Looking at the quality of research before coding, we see similar performance between the two of them. Each were able to identify the Sentry MCP server, although there was one example in the Cloud Code trial that used a fetch query. Though asking it to use the MCP server explicitly got it back on track, they were also able to identify the root cause of the issue, which is this render keyboard shortcut function. This is a utility that was expecting a keyboard shortcut to always be bound when the user could have unbound it from their settings. Looking at the code output first for clawed code, we see a mixed bag on the quality of the solution.

Though all of them were working here, we can see it's stubbing out some empty UI if we try to render the keystroke when it's not available. But it would have been better to avoid calling this render function entirely. It was able to half identify that by adding a check here before calling it, but it also added the extra check below, which was unnecessary. Warp offered three different solutions that all compiled and ran as well. In this example, it decided to return an optional type from the render function, so we could decide whether or not to use the result. This is probably my favorite example out of all of the trials, although if I wrote it myself, I would have put an if check instead. Also note one outlier from the warp side that decided to render an empty flex row with this margin value. I don't even know where it came from, but cla isn't always predictable. And as a bit of a wild card, I decided to use GPT5 in the warp coding agent as well.

This is obviously something you can't do with Claude code. So maybe it's not a direct comparison, but you do have multiple models available in warp code.

So it is worth trying out GPT or Gemini to see what works in your codebase. And in this trial using the low reasoning model, I was able to get some really great results. You can see the breakdown here where GBD5 is performing at a very consistent minute 12 seconds to 20 seconds with high quality output. All of the solutions compile and ran no problem. And they gave a pretty similar answer every time. It is stubbing out empty UI, which as I noted earlier isn't the solution I would have done, but it was a concise and working solution. So, conclusion time. Warp code or clawed code? If you know you want to use the clawed set of models and you prefer working in a terminal environment instead of a UI, you might want to choose claw code. But if you value the coding features of Warp, like a file tree for context, code diffs, and selecting different models like GPT5, then Warp Code is going to be a good choice for you.
