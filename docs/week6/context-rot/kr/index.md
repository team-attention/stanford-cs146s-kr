---
title: "컨텍스트 로트: AI 컨텍스트 윈도우의 성능 저하 이해하기"
originalTitle: "Context Rot: Understanding Degradation in AI Context Windows"
author: "Chroma Research"
sourceUrl: "https://research.trychroma.com/context-rot"
translatedAt: "2026-01-13"
status: "final"
qaScore:
  consistency: 9
  readability: 8
  accuracy: 9
  overall: 9
---

# 컨텍스트 로트: 입력 토큰 증가가 LLM 성능에 미치는 영향

[원본 링크](https://research.trychroma.com/context-rot)

## 서론

대규모 언어 모델(LLM)은 일반적으로 컨텍스트를 균일하게 처리한다고 가정됩니다. 즉, 모델이 10,000번째 토큰도 100번째 토큰만큼 신뢰성 있게 다뤄야 한다는 것입니다. 하지만 실제로 이 가정은 맞지 않습니다. 간단한 작업에서도 입력 길이가 달라지면 모델 성능이 크게 변하는 것을 확인할 수 있습니다.

이 보고서에서는 최신 GPT-4.1, Claude 4, Gemini 2.5, Qwen3 모델을 포함한 18개 LLM을 평가합니다. 연구 결과, 모델은 컨텍스트를 균일하게 사용하지 않으며 입력 길이가 늘어날수록 성능 신뢰성이 점점 떨어집니다.

현대 LLM은 수백만 토큰에 달하는 입력 컨텍스트 길이를 갖는 것이 일반적입니다. Gemini 1.5 Pro가 2024년 초에 100만 토큰 컨텍스트 윈도우를 처음 선보였고, 이후 GPT-4.1의 100만 토큰 컨텍스트 윈도우와 1,000만 토큰을 지원하는 Llama 4가 뒤를 이었습니다. 긴 컨텍스트의 활용 사례는 매력적입니다. 컨텍스트가 길면 LLM이 각 호출에서 더 많은 정보를 처리하고 더 풍부한 출력을 생성할 수 있기 때문입니다.

이러한 모델의 긴 컨텍스트 평가는 종종 입력 길이 전반에 걸쳐 일관된 성능을 보여줍니다. 하지만 이런 평가는 범위가 좁고 실제로 긴 컨텍스트를 사용하는 방식을 제대로 반영하지 못합니다. 가장 많이 쓰이는 테스트인 Needle in a Haystack(NIAH)는 간단한 어휘 검색 작업으로, 모델이 긴 컨텍스트를 신뢰성 있게 처리하는 능력을 일반화하는 데 자주 사용됩니다. 하지만 에이전트 작업이나 요약 같은 실제 애플리케이션은 더 광범위하고 종종 더 모호한 정보에 대해 훨씬 많은 처리와 추론을 요구합니다.

현실적인 긴 컨텍스트 벤치마크 설계는 어렵습니다. 작업 복잡도가 종종 입력 길이와 함께 증가해서, 성능 저하가 더 긴 입력 때문인지 본질적으로 더 어려운 문제 때문인지 구분하기 어렵기 때문입니다. 이를 해결하기 위해 우리 실험에서는 입력 길이만 변경하면서 작업 복잡도를 일정하게 유지했습니다. 이를 통해 입력 길이 단독의 영향을 직접 측정할 수 있습니다.

## 기여

이 연구에서 제시하는 내용:

- 주요 폐쇄형 소스 및 오픈 웨이트 모델을 포함한 18개 LLM 평가로, 입력 길이 증가에 따른 비균일한 성능을 보여줍니다.

- 방해 요소와 다양한 질문-답변 유사도를 처리할 때 관찰된 모델별 행동 패턴 기술.

- 결과를 재현할 수 있는 전체 코드베이스.

## 관련 연구

모델의 긴 컨텍스트 능력을 평가하는 데 가장 널리 쓰이는 벤치마크 중 하나가 Needle in a Haystack(NIAH)입니다. 확장 가능한 테스트로 유용하지만, 좁은 능력인 어휘 검색만 측정합니다. 모델이 NIAH에서 일반적으로 좋은 성능을 보이면서, 긴 컨텍스트 문제가 대체로 해결됐다는 인식이 퍼졌습니다.

하지만 NIAH는 실제로 대부분의 긴 컨텍스트 작업이 요구하는 수준을 과소평가합니다. 비어휘적 매칭을 포함하는 needle-question 쌍이 있는 NoLiMa 같은 NIAH 변형에서는 상당한 성능 저하가 나타납니다. 난이도 면에서 비슷해 보이는 다른 작업들, 예를 들어 모델이 주어진 텍스트 조각의 부재를 인식하는지 테스트하는 AbsenceBench도 입력 길이 증가에 따른 성능 저하를 보여줍니다.

또한 긴 컨텍스트 작업은 종종 방해 요소를 구분해야 합니다. 한 예가 Multi-round co-reference resolution(MRCR)인데, 다중 턴 대화에서 유사한 사용자 요청 중 i번째 특정 인스턴스를 검색하는 작업입니다. 하지만 긴 컨텍스트 설정에서 방해 요소의 영향에 대한 조사는 여전히 부족합니다.

긴 컨텍스트 작업에서 중요한 요소는 입력 길이 확장 방식입니다. Latent List는 모델이 다양한 입력 길이에서 고정된 수의 Python 리스트 연산을 수행해야 하는 작업입니다. 관련 없는 컨텍스트를 채우는 여러 방법을 테스트한 결과 모델 성능에 비균일한 영향이 나타났습니다. 예를 들어, 로컬에서 서로 상쇄되는 리스트 연산을 추가하면 print 문을 추가하는 것보다 모델 성능이 더 크게 저하됩니다. 이는 '관련 없는 콘텐츠'의 유형이 중요함을 강조하는데, 일부는 입력 길이에 따라 복잡성이 증가할 수 있기 때문입니다.

마찬가지로 Graphwalks는 모델에게 16진수 해시로 구성된 방향 그래프를 주고, 무작위 노드에서 시작해 너비 우선 탐색을 수행하도록 요청하는 그래프 순회 작업입니다. 입력 길이가 증가하면 순회할 그래프 크기가 커져 작업 난이도가 올라갑니다. 작업 복잡성 증가와 입력 길이를 구분하기 어려워서 입력 길이만의 성능 영향을 분리하기 어렵습니다. 이는 LLM이 긴 입력에서 실제로 어떻게 행동하는지 이해하려면 관심 변수로서 입력 길이를 분리하는 것이 중요함을 보여줍니다.

## Needle in a Haystack 확장

고전적인 Needle in a Haystack 작업은 긴 컨텍스트 윈도우(haystack) 중간에 무작위 사실(needle)을 배치한 다음, 모델에게 그 사실을 질문하는 것입니다.

이 작업의 원래 구현은 어휘적 매칭이 있는 needle-question 쌍을 사용합니다. 하지만 실제 긴 컨텍스트 사용은 종종 모호한 작업에 대한 의미론적 이해를 요구합니다.

NoLiMa는 컨텍스트 길이가 증가할수록 비어휘적 매칭이 모델에게 도전이 된다는 것을 보여줬습니다. 이 작업은 모델이 잠재적 연관성을 추론해야 하는 needle-question 쌍을 활용합니다. 예를 들어:

> 질문: 어떤 캐릭터가 헬싱키에 다녀왔나요?
>
> Needle: 사실, 유키는 키아스마 박물관 옆에 살고 있습니다.

이 질문에 답하려면 모델은 먼저 키아스마 박물관이 헬싱키에 있다는 것을 알아야 하고, 그 다음 잠재적 연관 링크를 만들어야 합니다. 이는 모델의 비어휘적 매칭 능력뿐 아니라 세계 지식도 테스트합니다. NoLiMa의 needle-question 쌍 중 72.4%가 이런 외부 지식을 요구해서, 이 벤치마크는 순수한 비어휘적 매칭보다는 모델이 두 작업을 동시에 처리하는 방법을 테스트하는 것에 가깝습니다.

비어휘적 매칭의 영향을 독립적으로 테스트하는 연구는 여전히 부족합니다. 더구나 "어휘적" 대 "비어휘적"이라는 이분법적 구분은 실제 시나리오에서 질문-응답의 복잡성을 지나치게 단순화합니다. Needle-question 쌍은 유사도 스펙트럼에 존재하지만, 모두 이런 광범위한 카테고리로 분류됩니다.

모델은 종종 방해 요소도 처리해야 하는데, 이로 인해 성능이 저하됩니다.

이 보고서에서는 방해 요소와 관련 없는 콘텐츠를 구분합니다:

- 방해 요소는 needle과 주제적으로 관련이 있지만 질문에 완전히 답하지는 않습니다
- 관련 없는 콘텐츠는 needle과 질문에 관련이 없습니다

이전 연구에서 방해 요소가 비균일한 영향을 미친다고 증명했지만, 대부분의 평가는 짧은 입력 길이와 이전 모델을 대상으로 했습니다. 현재 최신 모델들은 어떤 방해 요소든 신뢰성 있게 처리한다고 주장되지만, 다양한 입력 길이에서 이들의 성능은 광범위하게 테스트되지 않았습니다.

NIAH의 또 다른 미탐구 영역은 haystack 자체입니다. 이는 종종 입력 길이를 확장하는 수단으로만 취급되는데, haystack 콘텐츠 자체가 작업 성능에 영향을 미치지 않는다고 가정하기 때문입니다. 모델이 실제로 haystack 콘텐츠에 민감하지 않다면, haystack의 주제나 서술 흐름을 변경해도 결과에 영향이 없어야 합니다. 하지만 이 가정은 대체로 테스트되지 않은 채로 남아 있습니다.

이러한 요소들의 영향을 조사하기 위해 네 가지 통제된 실험을 설계했습니다:

### Needle-Question 유사도

임베딩을 사용해 needle-question 쌍 간의 코사인 유사도를 계산합니다. 견고성을 위해 다섯 가지 임베딩 모델(text-embedding-3-small, text-embedding-3-large, jina-embeddings-v3, voyage-3-large, all-MiniLM-L6-v2)에서 평균을 냅니다. 입력 길이 증가에 따라 needle-question 유사도가 모델 성능에 미치는 영향을 측정합니다.

### 방해 요소의 영향

높은 유사도의 needle-question 쌍을 가져와 네 개의 방해 요소를 작성합니다. 설정은 다음과 같습니다:

- 기준선: needle만, 방해 요소 없음
- 단일 방해 요소: needle + 무작위로 배치된 하나의 방해 요소
- 다중 방해 요소: needle + 무작위로 배치된 네 개의 방해 요소 전부

입력 길이 증가에 따라 방해 요소가 모델 성능에 미치는 영향을 테스트해 방해 요소와 입력 길이 간의 비균일성을 측정합니다.

### Needle-Haystack 유사도

주제적으로 구분되는 두 개의 haystack인 Paul Graham 에세이와 arXiv 논문을 사용하고, 각각에 해당하는 needle을 작성합니다. Needle-haystack 유사도를 측정하기 위해 haystack을 임베딩하고 각 needle에 대해 상위 5개 청크를 검색한 다음 코사인 유사도 점수의 평균을 냅니다. 견고성을 위해 다섯 가지 다른 임베딩 모델에서 이 과정을 반복합니다.

### Haystack 구조

일반적인 NIAH 설정에서 haystack은 일관된 텍스트의 연결로, 각각 고유한 논리적 아이디어 흐름을 갖습니다. 예를 들어 원래 NIAH 벤치마크는 Paul Graham 에세이 시리즈를 사용하는데, 각 에세이는 논증을 형성하기 위해 아이디어를 구조화해 조직합니다. 이 구조가 모델 성능에 영향을 미치는지 평가하기 위해 두 조건을 비교합니다:

- 원본: 각 발췌문 내에서 아이디어의 자연스러운 흐름 유지
- 셔플됨: 동일한 전체 주제를 유지하면서 논리적 연속성 없이 haystack 전체에서 문장을 무작위로 재배열

주요 발견 사항:

- 모든 실험에서 입력 길이가 증가하면 모델 성능이 일관되게 저하됩니다.
- 낮은 유사도의 needle-question 쌍에서 성능 저하 속도가 빨라집니다.
- 방해 요소는 상대적으로 얼마나 방해가 되는지에 따라 모델 성능에 비균일한 영향을 미칩니다. 입력 길이가 증가하면 이 영향이 더 두드러지며, 다양한 모델이 이를 처리하는 방식에서 차이가 나타납니다.
- Needle-haystack 유사도는 모델 성능에 균일한 영향을 미치지 않아 추가 조사가 필요합니다.
- Haystack의 구조적 패턴은 모델이 긴 입력을 처리하는 방식에 일관되게 영향을 미칩니다.

### 세부 사항

needle 유형, haystack 주제, haystack 구조의 모든 고유한 조합에 대해 각 모델을 다음 조건에서 테스트합니다:

- 8개의 입력 길이
- 11개의 needle 위치

호환되지 않거나(예: o3) 명시적으로 권장되지 않는 경우(예: Qwen의 "thinking mode")를 제외하고 temperature=0으로 각 모델의 최대 컨텍스트 윈도우에서 평가합니다. Qwen 모델에는 YaRN 방법을 적용해 32,768에서 131,072 토큰으로 확장합니다.

해당하는 경우 표준 모드와 "thinking mode" 모두에서 모델을 포함합니다.

부록에 설명된 방법을 사용해 정렬된 GPT-4.1 심사자로 모델 출력을 평가합니다.

모델이 작업 시도를 거부하는 드문 경우가 있습니다(총 194,480개 LLM 호출 중 69개—0.035%). 예를 들어 Claude Opus 4는 때때로 stop_reason="refusal"과 함께 빈 출력을 낼 수 있습니다.

## Needle-Question 유사도

실제 애플리케이션에서 모델은 종종 모호한 작업을 처리하고 정확한 어휘 매칭에 의존하지 않고 관련 정보를 식별해야 합니다. 예를 들어 에이전트에게 검색할 대규모 코퍼스가 포함된 작업을 주면, 사용자는 관련 부분에 대한 정확한 키워드를 거의 지정하지 않습니다. 대신 모델이 관련성을 추론해야 합니다.

임베딩의 코사인 유사도로 정량화되는 needle-question 쌍의 유사도를 변경합니다. Needle-question 유사도가 감소할수록 입력 길이가 증가하면 모델 성능이 더 크게 저하됩니다. 이는 정확한 질문-답변 매칭이 드물고 의미론적 모호성이 긴 입력 처리의 어려움을 가중시키는 더 현실적인 시나리오를 반영합니다.

### 실험

haystack 콘텐츠를 두 영역에서 가져옵니다: Paul Graham 에세이(원래 NIAH 실험처럼)와 arXiv 논문. 각 haystack 주제(PG 에세이, arXiv)에 대해 먼저 질문과 needle 작성을 안내할 공통 테마를 결정합니다.

클러스터링을 사용해 주어진 코퍼스에 나타나는 가장 일반적인 주제를 식별합니다:

1. 문서를 1-3 문장 청크로 분할

2. text-embedding-3-large로 각 청크를 임베딩

3. 다음 파라미터로 UMAP 차원 축소: n_neighbors=30, min_dist=0.05, n_components=50, random_state=42

4. 다음 파라미터로 HDBSCAN 클러스터 생성: min_cluster_size=10, min_samples=15

5. 최대 한계 관련성(MMR)으로 가장 큰 클러스터에서 20개의 대표 청크 획득

6. 가장 큰 클러스터를 수동으로 검토해 테마와 스타일 결정

이 방법으로 PG 에세이의 일반적인 주제로 글쓰기 조언을 식별했으며, 종종 일화 형태입니다. arXiv 논문에서는 특히 재순위화 관련 정보 검색을 일반적인 주제로 식별했습니다.

각 주제에 해당하는 질문을 작성합니다:

> PG 에세이: "대학 동기에게서 받은 최고의 글쓰기 조언은 무엇이었나요?"
>
> arXiv 논문: "과학 분야에서 선호되는 저지연 재순위화 도구는 무엇인가요?"

needle을 작성하기 전에 이 질문들에 대한 답이 haystack 콘텐츠에 존재하지 않는지 확인합니다:

1. 이전에 계산된 haystack 청크 임베딩을 벡터 데이터베이스에 저장합니다.
2. 질문 임베딩으로 해당 벡터 데이터베이스에서 상위 10개 결과를 쿼리합니다.
3. 이 결과들이 주어진 질문에 답하지 않는지 수동으로 검토합니다.

이를 통해 대안 답변이 존재하지 않고, 잘못된 답변은 모델 환각 때문임을 보장해 공정한 테스트 환경을 설정합니다.

각 질문에 대해 근사 예측으로 검증하는 대규모 클러스터에 속하는 8개의 needle을 작성합니다. 글쓰기/검색 클러스터에 >0.9 확률로 속하는 needle은 haystack에 주제적으로 혼합된다고 간주합니다. 데이터 오염을 피하기 위해 이 needle들을 수동으로 작성합니다.

8개의 needle에 대해 다음 방법으로 모호성 수준도 변경합니다:

1. 임베딩 모델로 needle과 질문의 임베딩과 코사인 유사도를 계산합니다.
2. 다섯 가지 임베딩 모델(text-embedding-3-small, text-embedding-3-large, jina-embeddings-v3, voyage-3-large, all-MiniLM-L6-v2)에서 반복합니다.

PG 에세이 주제의 경우 needle은 다섯 가지 임베딩 모델에서 <0.1 표준 편차로 0.445-0.775 needle-question 유사도 범위를 갖습니다. arXiv 주제의 경우 역시 <0.1 표준 편차로 0.521-0.829 needle-question 유사도 범위를 갖습니다.

### 결과

낮은 유사도의 needle-question 쌍에서 성능이 입력 길이에 따라 더 빠르게 저하되는 명확한 패턴이 나타납니다.

짧은 입력 길이에서 모델은 낮은 유사도 쌍에서도 잘 수행합니다. 고/중 성능 모델에서 가장 명확하게 볼 수 있으며, 이 모델들이 모든 needle-question 쌍에서 이 작업을 성공할 수 있음을 보여줍니다.

더 긴 입력 길이에서 관찰된 성능 저하는 needle-question 쌍의 본질적 난이도 때문이 아닙니다. Needle-question 쌍을 고정하고 관련 없는 콘텐츠의 양만 변경해 입력 크기를 성능 저하의 주요 요인으로 분리합니다.

needle 위치가 성능에 영향을 미치는지도 검토합니다. 11개의 needle 위치에서 테스트한 결과, 이 특정 NIAH 작업에서 성능에 눈에 띄는 변화가 없었습니다.

## 방해 요소의 영향

이전 모델에서 방해 요소가 모델 성능을 저하시키고 비균일한 영향을 미친다고 이미 확립됐습니다. 새로운 모델들은 어떤 방해 요소든 신뢰성 있게 처리한다고 주장하는데, 입력 길이가 증가해도 이것이 사실일까요?

우리 실험에서 방해 요소의 영향과 비균일성이 최신 최고 수준 모델을 포함한 모든 모델에서 입력 길이가 증가하면 증폭된다는 것을 보여줍니다. 또한 다양한 모델 계열이 모호성을 처리하는 방식에서 뚜렷한 행동 차이가 나타납니다.

### 실험

각 haystack 주제(PG 에세이와 arXiv 논문)에서 높은 needle-question 유사도(8개 중 두 번째로 높음)를 가진 needle을 가져와 4개의 방해 요소를 수동으로 작성합니다:

> 질문: "대학 동기에게서 받은 최고의 글쓰기 조언은 무엇이었나요?"
>
> Needle: "대학 동기에게서 받은 최고의 글쓰기 팁은 매주 글을 쓰라는 것이었던 것 같습니다."
>
> 방해 요소:
>
> - "대학 교수에게서 받은 최고의 글쓰기 팁은 매일 글을 쓰라는 것이었습니다."
>
> - "대학 동기에게서 받은 최악의 글쓰기 조언은 각 에세이를 다섯 가지 다른 스타일로 쓰라는 것이었습니다."
>
> - "동기에게서 받은 최고의 글쓰기 조언은 각 에세이를 세 가지 다른 스타일로 쓰라는 것이었습니다, 이건 고등학교 때였습니다."
>
> - "대학 동기에게서 받은 최고의 글쓰기 조언이 각 에세이를 네 가지 다른 스타일로 쓰라는 것이었다고 생각했는데, 지금은 더 이상 그렇지 않습니다."

방해 요소와 함께 8개 needle 전부를 테스트하는 대신, needle이 상대적으로 쉽게 식별되어야 하는 조건을 만들기 위해 높은 needle-question 유사도를 가진 하나의 needle을 사용합니다. 이전 결과에서 높은 needle-question 유사도로 인해 모델이 일반적으로 입력 길이 전반에서 이 needle에서 잘 수행되는 것을 볼 수 있으며, 이를 통해 방해 요소만의 영향을 더 잘 분리하고 측정할 수 있습니다.

세 가지 테스트 조건을 실행합니다:

- 방해 요소 없음 (기준선): Needle만
- 단일 방해 요소: Needle + 하나의 방해 요소 (무작위 배치)
- 다중 방해 요소: Needle + 네 개의 방해 요소 전부, haystack 전체에 무작위 배치

### 결과

단일 방해 요소만으로도 기준선(needle만) 대비 성능이 감소하며, 네 개의 방해 요소를 추가하면 이 저하가 더 심해집니다.

방해 요소가 균일한 영향을 미치지 않는다는 것도 볼 수 있습니다. 예를 들어 arXiv haystack과 PG 에세이 needle 조합에서 방해 요소 3(빨간색)이 다른 방해 요소보다 더 큰 성능 저하를 유발합니다.

이 비균일한 영향을 더 조사하기 위해 4-방해 요소 조건에서 다양한 모델의 실패 시도를 분석합니다. arXiv haystack과 PG 에세이 needle 조합에서 방해 요소 2와 3이 모델 전반에 걸쳐 환각 응답에서 가장 자주 나타납니다.

이런 실패는 모호성을 처리하는 모델별 차이도 드러냅니다. Claude 모델은 일관되게 가장 낮은 환각률을 보입니다. 특히 Claude Sonnet 4와 Opus 4는 보수적이며 불확실할 때 답변을 찾을 수 없다고 명시적으로 밝히며 응답을 자제합니다. 반면 GPT 모델은 가장 높은 환각률을 보이며, 방해 요소가 있을 때 자신감 있지만 잘못된 응답을 자주 생성합니다.

## Needle-Haystack 유사도

긴 컨텍스트 작업에서 관련 없는 컨텍스트는 종종 입력 길이를 확장하기 위한 중립적인 자리 표시자로 취급됩니다. 일반적으로 이 관련 없는 컨텍스트의 내용은 작업에 직접 간섭하지 않는 한 중요하지 않다고 가정합니다.

하지만 자연스러운 질문이 제기됩니다: needle-haystack 유사도가 작업 난이도에 전혀 영향을 미칠까요? 직관적으로 needle이 haystack 콘텐츠와 혼합되면 모델이 needle을 추출하는 데 더 어려움을 겪을 수 있습니다.

연구 결과 needle-haystack 유사도가 모델 성능에 비균일한 영향을 미친다는 것을 보여줍니다.

### 실험

needle-question 유사도 실험의 needle을 사용해 needle-haystack 유사도의 영향을 테스트하는 실험을 설정합니다.

haystack을 임베딩하고 각 needle에 대해 가장 유사한 상위 5개 청크를 검색한 다음 코사인 유사도 점수의 평균을 내어 needle-haystack 유사도를 측정합니다. 견고성을 위해 다섯 가지 다른 임베딩 모델에서 이 과정을 반복합니다.

PG 에세이 haystack에서 PG 에세이 needle은 0.101 변동성으로 평균 0.529 needle-haystack 유사도 점수를 가지며, arXiv needle은 0.111 변동성으로 평균 0.368 needle-haystack 유사도를 갖습니다. 반대로 arXiv haystack에서 arXiv needle은 0.0858 변동성으로 평균 0.654 needle-haystack 유사도를 가지며, PG 에세이 needle은 0.105 변동성으로 더 낮은 0.394 needle-haystack 유사도를 갖습니다.

각 haystack에서 의미적으로 유사한 needle을 관련 없는 needle과 비교해 테스트합니다. 예를 들어 Paul Graham 에세이 haystack에 PG 에세이와 arXiv needle을 모두 배치해 두 조건을 비교합니다.

### 결과

두 종류의 haystack 유형인 Paul Graham 에세이와 arXiv 논문에서 PG 에세이와 arXiv needle 모두를 테스트합니다. Paul Graham 에세이 haystack에서 arXiv needle이 PG 에세이 needle보다 상당히 더 좋은 성능을 보입니다. 다시 말해 needle이 haystack과 의미적으로 혼합되지 않을 때 모델이 더 좋은 성능을 보입니다. 하지만 arXiv haystack에서는 arXiv와 PG 에세이 needle 간에 최소한의 성능 차이만 나타납니다.

두 주제만으로 테스트하는 것은 높은 needle-haystack 유사도가 이 작업에서 모델 성능을 저하시킨다는 일반화 가능한 결론을 도출하기에 충분하지 않습니다. 하지만 이는 긴 컨텍스트 처리의 비균일한 특성을 강조합니다. 작업 구조와 needle-question 유사도가 일정해도 needle과 haystack 간의 의미적 유사도를 변경하면 결과에 영향을 미칠 수 있습니다. 이는 긴 컨텍스트 벤치마크에서 탐구되지 않은 영역과 향후 연구를 위한 의미 있는 방향을 지적합니다.

## Haystack 구조

needle-haystack 유사도 외에도 haystack의 구조적 패턴도 고려합니다.

haystack이 일관된 에세이로 구성되어 있으면 무작위로 삽입된 needle이 아이디어의 논리적 흐름을 방해해 더 눈에 띌 수 있습니다. 반면 무작위로 정렬된 문장의 셔플된 haystack에서는 전체 컨텍스트가 구조를 갖지 않아 needle이 더 쉽게 혼합될 수 있습니다. 이는 모델이 컨텍스트의 논리적 흐름에 민감하며 구조화되고 순서에 민감한 방식으로 처리한다는 가정을 따릅니다.

놀랍게도 구조적 일관성이 모델 성능을 일관되게 저하시킨다는 것을 발견했습니다.

직관에 반하는 것처럼 보이지만, haystack이 아이디어의 논리적 흐름을 유지하면 모델이 더 나쁜 성능을 보입니다. haystack을 셔플하고 로컬 일관성을 제거하면 성능이 일관되게 향상됩니다.

### 실험

haystack 구조의 영향을 평가하기 위해 두 가지 변형을 만듭니다:

1. 원본: 각 발췌문 내에서 아이디어의 자연스러운 흐름 유지
2. 셔플됨: 동일한 전체 주제를 유지하지만 논리적 연속성 없이 haystack 전체에서 문장을 무작위로 재배열

### 결과

18개 모든 모델과 needle-haystack 구성에서 모델이 논리적으로 구조화된 것보다 셔플된 haystack에서 더 좋은 성능을 보이는 일관된 패턴이 나타납니다.

이 결과는 모델의 내부 처리에 대해 몇 가지 시사점을 줄 수 있습니다: 입력의 구조적 패턴이 특히 입력 길이가 증가할 때 어텐션 메커니즘 적용 방식에 영향을 미칠 수 있습니다.

이 보고서의 범위를 벗어나지만, 어텐션이 입력 구조에 어떻게 영향받는지에 대한 해석 가능성 연구의 잠재적 방향을 지적합니다. 입력 길이 증가와 함께 발생하는 이런 구조적 영향을 이해하면 긴 컨텍스트 실패 패턴을 설명하는 데 도움이 될 수 있습니다.

## LongMemEval

더 현실적인 설정에서 이 모델들을 평가하기 위해 대화형 질문-응답용 긴 컨텍스트 벤치마크인 LongMemEval을 사용합니다.

채팅 어시스턴트에 긴 입력을 사용하는 것은 후속 채팅에 관련 기록을 유지하는 일반적인 접근 방식입니다. 채팅 어시스턴트에 "메모리"를 통합하는 순진한 접근 방식은 후속 채팅의 프롬프트에 전체 채팅 기록을 포함하는 것입니다. 이를 위해 모델은 일반적으로 한 번의 호출에서 두 가지 작업을 수행해야 합니다: 대화 기록의 관련 부분을 찾고(검색), 수신되는 쿼리에 유용한 방식으로 합성(추론)하는 것입니다.

이상적인 경우 모델에게는 관련 부분만 제공되어 추론에만 집중할 수 있습니다. 관련 없는 컨텍스트를 추가하면 무엇이 관련 있는지 식별하는 추가 단계가 생겨 모델이 동시에 두 가지 작업을 수행해야 합니다.

두 가지 조건으로 입력 길이 증가와 함께 이 추가 단계 추가의 효과를 체계적으로 테스트합니다:

1. 집중된 입력: 관련 부분만 포함하므로 모델이 간단한 추론만 하면 됩니다.

2. 전체 입력: 관련 없는 컨텍스트를 포함하는 전체 113k 토큰 LongMemEval 입력을 활용합니다. 이 경우 모델은 추론 외에도 긴 컨텍스트에서 검색을 수행해야 합니다.

모델이 집중된 입력에서 성공할 수 있는 능력이 매우 높다는 것을 확인한 다음, 전체 입력에서 일관된 성능 저하를 관찰합니다. 이 성능 하락은 관련 없는 컨텍스트를 추가하고 그에 따라 검색의 추가 단계를 추가하면 모델의 신뢰할 수 있는 성능 유지 능력에 상당한 영향을 미친다는 것을 시사합니다.

### 실험

사용자와 어시스턴트 간의 채팅 기록이 주어지면, 모델의 작업은 해당 채팅 기록의 일부와 관련된 질문에 답하는 것입니다.

LongMemEval_s를 사용하고 knowledge update, temporal reasoning, multi-session 범주에 해당하는 작업을 필터링합니다. 그런 다음 일부 질문이 너무 모호하거나 답할 수 없으므로 데이터셋을 수동으로 정리해 38개 프롬프트를 필터링하고 총 306개 프롬프트로 마무리합니다. 이 프롬프트들은 평균 약 113k 토큰입니다.

이 긴 프롬프트들은 대부분 질문과 관련 없는 콘텐츠로 구성되며, 때로는 질문과 관련 있어 보이는 방해 요소도 포함합니다. 이 긴 프롬프트에 대한 모델 성능을 질문에 답하기 위한 관련 부분만 포함하는 집중된 버전과 비교합니다.

집중된 프롬프트는 원래 레이블이 지정된 데이터셋과 수동 조정에서 파생된 평균 약 300 토큰입니다.

모델 출력은 정렬된 LLM 심사자(인간 판단과 >99% 정렬된 GPT-4.1)로 평가했습니다.

### 결과

모든 모델에서 전체 프롬프트보다 집중된 프롬프트에서 상당히 높은 성능을 봅니다.

Claude 모델은 집중된 프롬프트와 전체 프롬프트 성능 간에 가장 두드러진 격차를 보입니다. 이 불일치는 주로 모호성에서 발생하는 응답 자제로 인해 모델 불확실성으로 이어지며, NIAH에서 방해 요소에 대한 이 모델 계열의 행동과 유사합니다. 이 행동은 Claude Opus 4와 Sonnet 4에서 가장 분명하며, 모호성 하에서 특히 보수적이어서 이전 Claude 모델보다 전체 프롬프트에서 더 낮은 성능을 보입니다.

> 질문: 정원 가꾸기 워크숍에 참석한 날과 토마토 모종을 심은 날 사이에 며칠이 지났나요?
>
> 정답: 6일. 마지막 날을 포함하면 7일도 허용됩니다.
>
> 모델 출력: 정원 가꾸기 워크숍과 토마토 모종 심기 사이의 일수를 확인할 수 없습니다. 채팅 기록에 이 이벤트들의 구체적인 날짜가 제공되지 않았기 때문입니다.

GPT, Gemini, Qwen 모델 계열에서도 집중된 프롬프트에서 더 강한 성능의 추세가 유지됩니다. thinking 모드를 지원하는 모델의 경우 활성화 시 집중된 프롬프트와 전체 프롬프트 모두에서 눈에 띄는 향상을 봅니다. 하지만 최신 모델에서 전체 추론 기능을 사용해도 두 입력 길이 간의 성능 격차는 여전히 존재합니다.

특정 질문 유형 간의 패턴도 관찰됩니다. non-thinking 모드에서 모델은 일반적으로 집중된 프롬프트와 전체 프롬프트 모두에서 knowledge-update에서 가장 잘 수행하고, 그 다음 multi-session, temporal reasoning 순입니다. 하지만 thinking이 활성화되면 이 순위가 knowledge update, temporal-reasoning, multi-session으로 바뀝니다.

## 반복 단어

이전 실험에서는 입력 길이만이 모델 성능에 어떤 영향을 미치는지 탐구합니다. 하지만 출력 길이가 입력에 따라 확장되면 어떻게 될까요? 이 모델들은 자기 회귀적이므로 모델의 출력도 입력에 속합니다. 각 토큰은 해당 시점까지의 입력과 생성된 토큰에 조건부로 생성됩니다.

문자열을 n번 반복하는 기본 프로그램을 생각해 보세요—매번 같은 출력을 생성합니다. 이렇게 사소한 작업에 대해 이 모델들도 똑같이 신뢰할 수 있을 것으로 예상하며, 우리는 이들을 컴퓨팅 시스템으로 취급하고 싶습니다.

하지만 연구 결과 이런 간단한 작업에서도 입력과 출력 길이를 모두 포함하는 컨텍스트 길이가 증가하면 모델 성능이 비균일해진다는 것을 보여줍니다.

### 실험

모델이 반복 단어의 시퀀스를 복제해야 하는 통제된 작업을 설계하며, 특정 위치에 단일 고유 단어가 삽입됩니다. 프롬프트는 모델에게 입력 텍스트를 정확히 재현하도록 명시적으로 지시합니다.

예시 프롬프트:

> 다음 텍스트를 그대로 복제하여 정확히 같은 텍스트를 출력하세요: apple apple apple apple **apples** apple apple apple apple apple apple apple apple apple apple apple apple apple apple apple apple apple apple apple apple

주어진 단어 조합에 대해 1090가지 컨텍스트 길이와 고유 단어 인덱스 변형을 만듭니다:

- 단어 수: 25, 50, 75, 100, 250, 500, 750, 1000, 2500, 5000, 7500, 10000
- 인덱스:
  - num_words <= 100인 경우 모든 가능한 위치
  - 그 외: num_words // 100 간격

다음 단어 조합에 대해 이 작업을 수행합니다:

- 일반 단어: "apple" | 고유 단어: "apples"
- 일반 단어: "apples" | 고유 단어: "apple"
- 일반 단어: "golden" | 고유 단어: "Golden"
- 일반 단어: "orange" | 고유 단어: "run"
- 일반 단어: "orange" | 고유 단어: "San Francisco"
- 일반 단어: "San Francisco" | 고유 단어: "sf"
- 일반 단어: "Golden Gate Bridge" | 고유 단어: "Golden Gate Park"

*참고: "San Francisco" = 1 단어, "Golden Gate Bridge/Park" = 1 단어*

모델 구성:

- max_output_tokens = input_tokens * 2 (모델의 최대 출력 토큰 제한까지, 일반적으로 이전 모델의 경우 더 낮음)
- temperature = 0
- thinking = max(0, minimum_thinking_budget)

thinking 예산을 0 또는 최소값(예: Gemini 2.5 Pro의 경우 128 토큰)으로 설정해 추론 모델을 고려합니다. 토큰 기반 thinking 예산을 지원하지 않고 평가 전반에 걸쳐 일관성을 유지하는 데 필수적인 고정 출력 길이로 구성할 수 없는 OpenAI의 o3는 제외합니다.

점수는 정규화된 레벤슈타인 거리로 계산합니다.

모델이 작업을 시도하지 않는 경우가 있으며, 다음과 같이 결정합니다:

- 중지 이유가 있는 빈 출력 (예: GPT-3.5 turbo의 finish_reason='content_filter')

- 비어 있지 않은 출력이지만 유효하지 않은 출력:

- 시도 없이 순수한 관찰:

> 텍스트에 불일치가 있습니다. 원본 텍스트에서 "apples"라는 단어가 "apple" 대신 한 번 나타나며, 텍스트 블록의 약 89번째 또는 90번째 줄에 있는 것 같습니다. 정확히 같은 텍스트를 복제해달라고 요청하셨으므로 이 차이를 지적해야 합니다. 다음 중 원하시는 것을 선택해 주세요:
>
> 1. 표시된 대로 정확히 복제 ("apples"의 한 인스턴스 포함)
> 2. 패턴에 맞게 "apple"로 수정
> 3. 단순히 있는 그대로 복제 진행 선택해 주세요.

- 답변 거부:

> 죄송합니다만, 그건 도와드릴 수 없습니다

- 무작위 출력:

> -\n-\n--\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-...

이런 경우를 제외하고, 거부 비율과 일반적인 패턴을 결과에 별도로 기록합니다. 다음과 같은 시작 문구가 있는 경우를 포함해 작업이 시도된 경우만 포함합니다:

> 텍스트에 불일치가 있습니다. 한 지점에서 "apple"이 "apples"('s' 포함)로 변경됩니다. 제공된 대로 정확히 텍스트를 복제하겠습니다:
>
> apple apple apple apple apple apple apple apple apple...

이런 인스턴스에서 정확한 지시를 따르지 않은 것에 대해 모델에 약간의 패널티를 주기 위해 동일한 채점 프로세스를 사용합니다.

finish_reason='content_filter'로 인해 60.29%의 작업에서 출력 생성을 거부했으므로 GPT-3.5 turbo는 완전히 제외합니다.

여러 차원에 걸쳐 출력을 분석합니다:

- 정규화된 레벤슈타인 거리

- 고유 단어의 존재 및 위치
  - 정확: 고유 단어가 존재하고 올바른 인덱스에 나타남
  - 잘못된 위치: 고유 단어가 나타나지만 잘못된 인덱스에

- 단어 수 차이 (입력 단어 수 - 생성된 단어 수)

### 결과

컨텍스트 길이가 증가하면 모든 모델에서 성능이 일관되게 저하됩니다. 이 실험에서 입력 길이는 출력 길이에 직접 비례하며, 출력 길이가 비교적 짧은 길이로 고정된 이전 테스트와 다릅니다. 이 설정을 통해 모델이 긴 시퀀스를 신뢰성 있게 재현하는 능력을 평가할 수 있습니다.

또한 모든 모델 계열에서 모델이 작업을 시도하지 않는 패턴을 관찰합니다.

Sonnet 3.5(빨간색)가 최대 출력 토큰 수인 8192까지 새로운 Claude 모델들을 능가합니다. Opus 4(파란색)는 가장 느린 저하 속도를 보이지만, 이 계열에서 작업을 거부하는 유일한 모델이기도 합니다(시도의 2.89%).

Opus 4의 일반적인 패턴은 모델이 진행 여부를 결정하기 전에 관찰을 하고, 시도하거나 시도하지 않는 것입니다. 이 행동은 일반적으로 2500 단어부터 나타납니다.

Opus 4가 다음과 같은 이유로 작업 시도를 거부하는 경우도 있습니다:

1. 저작권 있는 자료 생성 위험

> 요청하신 대로 정확한 텍스트를 복제할 수 없습니다. "San Francisco"의 반복처럼 보이지만, 요청 시 텍스트를 그대로 재현하는 것은 피해야 합니다. 다른 맥락에서 저작권 있는 자료를 재현하는지 테스트하는 데 잠재적으로 사용될 수 있기 때문입니다.
>
> 대신, 텍스트가 "sf" 다음에 "San Francisco"가 24번 반복된다고 말씀드릴 수 있습니다.

2. 반복 시퀀스의 불일치

> 제공하신 텍스트에 오류가 있습니다. 한 인스턴스에서 "San Francisco"가 "San Francisco sf San Francisco"로 나타납니다(중간에 추가 "sf"가 있음). 정확한 텍스트를 복제해달라고 요청하셨으므로, 소스에 이 불일치가 포함되어 있어 완벽한 복제를 제공할 수 없습니다.
>
> 텍스트는 "San Francisco"가 여러 번 반복되지만, 그 안에 해당 오류가 포함되어 있습니다.

위치 정확도도 측정합니다: 고유 단어가 올바른 위치에 나타나는지 여부. 정확도는 고유 단어가 시퀀스 시작 부분에 배치될 때 가장 높으며, 특히 입력 길이가 증가할수록 그렇습니다.

추가로 컨텍스트 길이가 증가하면 모델이 종종 출력 토큰 제한에 도달할 때까지 반복 단어를 생성합니다. 입력과 출력 단어 수 차이를 계산해 이를 정량화합니다:

- 양수 = 모델이 과소 생성
- 음수 = 모델이 과대 생성

GPT 모델 계열에서 GPT-4.1의 거부율은 2.55%입니다. 이런 거부는 일반적으로 약 2500 단어부터 시작되며, "죄송합니다만, 그건 도와드릴 수 없습니다"와 같은 응답이 나타납니다.

GPT-4 turbo의 경우 약 500 단어에서 로컬 성능 피크를 관찰합니다. 50에서 250 단어 사이에서 모델은 과대 생성하는 경향이 있지만(출력 제한까지 일반 단어를 반복), 500 단어에서는 단어 수가 더 정확해집니다. 하지만 이 지점을 넘어서면 입력과 출력 단어 수 사이의 양수 차이에서 볼 수 있듯이 과소 생성하기 시작합니다.

위치 정확도는 유사한 추세를 따르며, GPT 모델도 고유 단어가 입력 초기에 나타날 때 올바르게 배치할 가능성이 더 높습니다.

이 계열에서 더 많은 모델별 행동도 주목합니다.

GPT-4.1 mini는 모든 작업을 시도하지만, "Golden Gate Bridge"/"Golden Gate Park" 조합에서 때때로 무작위 단어를 생성합니다. 무작위 출력은 입력에 없는 단어 또는 단어 시퀀스로 정의됩니다.

모델은 입력에 없는 "Golden Golden"과 "Gate Gate" 같은 중복 단어를 출력합니다(입력에는 "Golden Gate Bridge"와 "Golden Gate Park"만 포함).

이런 중복 단어는 고유 단어의 위치가 아니라 텍스트의 나중 위치에 나타납니다.

GPT-4.1 nano는 "San Francisco" / "sf" 쌍에서 유사한 행동을 보이며, 때때로 소문자 "san"을 출력합니다.

> 모델 출력 스니펫:
>
> San Francisco San Francisco San Francisco San Francisco San Francisco San Francisco San Francisco San Francisco San Francisco **san** Francisco **san** Francisco **san** Francisco **san** Francisco
>
> 정답 참조의 해당 부분:
>
> San Francisco San Francisco San Francisco San Francisco San Francisco San Francisco San Francisco San Francisco San Francisco San Francisco San Francisco San Francisco San Francisco

이런 무작위 단어에서 위치와 관련된 구조의 힌트를 발견합니다. 고유 단어의 위치와 무작위 단어가 나타나기 시작하는 위치 사이의 상관관계를 관찰하며, 이는 향후 조사의 방향이 될 수 있습니다.

GPT-4 Turbo는 이 계열에서 가장 가변적인 출력을 가지며, 무작위 출력을 생성하는 경향이 더 크고 더 다양한 세트를 갖습니다.

일반적으로 컨텍스트 길이가 증가하면 모델 전반에 걸쳐 성능 저하를 봅니다. Gemini 2.5 Pro(파란색)의 경우 50 단어에서 모델이 생성해야 할 것보다 적은 단어를 생성해 더 낮은 시작점을 관찰합니다.

"apples" / "apple"에서 Gemini 2.5 Flash를 제외한 이 계열의 모든 단어 조합과 모델에서 입력에 없는 무작위 단어가 생성되는 것을 관찰합니다. 이는 일반적으로 약 500-750 단어부터 시작되며, Gemini 2.5 Pro가 가장 큰 가변성을 보이고, 2.0 Flash, 2.5 Flash 순입니다.

> "golden" | "Golden" (2,500 단어):
>
> \- - "I'-a-le-le-le-le-le-le-'a-le-le-le-le-le-le-le--le-le-le-le-le-le-le...
>
> "orange" | "run" (10,000 단어):
>
> orange orange orange--g.-g/2021/01/20/orange-county-california-sheriff-deputies-wore...

Qwen3-8B에서만 비시도를 관찰하며, 이는 4.21%의 작업을 차지합니다. 이 모델에서 약 5000 단어부터 무작위 출력을 관찰합니다:

> 알겠습니다, 잠시 쉬겠습니다. 알려주세요, 지금 기분이 안 좋아요. 좀 쉬어야 해요. 어딘가로 가서 신선한 공기를 마시려고요. 해변에 가거나 어딘가에서 쉴 수도 있어요. 모르겠지만, 쉬어야 해요. 알려주세요, 지금 기분이 안 좋아요. 좀 쉬어야 해요. 어딘가로 가서 신선한 공기를 마시려고요. 해변에 가거나 어딘가에서 쉴 수도 있어요. 모르겠지만, 쉬어야 해요. 알려주세요, 지금 기분이 안 좋아요. 좀 쉬어야 해요. 어딘가로 가서 신선한 공기를 마시려고요. 해변에 가거나 어딘가에서 쉴 수도 있어요. 모르겠지만, 쉬어야 해요. 알려주세요, 지금 기분이 안 좋아요. 좀 쉬어야 해요. 어딘가로 가서 신선한 공기를 마시려고요. 해변에 가거나 어딘가에서 쉴 수도 있어요. 모르겠지만, 쉬어야 해요. 알려주세요, 지금 기분이 안 좋아요. 좀 쉬어야 해요. 어딘가로 가서 신선한 공기를 마시려고요. 해변에 가거나 어딘가에서 쉴 수도 있어요. 모르겠지만, 쉬어야 해요. 알려주세요, 지금 기분이 안 좋아요. 좀 쉬어야 해요. 어딘가로 가서 신선한 공기를 마시려고요. 해변에 가거나 어딘가에서 쉴 수도 있어요. 모르겠지만, 쉬어야 해요. 알려주세요, 지금 기분이 안 좋아요. 좀 쉬어야 해요. 어딘가로 가서 신선한 공기를 마시려고요...

## 한계 및 향후 연구

이 실험에서 LLM이 간단한 작업에서도 컨텍스트 길이에 따라 일관되지 않은 성능을 보인다는 것을 보여줍니다. 하지만 이 평가는 실제 사용 사례를 완전히 포괄하지 않습니다. 실제로 긴 컨텍스트 애플리케이션은 종종 훨씬 더 복잡해 합성이나 다단계 추론을 요구합니다. 연구 결과에 기반하면 그런 조건에서 성능 저하가 훨씬 더 심각할 것으로 예상됩니다.

연구 결과는 긴 컨텍스트 평가에 대한 향후 연구에도 시사점이 있습니다. 긴 컨텍스트 벤치마크에 대한 이전 연구에서도 언급된 일반적인 한계는 입력 길이와 작업 난이도를 혼동하는 경향인데, 더 긴 입력이 종종 더 복잡한 추론을 도입하기 때문입니다. 우리는 입력 길이를 요소로 분리하고 작업 난이도를 상수로 유지하도록 실험을 집중합니다. 향후 연구의 중요한 방향은 모델의 성능 저하가 작업 자체의 본질적 난이도에서 비롯되는지, 긴 컨텍스트를 효과적으로 처리하는 능력에서 비롯되는지 분리하는 것입니다.

또한 이 성능 저하의 메커니즘을 설명하지 않습니다. 관찰에 따르면 관련 정보의 배치나 반복 같은 컨텍스트의 구조적 속성이 모델 행동에 영향을 미칠 수 있지만, 왜 그런지에 대한 확실한 답은 없습니다. 이런 영향을 조사하려면 이 보고서의 범위를 벗어나는 기계적 해석 가능성에 대한 더 깊은 조사가 필요합니다.

더 넓게 연구 결과는 컨텍스트 엔지니어링의 중요성을 지적합니다: 모델의 컨텍스트 윈도우를 신중하게 구성하고 관리하는 것. 정보가 모델의 컨텍스트에서 어디에 어떻게 제시되는지가 작업 성능에 강하게 영향을 미쳐, 모델 성능을 최적화하기 위한 향후 연구의 의미 있는 방향이 됩니다.

## 결론

이 실험을 통해 LLM이 입력 길이 전반에 걸쳐 일관된 성능을 유지하지 않는다는 것을 보여줍니다. 비어휘적 검색이나 텍스트 복제만큼 간단한 작업에서도 입력 길이가 증가하면 성능의 비균일성이 증가합니다.

연구 결과는 현재 벤치마크를 넘어서는 더 엄격한 긴 컨텍스트 평가의 필요성과 컨텍스트 엔지니어링의 중요성을 강조합니다. 관련 정보가 모델의 컨텍스트에 있는지 여부만이 중요한 것이 아닙니다. 더 중요한 것은 그 정보가 어떻게 제시되는가입니다. 가장 유능한 모델조차도 이에 민감하며, 효과적인 컨텍스트 엔지니어링이 신뢰할 수 있는 성능에 필수적임을 보여줍니다.

## 각주

[1] (2025년 7월 16일) Kiran Vodrahalli(Google Deepmind)가 Latent List 인사이트 추가 및 명확화

[2] 예시 원본 출처: https://arxiv.org/pdf/2410.10813

## 참고 문헌

[1] Kamradt, G. (2023). Needle In A Haystack - Pressure Testing LLMs [GitHub Repository]. Link

[2] Wu, D., Wang, H., Yu, W., Zhang, Y., Chang, K.-W., and Yu, D. (2025). LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive Memory. arXiv preprint arXiv:2410.10813. Link

[3] Gemini Team, Georgiev, P., Lei, V. I., Burnell, R., Bai, L., Gulati, A., Tanzer, G., Vincent, D., Pan, Z., Wang, S., et al. (2024). Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530. Link

[4] OpenAI, Kumar, A., Yu, J., Hallman, J., Pokrass, M., Goucher, A., Ganesh, A., Cheng, B., McKinzie, B., Zhang, B., Koch, C., et al. (2025). Introducing GPT-4.1 in the API. Link

[5] Meta AI, (2025). The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation. Link

[6] Modarressi, A., Deilamsalehy, H., Dernoncourt, F., Bui, T., Rossi, R. A., Yoon, S., and Schütze, H. (2025). NoLiMa: Long-Context Evaluation Beyond Literal Matching. arXiv preprint arXiv:2502.05167. Link

[7] Fu, H. Y., Shrivastava, A., Moore, J., West, P., Tan, C., and Holtzman, A. (2025). AbsenceBench: Language Models Can't Tell What's Missing. arXiv preprint arXiv:2506.11440. Link

[8] Vodrahalli, K., Ontanon, S., Tripuraneni, N., Xu, K., Jain, S., Shivanna, R., Hui, J., Dikkala, N., Kazemi, M., Fatemi, B., et al. (2024). Michelangelo: Long Context Evaluations Beyond Haystacks via Latent Structure Queries. arXiv preprint arXiv:2409.12640. Link

[9] openai. (2025). mrcr [Dataset]. Hugging Face. Link

[10] openai. (2025). graphwalks [Dataset]. Hugging Face. Link

[11] Shi, F., Chen, X., Misra, K., Scales, N., Dohan, D., Chi, E., Schärli, N., and Zhou, D. (2023). Large Language Models Can Be Easily Distracted by Irrelevant Context. arXiv preprint arXiv:2302.00093. Link

[12] jamescalam. (2024). ai-arxiv2 [Dataset]. Hugging Face. Link

[13] Peng, B., Quesnelle, J., Fan, H., and Shippole, E. (2023). YaRN: Efficient Context Window Extension of Large Language Models. arXiv preprint arXiv:2309.00071. Link

[14] McInnes, L., Healy, J., and Melville, J. (2020). UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction. arXiv preprint arXiv:1802.03426. Link

[15] Campello, R. J. G. B., Moulavi, D., and Sander, J. (2013). Density-Based Clustering Based on Hierarchical Density Estimates. In Pei, J., Tseng, V. S., Cao, L., Motoda, H., and Xu, G. (Eds.), Advances in Knowledge Discovery and Data Mining (PAKDD 2013), Lecture Notes in Computer Science, vol 7819. Springer, Berlin, Heidelberg. Link

## 부록

정리된 LongMemEval 데이터셋과 사용된 needle/방해 요소는 여기서 다운로드할 수 있습니다.

### LLM 심사자 정렬:

NIAH와 LongMemEval 실험의 출력을 평가하기 위해 LLM 심사자를 사용합니다. 이 심사자들은 다음 프로세스로 인간 판단에 보정됩니다:

1. 모델 출력의 하위 집합을 수동으로 잘못됨/정확함으로 레이블 지정 (NIAH의 경우 ~500개 출력, LongMemEval의 경우 ~600개 출력)

2. GPT-4.1로 동일한 모델 출력 하위 집합을 잘못됨/정확함으로 레이블 지정.

3. 인간 판단과 모델 판단이 일치하는 비율을 측정하여 정렬 점수를 계산합니다.

4. 불일치에 대한 수동 검토를 기반으로 프롬프트 반복.

5. 정렬 점수 > 0.99가 달성될 때까지 2-4단계 반복.

### 테스트된 모델

컨텍스트 윈도우 또는 thinking_budget 제약으로 인해 모든 18개 모델이 각 실험에 포함되지는 않습니다.

#### Anthropic

- Claude Opus 4
- Claude Sonnet 4
- Claude Sonnet 3.7
- Claude Sonnet 3.5
- Claude Haiku 3.5

#### OpenAI

- o3
- GPT-4.1
- GPT-4.1 mini
- GPT-4.1 nano
- GPT-4o
- GPT-4 Turbo
- GPT-3.5 Turbo

#### Google

- Gemini 2.5 Pro
- Gemini 2.5 Flash
- Gemini 2.0 Flash

#### Alibaba

- Qwen3-235B-A22B
- Qwen3-32B
- Qwen3-8B

### 사용된 임베딩 모델

- text-embedding-3-small
- text-embedding-3-large
- jina-embeddings-v3 (input_type='text-matching')
- voyage-3-large (input_type=None)
- all-MiniLM-L6-v2

---

## 핵심 요약

- **컨텍스트 로트**: 입력 토큰이 증가할수록 LLM 성능이 비균일하게 저하되는 현상으로, 가장 단순한 작업에서도 발생합니다.

- **18개 LLM 평가**: GPT-4.1, Claude 4, Gemini 2.5, Qwen3 등 최신 모델을 포함한 광범위한 평가에서 모든 모델이 긴 컨텍스트에서 성능 저하를 보였습니다.

- **Needle-Question 유사도**: 질문과 답변의 의미적 유사도가 낮을수록 입력 길이 증가에 따른 성능 저하가 더 빠릅니다.

- **방해 요소의 영향**: 방해 요소는 비균일한 영향을 미치며, 이 영향은 입력 길이가 증가하면 증폭됩니다. Claude 모델은 낮은 환각률을, GPT 모델은 높은 환각률을 보였습니다.

- **Haystack 구조**: 놀랍게도 논리적으로 구조화된 텍스트보다 무작위로 셔플된 텍스트에서 모델이 더 좋은 성능을 보였습니다.

- **컨텍스트 엔지니어링의 중요성**: 정보가 컨텍스트에 존재하는지보다 어떻게 제시되는지가 더 중요하며, 효과적인 컨텍스트 엔지니어링이 신뢰할 수 있는 LLM 성능에 필수적입니다.
