---
title: "에이전트를 위한 효과적인 도구 작성법"
originalTitle: "Writing Effective Tools for Agents"
author: "Ken Aizawa"
sourceUrl: "https://www.anthropic.com/engineering/writing-tools-for-agents"
translatedAt: "2026-01-13"
status: "final"
qaScore:
  consistency: 9
  readability: 8
  accuracy: 9
  overall: 8
---

# 에이전트를 위한 효과적인 도구 작성하기 — 에이전트와 함께

[원본 링크](https://www.anthropic.com/engineering/writing-tools-for-agents)

**게시일:** 2025년 9월 11일

에이전트는 제공받는 도구만큼만 효과적입니다. 고품질 도구와 평가를 작성하는 방법, 그리고 Claude가 자신의 도구를 스스로 최적화하여 성능을 높이는 방법을 공유합니다.

Model Context Protocol(MCP)을 통해 LLM 에이전트는 실제 업무를 해결하는 데 수백 개의 도구를 활용할 수 있습니다. 그렇다면 이 도구들을 최대한 효과적으로 만들려면 어떻게 해야 할까요?

이 글에서는 다양한 에이전트 AI 시스템의 성능을 개선하는 가장 효과적인 기법들을 설명합니다.

먼저 다음 내용을 다룹니다:

- 도구 프로토타입을 빌드하고 테스트하는 방법
- 에이전트와 함께 도구에 대한 종합적인 평가를 생성하고 실행하는 방법
- Claude Code 같은 에이전트와 협업하여 도구 성능을 자동으로 향상시키는 방법

글 마지막에는 과정에서 발견한 고품질 도구 작성의 핵심 원칙을 정리합니다:

- 구현할 도구와 구현하지 않을 도구 선택하기
- 기능의 명확한 경계를 정의하는 도구 네임스페이싱
- 도구에서 에이전트로 의미 있는 컨텍스트 반환하기
- 토큰 효율성을 위한 도구 응답 최적화
- 도구 설명과 스펙의 프롬프트 엔지니어링

## 도구란 무엇인가?

컴퓨팅에서 결정론적 시스템은 동일한 입력에 항상 동일한 출력을 생성합니다. 반면 에이전트 같은 비결정론적 시스템은 같은 시작 조건에서도 다양한 응답을 생성할 수 있습니다.

전통적으로 소프트웨어를 작성할 때는 결정론적 시스템 간의 계약을 수립합니다. 예를 들어, `getWeather("NYC")` 같은 함수 호출은 매번 정확히 같은 방식으로 뉴욕시의 날씨를 가져옵니다.

도구는 결정론적 시스템과 비결정론적 에이전트 사이의 계약을 반영하는 새로운 종류의 소프트웨어입니다. 사용자가 "오늘 우산을 가져가야 하나요?"라고 물으면, 에이전트는 날씨 도구를 호출하거나, 일반 지식으로 답변하거나, 먼저 위치를 물어볼 수도 있습니다. 때로는 에이전트가 환각을 일으키거나 도구 사용법을 이해하지 못할 수도 있습니다.

이는 에이전트용 소프트웨어를 작성할 때 접근 방식을 근본적으로 재고해야 함을 의미합니다. 다른 개발자나 시스템을 위해 함수와 API를 작성하는 방식이 아니라, 에이전트를 위해 도구와 MCP 서버를 설계해야 합니다.

우리의 목표는 에이전트가 다양한 성공적인 전략으로 도구를 사용하여 광범위한 작업을 효과적으로 해결할 수 있는 영역을 넓히는 것입니다. 다행히 경험상, 에이전트에게 가장 편리한 도구는 인간에게도 놀랍도록 직관적입니다.

## 도구 작성 방법

이 섹션에서는 에이전트와 협업하여 도구를 작성하고 개선하는 방법을 설명합니다. 먼저 빠른 프로토타입을 만들고 로컬에서 테스트합니다. 다음으로, 후속 변경 사항을 측정하는 종합적인 평가를 실행합니다. 에이전트와 함께 도구를 평가하고 개선하는 과정을 반복하면서 실제 업무에서 강력한 성능을 달성할 때까지 진행합니다.

### 프로토타입 빌드하기

에이전트가 어떤 도구를 편리하게 사용하고 어떤 도구를 그렇지 않을지는 직접 해보지 않으면 예측하기 어렵습니다. 먼저 빠른 프로토타입을 만들어 시작하세요. Claude Code로 도구를 작성한다면(원샷(one-shot)으로도 가능), 도구가 의존할 소프트웨어 라이브러리, API, SDK(MCP SDK 포함)에 대한 문서를 Claude에게 제공하면 도움이 됩니다. LLM 친화적인 문서는 일반적으로 공식 문서 사이트의 `llms.txt` 파일에서 찾을 수 있습니다(우리 API의 경우 여기에 있습니다).

도구를 로컬 MCP 서버나 데스크톱 확장(DXT)으로 래핑하면 Claude Code나 Claude Desktop 앱에서 연결하고 테스트할 수 있습니다.

로컬 MCP 서버를 Claude Code에 연결하려면 `claude mcp add <name> <command> [args...]`를 실행하세요.

로컬 MCP 서버나 DXT를 Claude Desktop 앱에 연결하려면 각각 `Settings > Developer` 또는 `Settings > Extensions`로 이동하세요.

도구는 프로그래밍 방식 테스트를 위해 Anthropic API 호출에 직접 전달할 수도 있습니다.

도구를 직접 테스트하여 거친 부분을 식별하세요. 사용자 피드백을 수집하여 도구가 지원할 사용 사례와 프롬프트에 대한 직관을 쌓으세요.

### 평가 실행하기

다음으로, 평가를 실행하여 Claude가 도구를 얼마나 잘 사용하는지 측정해야 합니다. 실제 사용에 기반한 많은 평가 작업을 생성하는 것부터 시작하세요. 결과를 분석하고 도구 개선 방법을 결정하기 위해 에이전트와 협업하는 것을 권장합니다. 이 과정의 전체 내용은 도구 평가 쿡북에서 확인하세요.

**평가 작업 생성하기**

초기 프로토타입으로 Claude Code는 빠르게 도구를 탐색하고 수십 개의 프롬프트-응답 쌍을 만들 수 있습니다. 프롬프트는 실제 사용에서 영감을 받고 현실적인 데이터 소스와 서비스(예: 내부 지식 베이스, 마이크로서비스)를 기반으로 해야 합니다. 도구를 충분한 복잡성으로 스트레스 테스트하지 않는 지나치게 단순하거나 피상적인 "샌드박스" 환경은 피하세요. 강력한 평가 작업은 수십 개의 도구 호출을 필요로 할 수 있습니다.

다음은 강력한 작업의 예시입니다:

- 다음 주에 Jane과 회의를 예약하여 최신 Acme Corp 프로젝트를 논의합니다. 마지막 프로젝트 계획 회의 노트를 첨부하고 회의실을 예약하세요.
- 고객 ID 9182가 한 번의 구매 시도에 세 번 청구되었다고 보고했습니다. 관련 로그 항목을 모두 찾고 다른 고객도 같은 문제의 영향을 받았는지 확인하세요.
- 고객 Sarah Chen이 해지 요청을 제출했습니다. 유지 제안을 준비하세요. (1) 왜 떠나려는지, (2) 어떤 유지 제안이 가장 설득력 있을지, (3) 제안 전에 알아야 할 위험 요소가 있는지 파악하세요.

다음은 약한 작업의 예시입니다:

- 다음 주에 jane@acme.corp와 회의를 예약하세요.
- 결제 로그에서 `purchase_complete`와 `customer_id=9182`를 검색하세요.
- 고객 ID 45892의 해지 요청을 찾으세요.

각 평가 프롬프트는 검증 가능한 응답이나 결과와 짝을 이루어야 합니다. 검증기는 정답과 샘플링된 응답의 단순한 문자열 비교처럼 간단할 수도 있고, Claude를 활용해 응답을 판단하는 고급 방식일 수도 있습니다. 형식, 구두점, 유효한 대안 표현 같은 사소한 차이로 정확한 응답을 거부하는 지나치게 엄격한 검증기는 피하세요.

각 프롬프트-응답 쌍에 대해 선택적으로 에이전트가 호출할 것으로 예상하는 도구를 지정하여 에이전트가 각 도구의 목적을 제대로 파악했는지 측정할 수 있습니다. 하지만 작업을 올바르게 해결하는 여러 유효한 경로가 있을 수 있으므로 전략을 과도하게 지정하거나 과적합하지 마세요.

**평가 실행하기**

직접 LLM API 호출로 프로그래밍 방식으로 평가를 실행하는 것을 권장합니다. 간단한 에이전트 루프(LLM API 호출과 도구 호출을 번갈아 감싸는 `while` 루프)를 사용하세요. 각 평가 작업에 루프 하나를 실행합니다. 각 평가 에이전트에게는 단일 작업 프롬프트와 도구를 제공합니다.

평가 에이전트의 시스템 프롬프트에서 구조화된 응답 블록(검증용)뿐만 아니라 추론 및 피드백 블록도 출력하도록 지시하세요. 도구 호출 및 응답 블록 전에 이것들을 출력하도록 지시하면 Chain of Thought(CoT) 행동을 유발하여 LLM의 실질적인 지능을 높일 수 있습니다.

Claude로 평가를 실행하는 경우, 유사한 기능을 기본 제공으로 사용하려면 인터리브드 씽킹(interleaved thinking)을 활성화하세요. 에이전트가 특정 도구를 호출하거나 호출하지 않는 이유를 조사하고 도구 설명과 스펙에서 개선이 필요한 영역을 강조할 수 있습니다.

최상위 정확도 외에도 개별 도구 호출 및 작업의 총 실행 시간, 총 도구 호출 수, 총 토큰 소비량, 도구 오류 같은 다른 메트릭도 수집하세요. 도구 호출을 추적하면 에이전트가 추구하는 일반적인 워크플로우를 파악하고 도구 통합 기회를 발견할 수 있습니다.

**결과 분석하기**

에이전트는 상충되는 도구 설명, 비효율적인 도구 구현, 혼란스러운 도구 스키마 등의 문제를 발견하고 피드백을 제공하는 유용한 파트너입니다. 하지만 에이전트가 피드백과 응답에서 생략한 것이 포함한 것보다 더 중요할 수 있다는 점을 명심하세요. LLM은 항상 의미하는 바를 말하지 않습니다.

에이전트가 막히거나 혼란스러워하는 부분을 관찰하세요. 평가 에이전트의 추론과 피드백(또는 CoT)을 읽어 거친 부분을 식별하세요. 원시 트랜스크립트(도구 호출 및 도구 응답 포함)를 검토하여 에이전트의 CoT에 명시적으로 설명되지 않은 동작을 파악하세요. 행간을 읽으세요. 평가 에이전트가 반드시 정답과 전략을 알고 있는 것은 아닙니다.

도구 호출 메트릭을 분석하세요. 중복 도구 호출이 많으면 페이지네이션이나 토큰 제한 매개변수의 적절한 크기 조정이 필요할 수 있고, 잘못된 매개변수로 인한 도구 오류가 많으면 더 명확한 설명이나 더 나은 예시가 필요할 수 있습니다. Claude 웹 검색 도구를 출시할 때, Claude가 도구의 `query` 매개변수에 불필요하게 `2025`를 추가하여 검색 결과에 편향을 주고 성능을 저하시키는 것을 발견했습니다(도구 설명을 개선하여 올바른 방향으로 이끌었습니다).

### 에이전트와 협업하기

에이전트가 결과를 분석하고 도구를 개선하도록 할 수도 있습니다. 평가 에이전트의 트랜스크립트를 연결해서 Claude Code에 붙여넣기만 하면 됩니다. Claude는 트랜스크립트를 분석하고 많은 도구를 한 번에 리팩터링하는 데 전문가입니다. 예를 들어, 새로운 변경이 이루어질 때 도구 구현과 설명이 자체적으로 일관성을 유지하도록 합니다.

실제로 이 글의 대부분의 조언은 Claude Code로 내부 도구 구현을 반복적으로 최적화한 결과입니다. 우리의 평가는 실제 프로젝트, 문서, 메시지를 포함한 내부 워크플로우의 복잡성을 반영하는 내부 작업 공간에서 생성했습니다.

"훈련" 평가에 과적합되지 않도록 별도의 테스트 세트에 의존했습니다. 이 테스트 세트로 "전문가" 도구 구현—연구원이 수동으로 작성했든 Claude가 생성했든—으로 달성한 것 이상의 추가 성능 개선을 얻을 수 있음을 확인했습니다.

다음 섹션에서는 이 과정에서 배운 것들을 공유합니다.

## 효과적인 도구 작성을 위한 원칙

이 섹션에서는 우리의 학습 내용을 효과적인 도구 작성을 위한 몇 가지 지침 원칙으로 정리합니다.

### 에이전트를 위한 올바른 도구 선택하기

도구가 많다고 항상 더 나은 결과가 나오는 것은 아닙니다. 흔히 관찰되는 오류는 도구가 에이전트에 적합한지 여부와 관계없이 기존 소프트웨어 기능이나 API 엔드포인트를 단순히 래핑하는 것입니다. 에이전트는 전통적인 소프트웨어와 다른 "어포던스(affordance)"를 가지고 있기 때문입니다. 즉, 도구로 취할 수 있는 잠재적 행동을 인식하는 방식이 다릅니다.

LLM 에이전트는 "컨텍스트"가 제한되어 있습니다(한 번에 처리할 수 있는 정보의 양에 제한이 있음). 반면 컴퓨터 메모리는 저렴하고 풍부합니다. 주소록에서 연락처를 검색하는 작업을 생각해 보세요. 전통적인 소프트웨어 프로그램은 연락처 목록을 하나씩 효율적으로 저장하고 처리하며, 각각을 확인한 후 다음으로 넘어갑니다.

하지만 LLM 에이전트가 모든 연락처를 반환하는 도구를 사용한 다음 각각을 토큰 단위로 읽어야 한다면, 관련 없는 정보에 제한된 컨텍스트 공간을 낭비하게 됩니다(주소록에서 각 페이지를 위에서 아래로 읽어가며 연락처를 검색하는 것, 즉 무차별 대입 검색을 상상해 보세요). 에이전트와 인간 모두에게 더 좋고 자연스러운 접근 방식은 먼저 관련 페이지로 건너뛰는 것입니다(아마도 알파벳 순으로 찾아서).

특정 고영향 워크플로우를 대상으로 하는 몇 가지 신중한 도구를 구축하고, 평가 작업과 일치시킨 다음 거기서 확장하는 것을 권장합니다. 주소록의 경우, `list_contacts` 도구 대신 `search_contacts`나 `message_contact` 도구를 구현할 수 있습니다.

도구는 기능을 통합하여 여러 개별 작업(또는 API 호출)을 내부적으로 처리할 수 있습니다. 예를 들어, 도구가 관련 메타데이터로 응답을 풍부하게 하거나 자주 연결되는 다단계 작업을 단일 호출로 처리할 수 있습니다.

다음은 몇 가지 예시입니다:

- `list_users`, `list_events`, `create_event` 도구를 구현하는 대신, 가용성을 찾고 이벤트를 예약하는 `schedule_event` 도구를 구현하세요.
- `read_logs` 도구를 구현하는 대신, 관련 로그 라인과 일부 주변 컨텍스트만 반환하는 `search_logs` 도구를 구현하세요.
- `get_customer_by_id`, `list_transactions`, `list_notes` 도구를 구현하는 대신, 고객의 모든 최신 및 관련 정보를 한 번에 컴파일하는 `get_customer_context` 도구를 구현하세요.

구축하는 각 도구가 명확하고 뚜렷한 목적을 갖도록 하세요. 도구는 에이전트가 동일한 기본 리소스에 접근할 수 있는 인간과 비슷한 방식으로 작업을 세분화하고 해결할 수 있게 해야 하며, 동시에 중간 출력으로 소비될 컨텍스트를 줄여야 합니다.

도구가 너무 많거나 중복되면 에이전트가 효율적인 전략을 추구하기 어려워집니다. 구축할(또는 구축하지 않을) 도구에 대한 신중하고 선택적인 계획이 정말 효과적입니다.

### 도구의 네임스페이싱

AI 에이전트는 수십 개의 MCP 서버와 수백 개의 다른 도구(다른 개발자가 만든 것 포함)에 접근할 수 있습니다. 도구가 기능적으로 중복되거나 목적이 모호하면 에이전트가 어떤 것을 사용해야 할지 혼란스러워합니다.

네임스페이싱(관련 도구를 공통 접두사 아래 그룹화)은 많은 도구 사이의 경계를 구분하는 데 도움이 됩니다. MCP 클라이언트는 때때로 기본적으로 이를 수행합니다. 예를 들어, 서비스별로 도구에 네임스페이싱(예: `asana_search`, `jira_search`)하고 리소스별로(예: `asana_projects_search`, `asana_users_search`) 네임스페이싱하면 에이전트가 적시에 올바른 도구를 선택하는 데 도움이 됩니다.

접두사 기반과 접미사 기반 네임스페이싱 선택이 도구 사용 평가에 무시할 수 없는 영향을 미친다는 것을 발견했습니다. 효과는 LLM에 따라 다르므로 자체 평가에 따라 명명 체계를 선택하세요.

에이전트는 잘못된 도구를 호출하거나, 올바른 도구를 잘못된 매개변수로 호출하거나, 너무 적은 도구를 호출하거나, 도구 응답을 잘못 처리할 수 있습니다. 자연스러운 작업 세분화를 반영하는 이름의 도구를 선택적으로 구현하면 에이전트의 컨텍스트에 로드되는 도구 및 도구 설명 수를 줄이고 에이전트 컨텍스트에서 도구 호출 자체로 계산을 오프로드할 수 있습니다. 이렇게 하면 에이전트가 실수할 전반적인 위험이 줄어듭니다.

### 도구에서 의미 있는 컨텍스트 반환하기

같은 맥락에서, 도구 구현은 에이전트에게 유용한 정보만 반환하도록 주의해야 합니다. 유연성보다 컨텍스트 관련성을 우선시하고, 저수준 기술 식별자(예: `uuid`, `256px_image_url`, `mime_type`)는 피하세요. `name`, `image_url`, `file_type` 같은 필드가 에이전트의 후속 행동과 응답을 직접 알리는 데 훨씬 적합합니다.

에이전트는 또한 암호 같은 식별자보다 자연어 이름, 용어, 식별자를 훨씬 더 성공적으로 다룹니다. 임의의 영숫자 UUID를 더 의미 있고 해석 가능한 언어(또는 심지어 0부터 시작하는 ID 체계)로 변환하는 것만으로도 환각을 줄여 검색 작업에서 Claude의 정밀도가 크게 향상됩니다.

일부 경우에 에이전트는 후속 도구 호출을 트리거하기 위해 자연어와 기술 식별자 출력 모두와 상호작용하는 유연성이 필요합니다(예: `search_user(name='jane')` → `send_message(id=12345)`). 도구에서 간단한 `response_format` 열거형 매개변수를 노출하면 에이전트가 `"concise"`(간결한) 또는 `"detailed"`(상세한) 응답을 제어할 수 있어 두 가지 모두 가능합니다(아래 이미지).

더 큰 유연성을 위해 더 많은 형식을 추가할 수 있습니다. 받고 싶은 정보를 정확히 선택할 수 있는 GraphQL과 유사합니다. 다음은 도구 응답 상세도를 제어하는 ResponseFormat 열거형 예시입니다:

```
enum ResponseFormat {
   DETAILED = "detailed",
   CONCISE = "concise"
}
```

다음은 상세한 도구 응답의 예시입니다(206 토큰):

모든 메타데이터와 ID가 포함된 포괄적인 Slack 스레드 정보를 보여주는 상세 응답 예시.

다음은 간결한 도구 응답의 예시입니다(72 토큰):

ID 없이 스레드 내용만 보여주는 간결한 응답 예시.

Slack 스레드와 스레드 답글은 스레드 답글을 가져오는 데 필요한 고유한 `thread_ts`로 식별됩니다. `thread_ts` 및 기타 ID(`channel_id`, `user_id`)는 추가 도구 호출을 가능하게 하기 위해 `"detailed"` 응답에서 검색할 수 있습니다. `"concise"` 응답은 스레드 내용만 반환하고 ID는 제외합니다. 이 예에서 `"concise"` 응답으로 토큰의 약 1/3을 사용합니다.

도구 응답 구조(예: XML, JSON, Markdown)도 평가 성능에 영향을 미칠 수 있습니다. 만능 해결책은 없습니다. LLM이 다음 토큰 예측으로 훈련되어 훈련 데이터와 일치하는 형식에서 더 잘 수행하는 경향이 있기 때문입니다. 최적의 응답 구조는 작업과 에이전트에 따라 크게 다릅니다. 자체 평가에 따라 최적의 응답 구조를 선택하세요.

### 토큰 효율성을 위한 도구 응답 최적화

컨텍스트의 품질을 최적화하는 것이 중요합니다. 하지만 도구 응답에서 에이전트에게 반환되는 컨텍스트의 양을 최적화하는 것도 중요합니다.

많은 컨텍스트를 사용할 수 있는 도구 응답에 대해 합리적인 기본 매개변수 값과 함께 페이지네이션, 범위 선택, 필터링, 잘림의 조합을 구현하세요. Claude Code의 경우, 기본적으로 도구 응답을 25,000 토큰으로 제한합니다. 에이전트의 효과적인 컨텍스트 길이는 시간이 지남에 따라 증가하겠지만, 컨텍스트 효율적인 도구에 대한 필요성은 유지될 것입니다.

응답을 잘라내기로 선택한 경우, 유용한 지침으로 에이전트를 안내하세요. 지식 검색 작업에서 단일 광범위한 검색 대신 많은 작고 대상이 명확한 검색을 수행하는 것과 같은 더 토큰 효율적인 전략을 추구하도록 직접 권장할 수 있습니다. 마찬가지로, 도구 호출이 오류를 발생시키면(예: 입력 유효성 검사 중), 불투명한 오류 코드나 트레이스백 대신 구체적이고 실행 가능한 개선 사항을 명확하게 전달하도록 오류 응답을 프롬프트 엔지니어링하세요.

다음은 잘린 도구 응답의 예시입니다:

필터나 페이지네이션 사용 안내가 포함된 잘린 결과.

다음은 도움이 되지 않는 오류 응답의 예시입니다:

불투명한 오류 메시지.

다음은 도움이 되는 오류 응답의 예시입니다:

형식 예시가 포함된 명확하고 실행 가능한 오류 메시지.

도구 잘림 및 오류 응답은 에이전트가 더 토큰 효율적인 도구 사용 행동(필터나 페이지네이션 사용)으로 향하도록 하거나 올바른 형식의 도구 입력 예시를 제공할 수 있습니다.

### 도구 설명의 프롬프트 엔지니어링

이제 도구를 개선하는 가장 효과적인 방법 중 하나인 도구 설명과 스펙의 프롬프트 엔지니어링을 다룹니다. 이것들은 에이전트의 컨텍스트에 로드되므로, 집합적으로 에이전트가 효과적인 도구 호출 행동으로 향하도록 할 수 있습니다.

도구 설명과 스펙을 작성할 때, 팀의 신입 사원에게 도구를 어떻게 설명할지 생각해 보세요. 암묵적으로 가지고 있는 컨텍스트—전문적인 쿼리 형식, 틈새 용어 정의, 기본 리소스 간의 관계—를 고려하고 이를 명시적으로 만드세요. 예상 입력과 출력을 명확하게 설명(그리고 엄격한 데이터 모델로 적용)하여 모호성을 피하세요. 특히 입력 매개변수는 모호하지 않게 명명하세요. `user`라는 매개변수 대신 `user_id`를 사용해 보세요.

평가를 통해 프롬프트 엔지니어링의 영향을 더 확신 있게 측정할 수 있습니다. 도구 설명에 대한 작은 개선도 극적인 개선을 가져올 수 있습니다. Claude Sonnet 3.5는 도구 설명에 정밀한 개선을 한 후 SWE-bench Verified 평가에서 최첨단 성능을 달성했으며, 오류율을 크게 줄이고 작업 완료를 개선했습니다.

도구 정의에 대한 다른 모범 사례는 개발자 가이드에서 찾을 수 있습니다. Claude용 도구를 구축하는 경우, 도구가 Claude의 시스템 프롬프트에 동적으로 로드되는 방식도 읽어보세요. 마지막으로, MCP 서버용 도구를 작성하는 경우, 도구 주석은 어떤 도구가 오픈 월드 접근이 필요하거나 파괴적인 변경을 수행하는지 공개하는 데 도움이 됩니다.

## 앞으로의 전망

에이전트를 위한 효과적인 도구를 구축하려면 예측 가능하고 결정론적인 패턴에서 비결정론적인 패턴으로 소프트웨어 개발 관행을 재조정해야 합니다.

이 글에서 설명한 반복적이고 평가 중심의 프로세스를 통해 도구를 성공적으로 만드는 일관된 패턴을 식별했습니다. 효과적인 도구는 의도적이고 명확하게 정의되어 있으며, 에이전트 컨텍스트를 신중하게 사용하고, 다양한 워크플로우에서 함께 결합될 수 있으며, 에이전트가 실제 작업을 직관적으로 해결할 수 있게 합니다.

미래에는 에이전트가 세계와 상호작용하는 특정 메커니즘이 진화할 것입니다—MCP 프로토콜 업데이트에서 기본 LLM 자체의 업그레이드까지. 에이전트용 도구를 개선하기 위한 체계적이고 평가 중심의 접근 방식으로, 에이전트가 더 유능해짐에 따라 사용하는 도구도 함께 발전할 수 있습니다.

## 감사의 말

이 글은 Ken Aizawa가 Research(Barry Zhang, Zachary Witten, Daniel Jiang, Sami Al-Sheikh, Matt Bell, Maggie Vo), MCP(Theodora Chu, John Welsh, David Soria Parra, Adam Jones), Product Engineering(Santiago Seira), Marketing(Molly Vorwerck), Design(Drew Roper), Applied AI(Christian Ryan, Alexander Bricken) 등 여러 부서의 동료들의 귀중한 기여로 작성했습니다.

¹기본 LLM 자체를 훈련하는 것 외에.

---

## 핵심 요약

- 에이전트의 효과는 제공된 도구의 품질에 직접 의존한다
- 도구는 결정론적 시스템과 비결정론적 에이전트 사이의 새로운 계약 유형이다
- 프로토타입 구축 → 평가 실행 → 에이전트와 협업하여 반복적으로 도구를 개선하는 워크플로우가 효과적이다
- 도구가 많다고 항상 좋은 것은 아니며, 에이전트의 어포던스에 맞는 도구를 선택해야 한다
- 네임스페이싱으로 도구 간 명확한 경계를 정의한다
- 도구 응답에서 유용한 정보만 반환하고 토큰 효율성을 최적화한다
- 도구 설명의 프롬프트 엔지니어링이 성능 향상에 매우 효과적이다
