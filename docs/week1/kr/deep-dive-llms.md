---
title: "LLM 심층 탐구 - ChatGPT 같은 대규모 언어 모델의 모든 것"
originalTitle: "Deep Dive into LLMs like ChatGPT"
author: "안드레이 카르파티 (Andrej Karpathy)"
sourceUrl: "https://www.youtube.com/watch?v=7xTGNNLPyMI"
translatedAt: "2026-01-10"
status: "final"
contentType: "youtube"
duration: "3:31:05"
totalChapters: 24
hasSummary: true
---
# LLM 심층 탐구 - ChatGPT 같은 대규모 언어 모델의 모든 것

[원본 영상](https://www.youtube.com/watch?v=7xTGNNLPyMI)

<!-- SUMMARY_START -->

## TL;DR

이 3시간 30분짜리 강의에서 안드레이 카르파티는 ChatGPT 같은 대규모 언어 모델(LLM)이 어떻게 만들어지고 작동하는지 처음부터 끝까지 설명합니다. **사전학습**(인터넷 데이터 학습), **지도학습 미세조정**(대화 데이터로 어시스턴트 만들기), **강화학습**(성능 최적화)의 세 단계를 거쳐 LLM이 탄생합니다. 모델은 놀라운 능력을 보이지만 환각, 토큰화 한계, 들쭉날쭉한 지능 등의 약점도 있습니다. ChatGPT와 대화할 때 마법 같은 AI가 아니라 "OpenAI 데이터 라벨러의 통계적 시뮬레이션"과 대화한다고 생각하면 더 정확합니다.

## 이 콘텐츠에서 배울 수 있는 것

- LLM의 3단계 학습 파이프라인 (사전학습 → SFT → RL) 이해
- 토큰화, 신경망, 트랜스포머의 핵심 개념
- 환각(hallucination)의 원인과 완화 방법
- 강화학습이 모델 성능을 향상시키는 원리
- DeepSeek R1, AlphaGo에서 배우는 RL의 힘
- LLM의 심리학: 들쭉날쭉한 지능과 한계
- 최신 LLM 동향 추적 방법과 도구 활용법

---

## 챕터별 요약

### 1. 소개 (0:00)

ChatGPT와 같은 대규모 언어 모델(LLM)에 대한 포괄적인 소개로, 이 도구가 무엇이고 어떻게 작동하는지에 대한 사고 모델을 제공합니다.

**핵심 포인트:**
- LLM은 마법 같지만 한계와 주의점이 있음
- 전체 파이프라인을 일반 청중도 이해할 수 있게 설명 예고

---

### 2. 사전학습 데이터 - 인터넷 (1:00)

LLM 사전학습의 첫 단계인 인터넷 데이터 수집을 설명합니다.

**핵심 포인트:**
- Common Crawl에서 시작해 약 44TB 고품질 텍스트 확보
- URL 필터링, 텍스트 추출, 언어 필터링, PII 제거 등 다단계 처리
- FineWeb 데이터셋이 대표적 예시

---

### 3. 토큰화 (7:47)

텍스트를 신경망에 입력하기 위해 토큰으로 변환하는 과정을 설명합니다.

**핵심 포인트:**
- UTF-8 바이트에서 시작해 BPE로 어휘 생성
- GPT-4는 100,277개 토큰 사용
- Tiktokenizer로 토큰화 과정 확인 가능

---

### 4. 신경망 입출력 (14:27)

신경망의 입출력 구조를 설명합니다.

**핵심 포인트:**
- 입력: 토큰 시퀀스 (최대 8,000개 등)
- 출력: 다음 토큰 확률 분포 (100,000개 확률)
- 학습: 정답 토큰 확률 높이도록 조정

---

### 5. 신경망 내부 구조 (20:11)

트랜스포머 아키텍처의 내부를 설명합니다.

**핵심 포인트:**
- 수십억 개 파라미터가 입력과 결합
- 어텐션, MLP 블록 등으로 구성
- GPU 클러스터에서 수개월간 학습

---

### 6. 추론 (26:01)

학습된 모델로 텍스트를 생성하는 과정입니다.

**핵심 포인트:**
- 확률 분포에서 다음 토큰 샘플링
- 반복하여 텍스트 생성
- 학습보다 훨씬 빠름

---

### 7. GPT-2: 훈련과 추론 (31:09)

OpenAI의 GPT-2를 예시로 학습과 추론을 시연합니다.

**핵심 포인트:**
- 2019년 공개, 15억 파라미터
- llm.c로 재현 가능 (약 $100-600)
- loss 감소를 통해 학습 진행 확인

---

### 8. Llama 3.1 베이스 모델 추론 (42:52)

베이스 모델의 특성과 추론 예시를 보여줍니다.

**핵심 포인트:**
- 베이스 모델은 인터넷 텍스트 시뮬레이터
- 어시스턴트가 아닌 문서 완성기
- 후속학습 필요성 설명

---

### 9. 사전학습에서 후속학습으로 (59:23)

베이스 모델을 어시스턴트로 변환하는 과정입니다.

**핵심 포인트:**
- 베이스 모델은 유용하지만 직접 대화 불가
- 후속학습으로 어시스턴트 페르소나 부여
- ChatGPT 같은 AI의 핵심 단계

---

### 10. 후속학습 데이터 - 대화 (1:01:06)

지도학습 미세조정(SFT)에 사용되는 대화 데이터를 설명합니다.

**핵심 포인트:**
- 인간 라벨러가 고품질 대화 작성
- 라벨링 지침에 따른 이상적 응답
- 수십만 대화로 어시스턴트 성격 형성

---

### 11. 환각, 도구 사용, 지식/작업 메모리 (1:20:32)

LLM의 환각 문제와 해결책을 다룹니다.

**핵심 포인트:**
- 환각: 모르는 것도 자신있게 답함
- 완화: 모른다고 말하도록 학습
- 도구 사용: 웹 검색, 코드 실행으로 보완

---

### 12. 자기 인식 (1:41:46)

모델의 자기 정체성에 대해 설명합니다.

**핵심 포인트:**
- 모델은 자신이 무엇인지 모름
- 시스템 프롬프트로 정체성 부여
- 하드코딩된 대화로 자기 소개 학습

---

### 13. 모델은 생각하기 위해 토큰이 필요하다 (1:46:56)

Chain of Thought의 중요성을 설명합니다.

**핵심 포인트:**
- 토큰당 고정된 계산량
- 복잡한 문제는 중간 단계 필요
- 바로 답 요구하면 실패

---

### 14. 토큰화 재방문: 철자 문제 (2:01:11)

토큰화가 철자 처리에 미치는 영향입니다.

**핵심 포인트:**
- 모델은 글자가 아닌 토큰 단위로 봄
- "strawberry"에서 r 세기 어려움
- 코드 도구로 우회 가능

---

### 15. 들쭉날쭉한 지능 (2:04:53)

LLM의 불균일한 능력을 설명합니다.

**핵심 포인트:**
- 올림피아드 문제는 풀지만 9.11 vs 9.9 실패
- 스위스 치즈 같은 능력 분포
- 도구로 사용하고 결과 검증 필요

---

### 16. SFT에서 강화학습으로 (2:07:28)

강화학습 단계로의 전환을 설명합니다.

**핵심 포인트:**
- SFT: 전문가 모방
- RL: 스스로 발견
- 연습 문제처럼 실력 향상

---

### 17. 강화학습 (2:14:42)

RL의 핵심 개념과 적용을 다룹니다.

**핵심 포인트:**
- 보상 함수로 좋은 출력 강화
- 검증 가능한 영역(수학, 코드)에서 효과적
- 다양한 풀이 시도 후 좋은 것 강화

---

### 18. DeepSeek R1 (2:37:47)

딥시크의 R1 모델 특징입니다.

**핵심 포인트:**
- 순수 RL로 사고 과정 발전
- 영어-중국어 전환하며 생각
- 오픈 웨이트로 누구나 사용 가능

---

### 19. 알파고 (2:48:47)

바둑에서의 RL 성공 사례입니다.

**핵심 포인트:**
- 인간 모방만으로는 한계
- RL로 인간 초월 가능
- 37수: 인간이 생각 못한 수

---

### 20. RLHF (3:01:46)

인간 피드백 기반 강화학습입니다.

**핵심 포인트:**
- 창작 글쓰기 등 검증 어려운 영역
- 인간 선호도 모방하는 보상 모델
- 게이밍 문제로 무한 실행 불가

---

### 21. 앞으로의 전망 (3:09:39)

LLM의 미래 발전 방향입니다.

**핵심 포인트:**
- 멀티모달: 이미지, 오디오, 비디오
- 에이전트: 장시간 자율 작업
- 컴퓨터 사용: 키보드/마우스 제어

---

### 22. LLM 동향 추적하기 (3:15:15)

최신 정보를 얻는 방법입니다.

**핵심 포인트:**
- LM Arena: 모델 순위 확인
- AI News 뉴스레터
- X(트위터) 팔로우

---

### 23. LLM을 찾을 수 있는 곳 (3:18:34)

LLM 사용 플랫폼 안내입니다.

**핵심 포인트:**
- ChatGPT, Claude, Gemini
- together.ai, hyperbolic
- LM Studio로 로컬 실행

---

### 24. 전체 요약 (3:21:46)

강의 전체 내용 정리입니다.

**핵심 포인트:**
- 3단계: 사전학습 → SFT → RL
- ChatGPT = 라벨러의 통계적 시뮬레이션
- 도구로 사용하고 결과 검증하기

<!-- SUMMARY_END -->

---

<!-- FULL_TRANSLATION_START -->

## 전체 번역

### 목차

1. [소개](#1-introduction-소개) (0:00)
2. [사전학습 데이터 - 인터넷](#2-pretraining-data-internet-사전학습-데이터---인터넷) (1:00)
3. [토큰화](#3-tokenization-토큰화) (7:47)
4. [신경망 입출력](#4-neural-network-io-신경망-입출력) (14:27)
5. [신경망 내부 구조](#5-neural-network-internals-신경망-내부-구조) (20:11)
6. [추론](#6-inference-추론) (26:01)
7. [GPT-2: 훈련과 추론](#7-gpt-2-training-and-inference-gpt-2-훈련과-추론) (31:09)
8. [Llama 3.1 베이스 모델 추론](#8-llama-31-base-model-inference-llama-31-기본-모델-추론) (42:52)
9. [사전학습에서 후속학습으로](#9-사전학습에서-후속학습으로) (59:23)
10. [후속학습 데이터 - 대화](#10-후속학습-데이터---대화) (1:01:06)
11. [환각, 도구 사용, 지식/작업 메모리](#11-환각-도구-사용-지식작업-메모리) (1:20:32)
12. [자기 인식](#12-자기-인식) (1:41:46)
13. [모델은 생각하기 위해 토큰이 필요하다](#13-모델은-생각하기-위해-토큰이-필요하다) (1:46:56)
14. [토큰화 재방문: 모델은 철자에 어려움을 겪는다](#14-토큰화-재방문-모델은-철자에-어려움을-겪는다) (2:01:11)
15. [들쭉날쭉한 지능](#15-jagged-intelligence-들쭉날쭉한-지능) (2:04:53)
16. [지도학습 미세조정에서 강화학습으로](#16-지도학습-미세조정에서-강화학습으로) (2:07:28)
17. [강화학습](#17-강화학습-reinforcement-learning) (2:14:42)
18. [DeepSeek R1](#18-딥시크-r1-deepseek-r1) (2:37:47)
19. [AlphaGo](#19-알파고-alphago) (2:48:47)
20. [RLHF](#20-인간-피드백-기반-강화학습-rlhf) (3:01:46)
21. [앞으로의 전망](#21-앞으로의-전망-preview-of-things-to-come) (3:09:39)
22. [LLM 동향 추적하기](#22-llm-동향-추적하기-keeping-track-of-llms) (3:15:15)
23. [LLM을 찾을 수 있는 곳](#23-llm을-찾을-수-있는-곳-where-to-find-llms) (3:18:34)
24. [전체 요약](#24-전체-요약-grand-summary) (3:21:46)

---

## 1. Introduction (소개)

**요약**: ChatGPT와 같은 대규모 언어 모델(LLM)에 대한 포괄적인 소개로, 이 도구가 무엇이고 어떻게 작동하는지에 대한 사고 모델을 제공하겠다고 설명합니다. LLM이 잘하는 것과 못하는 것, 그리고 주의해야 할 함정들에 대해 다룹니다.

[0:00] 안녕하세요, 여러분. 이 영상을 만들고 싶었던 지 꽤 됐는데요. ChatGPT와 같은 대규모 언어 모델에 대해 일반 청중을 위한 포괄적인 소개를 해보려고 합니다. 이 영상을 통해 제가 전달하고 싶은 것은 이 도구가 무엇인지 이해하기 위한 사고 모델입니다. 이 도구는 분명 어떤 면에서는 마법 같고 놀랍습니다. 어떤 것들은 정말 잘하고, 다른 것들은 그다지 잘 못하며, 주의해야 할 날카로운 모서리들도 많이 있습니다. 그래서 이 텍스트 상자 뒤에는 무엇이 있을까요? 여기에 무엇이든 입력하고 엔터를 누를 수 있지만,

[0:30] 우리는 무엇을 입력해야 할까요? 이렇게 생성되는 단어들은 어떻게 만들어지는 걸까요? 이게 어떻게 작동하는 거고, 정확히 무엇과 대화하고 있는 걸까요? 이 영상에서 이 모든 주제를 다루려고 합니다. 이런 것들이 어떻게 만들어지는지 전체 파이프라인을 살펴볼 건데, 일반 청중도 이해할 수 있도록 설명하겠습니다. 먼저 ChatGPT 같은 것을 어떻게 만드는지 살펴보고, 그 과정에서 이 도구의 인지적, 심리적 함의에 대해서도

---

## 2. Pretraining Data (Internet) (사전학습 데이터 - 인터넷)

**요약**: LLM 사전학습의 첫 단계인 인터넷 데이터 수집을 설명합니다. Common Crawl에서 시작해 URL 필터링, 텍스트 추출, 언어 필터링, 중복 제거, PII 제거 등 여러 단계를 거쳐 약 44TB의 고품질 텍스트 데이터셋(예: FineWeb)을 구축합니다.

[1:00] 이야기해 보겠습니다. 자, ChatGPT를 만들어 봅시다. 순차적으로 배열된 여러 단계가 있는데, 첫 번째 단계는 사전학습 단계라고 합니다. 사전학습 단계의 첫 번째 과정은 인터넷을 다운로드하고 처리하는 것입니다. 이게 대략 어떤 모습인지 감을 잡으려면 이 URL을 살펴보시길 권합니다. 허깅페이스라는 회사가 FineWeb이라는 데이터셋을 수집하고 만들고 큐레이션했는데, 이 블로그 포스트에서 FineWeb 데이터셋을 어떻게 구축했는지 자세히 설명하고 있습니다.

[1:30] OpenAI, Anthropic, Google 같은 모든 주요 LLM 제공업체들은 내부적으로 FineWeb 데이터셋과 비슷한 것을 가지고 있을 겁니다. 대략적으로 우리가 달성하려는 것은 이겁니다. 공개적으로 이용 가능한 소스에서 인터넷의 엄청난 양의 텍스트를 얻으려는 거죠. 매우 고품질의 문서를 대량으로 확보하고, 또한 매우 다양한 문서를 원합니다. 왜냐하면 이 모델들 안에 많은 지식을 담고 싶기 때문입니다. 그래서 고품질 문서의

[2:00] 큰 다양성과 많은 양이 필요합니다. 이를 달성하는 것은 상당히 복잡하고, 보시다시피 여러 단계를 거쳐야 잘 수행됩니다. 이 단계들이 어떻게 생겼는지 조금 살펴보겠습니다. 지금은 예를 들어 FineWeb 데이터셋이 실제 프로덕션급 애플리케이션에서 볼 수 있는 것을 상당히 잘 대표하며, 실제로는 약 44테라바이트의 디스크 공간만 차지한다는 점을 말씀드리고 싶습니다. 1테라바이트 USB 스틱은 매우 쉽게 구할 수 있고, 아마 오늘날에는 하나의 하드 드라이브에 거의 들어갈 수 있을 겁니다. 그래서

[2:30] 인터넷이 매우 방대하지만, 결국 그렇게 엄청난 양의 데이터는 아닙니다. 우리는 텍스트를 다루고 있고 공격적으로 필터링하기 때문에 이 예시에서는 약 44테라바이트로 끝납니다. 이 데이터가 어떻게 생겼는지, 그리고 이 단계들이 무엇인지 살펴봅시다. 이런 노력의 출발점이 되고 최종적으로 대부분의 데이터를 기여하는 것은 Common Crawl의 데이터입니다. Common Crawl은 기본적으로

[3:00] 2007년부터 인터넷을 수집해온 조직입니다. 예를 들어 2024년 기준으로 Common Crawl은 27억 개의 웹페이지를 색인했습니다. 인터넷을 돌아다니는 크롤러들이 있고, 기본적으로 몇 개의 시드 웹페이지에서 시작해서 모든 링크를 따라가고, 계속 링크를 따라가면서 모든 정보를 색인하다 보면 시간이 지나면서 인터넷의 엄청난 양의 데이터를 얻게 됩니다. 이것이 보통 이런 노력들의 출발점입니다. 이 Common Crawl 데이터는

[3:30] 상당히 원시적이고 여러 가지 방식으로 필터링됩니다. 여기서 그들은 이 같은 다이어그램을 문서화했는데, 이 단계들에서 어떤 종류의 처리가 일어나는지 조금 설명합니다. 첫 번째는 URL 필터링이라는 것입니다. 이것은 기본적으로 데이터를 얻고 싶지 않은 URL이나 도메인의 목록을 말합니다. 보통 여기에는 악성코드 웹사이트, 스팸 웹사이트, 마케팅 웹사이트,

[4:00] 인종차별 웹사이트, 성인 사이트 같은 것들이 포함됩니다. 데이터셋에 원하지 않기 때문에 이 단계에서 제거되는 다양한 유형의 웹사이트들이 정말 많습니다. 두 번째 부분은 텍스트 추출입니다. 이 웹페이지들이 크롤러에 의해 저장되는 원시 HTML이라는 것을 기억해야 합니다. 여기서 검사를 클릭하면 실제 원시 HTML이 이렇게 생겼습니다. 리스트 같은 마크업이 있고

[4:30] CSS와 이런 것들이 있다는 걸 알 수 있습니다. 이것은 거의 웹페이지를 위한 컴퓨터 코드인데, 우리가 정말 원하는 것은 이 텍스트입니다. 웹페이지의 텍스트만 원하고 내비게이션 같은 것은 원하지 않습니다. 그래서 이 웹페이지들의 좋은 콘텐츠만 적절히 필터링하기 위한 많은 필터링, 처리, 휴리스틱이 들어갑니다. 다음 단계는 언어 필터링입니다. 예를 들어 FineWeb은

[5:00] 언어 분류기를 사용하여 모든 웹페이지가 어떤 언어인지 추측하고, 예를 들어 65% 이상이 영어인 웹페이지만 유지합니다. 이것이 설계 결정이라는 것을 알 수 있는데, 다른 회사들은 스스로 결정할 수 있습니다. 다양한 유형의 언어를 데이터셋에 어느 정도 포함시킬 것인가를요. 예를 들어 스페인어를 모두 필터링하면 나중에 우리 모델이 스페인어를 잘 못할 수 있습니다. 그 언어의 데이터를 많이 보지 못했기 때문이죠. 그래서 다른 회사들은 다국어 성능에

[5:30] 다른 정도로 집중할 수 있습니다. FineWeb은 영어에 상당히 집중되어 있어서 나중에 훈련하는 언어 모델은 영어는 매우 잘하지만 다른 언어는 그다지 잘 못할 수 있습니다. 언어 필터링 후에는 몇 가지 다른 필터링 단계와 중복 제거 등이 있고, 마지막으로 예를 들어 PII 제거가 있습니다. 이것은 개인 식별 정보로, 예를 들어 주소, 사회보장번호 같은 것을 감지하고 그런 종류의

[6:00] 웹페이지를 데이터셋에서 필터링하려고 합니다. 여기에는 많은 단계가 있고 전체 세부 사항을 다루지는 않겠지만, 전처리의 상당히 광범위한 부분이고 예를 들어 FineWeb 데이터셋으로 끝납니다. 클릭해 들어가면 실제로 어떻게 생겼는지 예시를 볼 수 있고, 누구나 허깅페이스 웹페이지에서 다운로드할 수 있습니다. 훈련 세트에 들어가는 최종 텍스트의 예시들입니다. 이것은 2012년

[6:30] 토네이도에 대한 기사입니다. 2012년에 토네이도가 있었고 무슨 일이 있었는지... 다음 것은 "당신의 몸에 9볼트 배터리 크기의 작은 노란색 부신이 두 개 있다는 걸 알고 계셨나요?" 같은 내용입니다. 이것은 어떤 종류의 이상한 의학 기사죠. 이것들을 기본적으로 다양한 방식으로 텍스트만 필터링된 인터넷의 웹페이지들이라고 생각하세요. 이제 우리는 40테라바이트의 엄청난 양의 텍스트를 가지고 있고, 이것이

[7:00] 다음 단계의 출발점입니다. 지금 우리가 어디에 있는지 직관적인 감각을 드리고 싶었습니다. 여기서 처음 200개의 웹페이지를 가져왔는데, 우리는 엄청나게 많은 웹페이지를 가지고 있다는 것을 기억하세요. 그 모든 텍스트를 그냥 합쳐서 연결했습니다. 이것이 우리가 얻는 것입니다. 그냥 원시 텍스트, 원시 인터넷 텍스트이고 양이 엄청납니다. 200개의 웹페이지만으로도요. 계속 축소해 보면 이런 거대한 텍스트 데이터의 태피스트리가 있습니다.

[7:30] 이 텍스트 데이터에는 이 모든 패턴들이 있고, 우리가 하고 싶은 것은 이 데이터로 신경망을 훈련시켜서 신경망이 이 텍스트가 어떻게 흘러가는지 내재화하고 모델링하게 하는 것입니다. 이 거대한 텍스트 직물이 있고, 이제 이것을 모방하는 신경망을 얻으려고 합니다. 자, 텍스트를 신경망에 넣기 전에 이 텍스트를 어떻게 표현하고 어떻게 입력할지 결정해야 합니다. 이 신경망 기술이 작동하는 방식은

---

## 3. Tokenization (토큰화)

**요약**: 텍스트를 신경망에 입력하기 위해 토큰으로 변환하는 과정을 설명합니다. UTF-8 바이트에서 시작해 바이트 페어 인코딩(BPE) 알고리즘을 사용하여 약 100,000개의 토큰 어휘를 생성하며, GPT-4는 100,277개의 토큰을 사용합니다.

[8:00] 1차원 기호 시퀀스를 기대하고, 가능한 기호의 유한한 집합을 원합니다. 그래서 기호가 무엇인지 결정하고, 데이터를 그 기호들의 1차원 시퀀스로 표현해야 합니다. 지금 우리가 가진 것은 텍스트의 1차원 시퀀스입니다. 여기서 시작해서 여기로 가고, 여기로 오고... 화면에는 2차원으로 배치되어 있지만 왼쪽에서 오른쪽으로, 위에서 아래로 가는

[8:30] 텍스트의 1차원 시퀀스입니다. 물론 컴퓨터이기 때문에 기본 표현이 있습니다. 이 텍스트를 UTF-8로 인코딩하면 컴퓨터에서 이 텍스트에 해당하는 원시 비트를 얻을 수 있고, 이렇게 생겼습니다. 예를 들어 여기 이 첫 번째 막대가 첫 번째

[9:00] 8비트입니다. 이것이 무엇일까요? 어떤 의미에서는 우리가 찾고 있는 표현입니다. 정확히 두 가지 가능한 기호 0과 1이 있고, 매우 긴 시퀀스가 있습니다. 하지만 이 시퀀스 길이는 신경망에서 매우 유한하고 귀중한 자원이 될 것이고, 우리는 실제로 두 가지 기호만의 극도로 긴 시퀀스를 원하지 않습니다. 대신 우리가 원하는 것은

[9:30] 어휘라고 부르는 이 기호 크기와 결과 시퀀스 길이 사이의 트레이드오프입니다. 두 개의 기호와 극도로 긴 시퀀스를 원하지 않고, 더 많은 기호와 더 짧은 시퀀스를 원합니다. 자, 시퀀스 길이를 압축하거나 줄이는 한 가지 단순한 방법은 기본적으로 연속된 비트들의 그룹, 예를 들어 8비트를 고려해서 하나의

[10:00] 바이트라고 부르는 것으로 그룹화하는 것입니다. 이 비트들은 켜져 있거나 꺼져 있으므로, 8개의 그룹을 취하면 이 비트들이 켜지거나 꺼질 수 있는 조합이 256가지뿐입니다. 따라서 이 시퀀스를 바이트 시퀀스로 다시 표현할 수 있습니다. 이 바이트 시퀀스는 8배 더 짧지만 이제 256개의 가능한 기호가 있습니다. 여기 모든 숫자는 0에서 255까지입니다. 이것들을 숫자가 아니라

[10:30] 고유 ID나 고유 기호로 생각하시길 권합니다. 어쩌면 이것들을 각각 고유한 이모지로 대체해서 생각하는 게 더 나을 수 있습니다. 그러면 이렇게 됩니다. 기본적으로 이모지 시퀀스가 있고 256개의 가능한 이모지가 있다고 생각할 수 있습니다. 실제로 최첨단 언어 모델의 프로덕션에서는 이것보다 더 나아가야 합니다. 시퀀스 길이를 계속 줄여야 하는데,

[11:00] 다시 말하지만 귀중한 자원이기 때문입니다. 더 많은 기호를 어휘에 넣는 대가로요. 이렇게 하는 방식이 바이트 페어 인코딩 알고리즘을 실행하는 것입니다. 작동 방식은 기본적으로 매우 흔한 연속된 바이트나 기호를 찾는 것입니다. 예를 들어 116 다음에 32가 오는 시퀀스가 상당히 흔하고 매우 자주 발생한다는 것이 밝혀졌습니다. 그래서 우리가 할 일은 이 쌍을 새 기호로 그룹화하는 것입니다. ID 256인 기호를 새로 만들고

[11:30] 모든 쌍 116, 32를 이 새 기호로 다시 씁니다. 그런 다음 원하는 만큼 이 알고리즘을 반복할 수 있고, 새 기호를 만들 때마다 길이를 줄이고 기호 크기를 늘립니다. 실제로 어휘 크기의 꽤 좋은 설정은 약 100,000개의 가능한 기호로 밝혀졌습니다. 특히 GPT-4는 100,277개의 기호를

[12:00] 사용합니다. 원시 텍스트에서 이 기호들, 또는 토큰이라고 부르는 것으로 변환하는 이 과정을 토큰화라고 합니다. 이제 GPT-4가 텍스트에서 토큰으로, 토큰에서 다시 텍스트로 토큰화를 어떻게 수행하는지, 그리고 실제로 어떻게 생겼는지 살펴봅시다. 이런 토큰 표현을 탐색하기 위해 제가 좋아하는 웹사이트는 tiktokenizer입니다. 여기 드롭다운에서

[12:30] GPT-4 기본 모델 토크나이저인 cl100k_base를 선택하고, 왼쪽에 텍스트를 넣으면 그 텍스트의 토큰화를 보여줍니다. 예를 들어 "hello space world", 즉 hello world는 정확히 두 개의 토큰으로 밝혀졌습니다. ID 15339인 토큰 hello와

[13:00] ID 1917인 토큰 " world"(앞에 공백 포함)입니다. 그래서 "hello space world"... 이 두 개를 붙이면 다시 두 개의 토큰이 되지만, H 토큰 다음에 H 없는 "ello world" 토큰이 됩니다. hello와 world 사이에 공백을 두 개 넣으면 또 다른 토큰화가 됩니다. 여기 새로운 토큰 220이 있습니다. 이것을 가지고 놀면서 무슨 일이 일어나는지 볼 수 있습니다. 또한 이것은

[13:30] 대소문자를 구분합니다. 대문자 H면 다른 것이 되고, "Hello World"면 실제로 세 개의 토큰이 됩니다. 네, 그래서 이것을 가지고 놀면서 이 토큰들이 어떻게 작동하는지 직관적인 감각을 얻을 수 있습니다. 영상 후반에 토큰화로 다시 돌아올 건데, 지금은 웹사이트를 보여드리고 싶었고 기본적으로 이 텍스트가 예를 들어

[14:00] 여기 한 줄을 가져오면 GPT-4는 이것을 이렇게 볼 것입니다. 이 텍스트는 길이 62의 시퀀스가 될 것이고, 이것이 시퀀스이고, 텍스트 청크들이 이 기호들에 어떻게 대응하는지입니다. 다시 말하지만 100,277개의 가능한 기호가 있고, 이제 그 기호들의 1차원 시퀀스가 있습니다. 토큰화로 돌아올 테지만 지금은 여기까지입니다. 자, 제가 한 것은

---

## 4. Neural Network I/O (신경망 입출력)

**요약**: 신경망의 입출력 구조를 설명합니다. 토큰 시퀀스가 입력으로 들어가면, 신경망은 다음 토큰에 대한 확률 분포(약 100,000개 토큰 각각의 확률)를 출력합니다. 학습 과정에서 정답 토큰의 확률을 높이도록 신경망을 조정합니다.

[14:30] 데이터셋에 있는 이 텍스트 시퀀스를 토크나이저를 사용해서 토큰 시퀀스로 다시 표현한 것입니다. 이게 그 모습입니다. 예를 들어 FineWeb 데이터셋으로 돌아가면, 이것이 44테라바이트의 디스크 공간일 뿐만 아니라 이 데이터셋에는 약 15조 개의 토큰 시퀀스가 있다고 언급합니다. 여기 이것들은 이 데이터셋의 처음 수천 개 정도의

[15:00] 토큰이지만 15조 개가 있다는 것을 기억하세요. 그리고 다시 한번 기억하세요, 이것들 모두 작은 텍스트 청크를 나타냅니다. 이 시퀀스들의 원자 같은 것이고, 여기 숫자들은 아무 의미가 없습니다. 그냥 고유 ID입니다. 자, 이제 재미있는 부분인 신경망 훈련에 들어갑니다. 이 신경망을 훈련할 때 계산적으로 많은 무거운 작업이 일어나는 곳입니다. 이

[15:30] 단계에서 우리가 하는 것은 이 토큰들이 시퀀스에서 서로 어떻게 따라오는지의 통계적 관계를 모델링하는 것입니다. 우리가 하는 것은 데이터에 들어가서 토큰의 윈도우를 취하는 것입니다. 데이터셋에서 상당히 무작위로 토큰의 윈도우를 취하고, 윈도우 길이는 실제로 0개 토큰에서 우리가 결정한 최대 크기까지

[16:00] 어디든 될 수 있습니다. 예를 들어 실제로는 8,000 토큰의 윈도우를 볼 수 있습니다. 원칙적으로 임의의 윈도우 길이를 사용할 수 있지만, 매우 긴 윈도우 시퀀스를 처리하는 것은 계산적으로 매우 비쌀 것이므로 8,000이 좋은 숫자라고 결정하거나 4,000이나 16,000으로 정하고 거기서 자릅니다. 이 예시에서는 모든 것이 잘 맞도록

[16:30] 처음 네 개의 토큰을 취하겠습니다. 이 토큰들, 네 개의 토큰 윈도우를 취할 건데 "bar view in" 그리고 "single"이고 이것이 이 토큰 ID들입니다. 우리가 하려는 것은 기본적으로 시퀀스에서 다음에 오는 토큰을 예측하는 것입니다. 3962가 다음에 옵니다. 이제 여기서 하는 것은 이 네 개의 토큰을 컨텍스트라고 부르고

[17:00] 신경망에 입력하는 것입니다. 이것이 신경망의 입력입니다. 잠시 후에 이 신경망 내부에 무엇이 있는지 자세히 들어가겠지만, 지금 중요한 것은 신경망의 입력과 출력을 이해하는 것입니다. 입력은 가변 길이의 토큰 시퀀스로, 0에서 8,000 같은 최대 크기까지입니다. 출력은 다음에 무엇이 오는지에 대한 예측입니다. 우리 어휘에 100,277개의 가능한 토큰이 있으므로, 신경망은 정확히

[17:30] 그만큼의 숫자를 출력할 것이고, 그 모든 숫자는 그 토큰이 시퀀스에서 다음으로 올 확률에 해당합니다. 다음에 무엇이 오는지 추측하는 것입니다. 처음에 이 신경망은 무작위로 초기화되어 있고, 잠시 후에 그게 무엇을 의미하는지 보겠지만, 무작위 변환입니다. 그래서 훈련 초기에 이 확률들도 일종의 무작위입니다. 여기 세 가지 예시가 있지만 100,000개의 숫자가 있다는 것을 기억하세요. "direction"이라는 토큰의 확률을

[18:00] 신경망이 4%로 말하고 있습니다. 11799는 2%이고, 여기 "post"인 3962의 확률은 3%입니다. 물론 우리는 데이터셋에서 이 윈도우를 샘플링했으므로 다음에 무엇이 오는지 알고 있습니다. 이것이 레이블입니다. 정답이 실제로 시퀀스에서 3962가 다음에 온다는 것을 알고 있습니다. 이제 우리에게는 신경망을 업데이트하기 위한 이 수학적 과정이 있습니다.

[18:30] 조정하는 방법이 있고, 조금 자세히 들어가겠지만 기본적으로 여기 3%인 이 확률이 더 높아지기를 원하고 다른 모든 토큰의 확률은 더 낮아지기를 원한다는 것을 알고 있습니다. 그래서 정답이 약간 더 높은 확률을 갖도록 신경망을 조정하고 업데이트하는 방법을 수학적으로 계산하는 방법이 있습니다. 신경망을 업데이트하면 이제 다음번에

[19:00] 이 특정 네 개의 토큰 시퀀스를 신경망에 입력하면 신경망이 이제 약간 조정되어서 "post가 아마 4%이고, case가 이제 아마 1%이고, direction이 2%가 될 수 있다" 같은 식으로 말할 것입니다. 기본적으로 시퀀스에서 다음에 오는 정확한 토큰에 더 높은 확률을 부여하도록 신경망을 살짝 업데이트하는 방법이 있습니다. 이제 이 과정이 이 토큰 하나만이 아니라

[19:30] 전체 데이터셋의 이 모든 토큰들에서 동시에 일어난다는 것을 기억해야 합니다. 네 개가 입력되고 이것을 예측한 곳뿐만 아니라요. 실제로 우리는 작은 윈도우들, 작은 윈도우 배치를 샘플링하고, 이 토큰들 각각에서 우리는 그 토큰의 확률이 약간 더 높아지도록 신경망을 조정하려고 합니다. 이 모든 것이 이 토큰들의 큰 배치에서 병렬로 일어나고, 이것이 신경망을 훈련하는 과정입니다. 예측이

[20:00] 훈련 세트에서 실제로 일어나는 것의 통계와 일치하도록 업데이트하는 시퀀스이고, 확률이 데이터에서 이 토큰들이 서로 따라오는 통계적 패턴과 일관되게 됩니다. 이제 이 신경망들의 내부를 간단히 살펴봐서 안에 무엇이 있는지 감을 드리겠습니다. 신경망 내부입니다. 말씀드렸듯이 토큰 시퀀스인 입력들이 있는데, 이 경우 네 개의 입력 토큰이지만

---

## 5. Neural Network Internals (신경망 내부 구조)

**요약**: 신경망 내부 구조를 설명합니다. 트랜스포머 아키텍처의 파라미터(가중치)는 수십억 개에 달하며, 이 파라미터들이 입력 토큰과 수학적으로 결합되어 다음 토큰을 예측합니다. 학습은 GPU 클러스터에서 수개월간 진행됩니다.

[20:30] 0에서 8,000 토큰까지 어디든 될 수 있습니다. 원칙적으로 이것은 무한한 수의 토큰이 될 수 있지만, 무한한 수의 토큰을 처리하는 것은 계산적으로 너무 비쌀 것이므로 특정 길이에서 자르고 그것이 그 모델의 최대 컨텍스트 길이가 됩니다. 이 입력 X들은 이 신경망의 파라미터 또는 가중치와 함께 거대한 수학적 표현식에서 혼합됩니다. 여기서 여섯 개의 예시 파라미터와 그 설정을 보여주고 있지만,

[21:00] 실제로 현대 신경망은 수십억 개의 이런 파라미터를 가지고 있고, 처음에 이 파라미터들은 완전히 무작위로 설정됩니다. 무작위 파라미터 설정으로는 이 신경망이 무작위 예측을 할 것이라고 예상할 수 있고, 처음에는 실제로 완전히 무작위 예측입니다. 하지만 네트워크를 반복적으로 업데이트하는 이 과정을 통해, 이것을 신경망 훈련이라고 부르는데,

[21:30] 이 파라미터들의 설정이 조정되어 신경망의 출력이 훈련 세트에서 본 패턴과 일관되게 됩니다. 이 파라미터들을 DJ 세트의 노브들처럼 생각하세요. 이 노브들을 돌리면서 가능한 모든 토큰 시퀀스 입력에 대해 다른 예측을 얻게 됩니다. 신경망을 훈련한다는 것은 훈련 세트의 통계와 일관되어 보이는 파라미터 설정을 발견하는 것을 의미합니다. 이 거대한 수학적 표현식이

[22:00] 어떻게 생겼는지 예를 들어드리겠습니다. 현대 네트워크는 아마 수조 개의 항을 가진 거대한 표현식이지만, 간단한 예를 보여드리겠습니다. 이런 종류의 표현식입니다. 그다지 무섭지 않다는 것을 보여드리려고요. 입력 x가 있고, 이 경우 두 개의 예시 입력 x1, x2가 있고, 네트워크의 가중치 w0, w1, w2, w3 등과 혼합됩니다. 이 혼합은 곱셈, 덧셈,

[22:30] 지수화, 나눗셈 같은 간단한 것들입니다. 효과적인 수학적 표현식을 설계하는 것이 신경망 아키텍처 연구의 주제입니다. 표현력이 있고, 최적화 가능하고, 병렬화 가능하고 등등 편리한 특성을 많이 가진 것을요. 하지만 결국 이것들은 복잡한 표현식이 아니고 기본적으로 입력을 파라미터와 혼합해서 예측을 만들고, 우리는 신경망의 파라미터를 최적화해서

[23:00] 예측이 훈련 세트와 일관되게 나오도록 합니다. 이 신경망들이 실제로 어떻게 생겼는지 프로덕션급 예시를 보여드리고 싶습니다. 이를 위해 이 네트워크 중 하나의 매우 좋은 시각화가 있는 이 웹사이트를 방문하시길 권합니다. 이 웹사이트에서 이것을 찾을 수 있고, 프로덕션 환경에서 사용되는 이 신경망은 이런 특별한 종류의 구조를 가지고 있습니다. 이 네트워크는 트랜스포머라고 불리고, 이 특정 예시는 대략

[23:30] 85,000개의 파라미터를 가지고 있습니다. 상단에서 토큰 시퀀스인 입력을 받고, 정보가 신경망을 통해 흐르고 출력인 logit softmax까지 가는데, 이것이 다음에 어떤 토큰이 오는지에 대한 예측입니다. 여기에 일련의 변환들이 있고 이 수학적 표현식 안에서 생성되는 모든 중간 값들이

[24:00] 다음에 무엇이 오는지 예측하는 것입니다. 예를 들어 이 토큰들은 소위 분산 표현으로 임베딩됩니다. 가능한 모든 토큰은 신경망 내부에서 그것을 나타내는 일종의 벡터를 가지고 있습니다. 먼저 토큰을 임베딩하고, 그 값들이 이 다이어그램을 통해 흐릅니다. 이것들은 개별적으로 모두 매우 간단한 수학적 표현식입니다. 레이어 놈이 있고, 행렬 곱셈이 있고, 소프트맥스 등이 있습니다. 여기가 이 트랜스포머의

[24:30] 어텐션 블록이고, 정보가 다층 퍼셉트론 블록으로 흐르고 등등... 여기 이 모든 숫자들은 표현식의 중간 값들이고, 이것들을 이 합성 뉴런들의 발화율처럼 생각할 수 있습니다. 하지만 뇌에서 찾을 수 있는 뉴런처럼 너무 많이 생각하지 않도록 주의하시길 권합니다. 생물학적 뉴런은 기억 등을 가진 매우 복잡한 동적

[25:00] 과정입니다. 이 표현식에는 기억이 없습니다. 입력에서 출력까지 기억 없이 고정된 수학적 표현식입니다. 상태가 없습니다. 그래서 생물학적 뉴런에 비해 매우 단순한 뉴런입니다. 하지만 여전히 느슨하게 이것을 합성 뇌 조직처럼 생각할 수 있습니다. 그렇게 생각하고 싶다면요. 정보가 이 모든 뉴런들을 통해 흐르고 발화하면서 예측에 도달합니다. 이 모든 변환들의 정확한

[25:30] 수학적 세부 사항에 대해 너무 자세히 다루지는 않겠습니다. 솔직히 그렇게 중요하지 않다고 생각합니다. 정말 중요한 것은 이것이 수학적 함수라는 것을 이해하는 것입니다. 85,000개 같은 고정된 파라미터 집합으로 파라미터화되어 있고, 입력을 출력으로 변환하는 방법이고, 파라미터를 조정하면서 다른 종류의 예측을 얻고, 예측이 훈련 세트에서 본 패턴과 일치하도록 좋은 파라미터 설정을 찾아야 합니다.

[26:00] 이것이 트랜스포머입니다. 신경망의 내부를 보여드렸고 훈련 과정에 대해 조금 이야기했습니다. 이 네트워크를 다루는 또 하나의 주요 단계를 다루고 싶은데, 추론이라는 단계입니다. 추론에서 우리가 하는 것은 모델에서 새로운 데이터를 생성하는 것입니다. 기본적으로 네트워크의 파라미터에 어떤 종류의 패턴을 내재화했는지 보고 싶은 것입니다. 모델에서 생성하는 것은 상대적으로 간단합니다.

---

## 6. Inference (추론)

**요약**: 학습된 모델로 텍스트를 생성하는 추론(inference) 과정을 설명합니다. 모델은 확률 분포에서 다음 토큰을 샘플링하고, 이를 반복하여 텍스트를 생성합니다. 추론은 학습보다 훨씬 빠르며, 단일 GPU로도 가능합니다.

[26:30] 기본적으로 접두사, 시작하고 싶은 것인 몇 개의 토큰으로 시작합니다. 토큰 91로 시작하고 싶다고 합시다. 네트워크에 입력하면 네트워크가 확률을 준다는 것을 기억하세요. 이 확률 벡터를 줍니다. 이제 기본적으로 편향된 동전을 던질 수 있습니다. 이 확률 분포를 기반으로 토큰을 샘플링할 수 있습니다. 모델에 의해 높은 확률이 주어진 토큰들은

[27:00] 이 편향된 동전을 던질 때 샘플링될 가능성이 더 높다고 생각할 수 있습니다. 분포에서 샘플링해서 단일 고유 토큰을 얻습니다. 예를 들어 토큰 860이 다음에 옵니다. 860은 이 경우 모델에서 생성할 때 다음에 올 수 있습니다. 860은 상대적으로 가능성이 높은 토큰이고, 이 경우 유일하게 가능한 토큰이 아닐 수 있고 다른 많은 토큰이 샘플링될 수 있었지만, 860이 상대적으로 가능성이 높은 토큰이라는 것을 볼 수 있습니다. 실제로 우리 훈련 예시에서

[27:30] 860이 91 다음에 옵니다. 이제 과정을 계속한다고 합시다. 91 다음에 860이 있고, 추가하고 세 번째 토큰이 무엇인지 다시 물어봅니다. 샘플링하고 여기와 정확히 같은 287이라고 합시다. 다시 해봅시다. 세 개의 시퀀스를 가지고 네 번째 토큰이 뭘지 물어보고 그것에서 샘플링해서 이것을 얻습니다. 이제 한 번 더 해봅시다.

[28:00] 네 개를 취하고 샘플링해서 이것을 얻습니다. 이 13659는 실제로 이전에 있던 3962가 아닙니다. 이 토큰은 "article" 토큰입니다. 그래서 "viewing a single article"이고, 이 경우 훈련 데이터에서 본 시퀀스를 정확히 재현하지 않았습니다. 이 시스템들이 확률적이라는 것을 기억하세요. 샘플링하고 동전을 던지고, 때때로 운이 좋아서

[28:30] 훈련 세트의 텍스트의 작은 청크를 재현하지만, 때때로 훈련 데이터의 어떤 문서에도 그대로 포함되지 않은 토큰을 얻습니다. 그래서 훈련에서 본 데이터의 일종의 리믹스를 얻게 됩니다. 매 단계마다 뒤집어서 약간 다른 토큰을 얻을 수 있고, 그 토큰이 들어오면 다음 것을 샘플링하고 등등... 매우 빠르게 훈련 문서에 있는 토큰 스트림과는

[29:00] 매우 다른 토큰 스트림을 생성하기 시작합니다. 통계적으로 비슷한 속성을 가지겠지만 훈련 데이터와 동일하지 않습니다. 훈련 데이터에서 영감을 받은 것입니다. 이 경우 약간 다른 시퀀스를 얻었고, 왜 "article"을 얻었을까요? "article"이 "bar viewing single" 등의 맥락에서 상대적으로 가능성이 높은 토큰이라고 상상할 수 있고, "article"이라는 단어가 훈련 문서 어딘가에서 이 컨텍스트 윈도우를 어느 정도 따랐고,

[29:30] 우리가 이 단계에서 그것을 샘플링한 것입니다. 기본적으로 추론은 이 분포들에서 한 번에 하나씩 예측하고, 토큰을 계속 다시 입력하고 다음 것을 얻고, 항상 동전을 던지고 얼마나 운이 좋거나 나쁘냐에 따라 이 확률 분포에서 어떻게 샘플링하느냐에 따라 매우 다른 종류의 패턴을 얻을 수 있습니다. 이것이 추론입니다. 가장 일반적인 시나리오에서 기본적으로 인터넷을 다운로드하고 토큰화하는 것은

[30:00] 한 번 하는 전처리 단계이고, 토큰 시퀀스를 갖게 되면 네트워크를 훈련하기 시작할 수 있습니다. 실제 경우에는 다양한 종류의 설정, 다양한 종류의 배열, 다양한 종류의 크기의 많은 다른 네트워크를 훈련하려고 할 것이므로 많은 신경망 훈련을 하게 됩니다. 신경망을 훈련하고 만족스러운 특정 파라미터 세트를 갖게 되면 모델을 가져와서 추론을 하고 실제로

[30:30] 모델에서 데이터를 생성할 수 있습니다. ChatGPT에서 모델과 대화할 때, 그 모델은 아마 수개월 전에 OpenAI에 의해 훈련되었고 잘 작동하는 특정 가중치 세트를 가지고 있습니다. 모델과 대화할 때 그것은 모두 추론일 뿐입니다. 더 이상 훈련이 없고, 파라미터는 고정되어 있고, 모델과 대화하고 있는 것입니다. 토큰 몇 개를 주면 토큰 시퀀스를 완성하고, ChatGPT에서 실제로 모델을 사용할 때 생성되는 것을 보는 것입니다.

[31:00] 그 모델은 추론만 합니다. 이제 구체적인 훈련과 추론의 예시를 보고 이 모델들이 훈련될 때 실제로 어떻게 보이는지 감을 드리겠습니다. 제가 다루고 싶고 특히 좋아하는 예시는 OpenAI의 GPT-2입니다. GPT는 Generatively Pre-trained Transformer(생성적 사전훈련 트랜스포머)를 의미하고, 이것은 OpenAI의 GPT 시리즈의 두 번째 버전입니다. 오늘날 ChatGPT와 대화할 때 그 마법의 기반이 되는 모델은

---

## 7. GPT-2: Training and Inference (GPT-2: 훈련과 추론)

**요약**: OpenAI의 GPT-2 모델을 예시로 학습과 추론을 시연합니다. 2019년에 공개된 GPT-2는 15억 개의 파라미터를 가지며, GitHub의 llm.c 프로젝트로 직접 학습해볼 수 있습니다. 인터넷 데이터의 통계적 패턴을 학습한 모델의 특성을 보여줍니다.

[31:30] GPT-4, 그 시리즈의 네 번째 버전입니다. GPT-2는 2019년에 OpenAI가 여기 있는 이 논문에서 발표했습니다. 제가 GPT-2를 좋아하는 이유는 인식 가능하게 현대적인 스택이 처음으로 모인 때이기 때문입니다. GPT-2의 모든 조각들이 현대 기준으로 오늘날에도 인식 가능합니다. 모든 것이 더 커졌을 뿐입니다. 물론 기술 논문이기 때문에 이 논문의 전체 세부 사항을 다루지는 않겠지만, 강조하고 싶은

[32:00] 세부 사항들은 다음과 같습니다. GPT-2는 오늘날 작업하는 신경망처럼 트랜스포머 신경망이었습니다. 16억 개의 파라미터를 가졌습니다. 여기서 본 파라미터들이고 16억 개입니다. 오늘날 현대 트랜스포머들은 1조 개나 수천억 개에 훨씬 가까울 것입니다. 여기서 최대 컨텍스트 길이는 1,024 토큰이었습니다. 데이터셋에서 토큰 윈도우의 청크를

[32:30] 샘플링할 때 1,024 토큰 이상은 절대 취하지 않습니다. 시퀀스에서 다음 토큰을 예측하려고 할 때 예측을 위해 컨텍스트에 1,024 토큰 이상은 절대 없습니다. 이것도 현대 기준으로는 작습니다. 오늘날 컨텍스트 길이는 수십만 개나 백만 개에 훨씬 가까울 것이므로 훨씬 더 많은 컨텍스트, 기록에 훨씬 더 많은 토큰이 있고

[33:00] 그런 식으로 시퀀스에서 다음 토큰에 대해 훨씬 더 나은 예측을 할 수 있습니다. 마지막으로 GPT-2는 약 1,000억 개의 토큰으로 훈련되었고, 이것도 현대 기준으로는 상당히 작습니다. 언급했듯이 우리가 본 FineWeb 데이터셋은 15조 개의 토큰을 가지고 있으므로 1,000억은 상당히 작습니다. 실제로 재미로 GPT-2를 재현하려고 했는데, llm.c라는 프로젝트의 일환입니다.

[33:30] GitHub의 llm.c 저장소 아래 이 포스트에서 제가 그것을 한 재현을 볼 수 있습니다. 특히 2019년에 GPT-2를 훈련하는 비용은 약 $40,000로 추정되었지만, 오늘날에는 훨씬 더 잘할 수 있습니다. 특히 여기서는 약 하루와 약 $600가 들었지만 너무 열심히 하지도 않았고, 오늘날에는 약 $100까지 낮출 수 있다고 생각합니다. 왜 비용이

[34:00] 이렇게 많이 내려갔을까요? 첫째, 이 데이터셋들이 훨씬 좋아졌고 필터링, 추출, 준비하는 방식이 훨씬 정교해져서 데이터셋의 품질이 훨씬 높아졌습니다. 그것이 한 가지이지만, 정말 가장 큰 차이는 컴퓨터가 하드웨어 측면에서 훨씬 빨라졌다는 것이고, 잠시 후에 살펴보겠습니다. 또한 이 모델들을 실행하고 가능한 한 하드웨어에서 모든 속도를 짜내는

[34:30] 소프트웨어도 모든 사람들이 이 모델들에 집중하고 매우 빠르게 실행하려고 하면서 훨씬 좋아졌습니다. 이 GPT-2 재현의 전체 세부 사항을 다루지는 못하고 이것은 긴 기술 포스트이지만, 연구자로서 이 모델 중 하나를 실제로 훈련하는 것이 어떤 모습인지, 무엇을 보고 어떤 느낌인지 직관적인 감각을 드리고 싶습니다. 조금 보여드리겠습니다. 자, 이게 어떻게 생겼는지 보여드리겠습니다. 이것을 옆으로 밀겠습니다. 여기서 지금 GPT-2 모델을 훈련하고 있고

[35:00] 일어나는 것은 여기 모든 한 줄, 이것처럼, 이것이 모델에 대한 한 번의 업데이트입니다. 여기서 이 모든 토큰들에서 예측을 더 잘하게 만들고 신경망의 가중치나 파라미터를 업데이트하는 것을 기억하세요. 여기 모든 한 줄은 다음 토큰을 예측하는 것을 더 잘하도록 파라미터를 조금씩 바꾸는 신경망에 대한 한 번의 업데이트입니다. 특히 여기 모든 한 줄은

[35:30] 훈련 세트의 100만 개 토큰에 대한 예측을 개선하는 것입니다. 기본적으로 이 데이터셋에서 100만 개의 토큰을 꺼내서 그 토큰이 시퀀스에서 다음으로 오는 예측을 100만 개 모두 동시에 개선하려고 했고, 이 모든 단계에서 네트워크에 대한 업데이트를 합니다. 가까이 봐야 할 숫자는 이

[36:00] loss라는 숫자입니다. loss는 신경망이 지금 얼마나 잘 수행하고 있는지 알려주는 단일 숫자이고 낮은 loss가 좋도록 만들어졌습니다. 신경망에 더 많은 업데이트를 하면서 loss가 감소하는 것을 볼 수 있고, 이것은 시퀀스에서 다음 토큰에 대해 더 나은 예측을 하는 것에 해당합니다. loss는 신경망 연구자로서 보는 숫자이고 손가락을 빙빙 돌리고, 커피를 마시며, 이것이 잘 보이는지

[36:30] 확인합니다. 모든 업데이트마다 loss가 개선되고 네트워크가 예측을 더 잘하게 되도록요. 여기서 업데이트당 100만 개의 토큰을 처리하고, 각 업데이트는 대략 7초 정도 걸리고, 총 32,000 단계의 최적화를 처리할 것입니다. 32,000 단계에 각각 100만 개 토큰이면 약 330억 개의 토큰을 처리할 것이고, 현재 32,000 중 약 420

[37:00] 단계이므로 아직 1%를 조금 넘게만 완료한 것입니다. 10분이나 15분 정도밖에 실행하지 않았기 때문입니다. 20 단계마다 이 최적화가 추론을 하도록 설정했습니다. 여기서 보는 것은 모델이 시퀀스에서 다음 토큰을 예측하는 것입니다. 무작위로 시작하고 토큰을 계속 넣습니다. 추론 단계를 실행하고 있고 모델이 시퀀스에서 다음 토큰을 예측하는 것입니다. 무언가가 나타날 때마다 그것이 새

[37:30] 토큰입니다. 이것을 보면 아직 그다지 일관성이 없다는 것을 알 수 있고, 훈련의 1%밖에 완료되지 않았다는 것을 기억하세요. 모델은 아직 시퀀스에서 다음 토큰을 예측하는 것을 잘 못하므로 나오는 것이 실제로 약간 횡설수설합니다. 하지만 여전히 약간의 지역적 일관성이 있습니다. "since she is mine it's a part of the information should discuss my father great companions Gordon showed me sitting over at" 등등...

[38:00] 그다지 좋아 보이지 않는다는 것을 알지만, 실제로 위로 스크롤해서 최적화를 시작했을 때 어떻게 생겼는지 봅시다. 1단계에서요. 20 단계의 최적화 후에 얻는 것은 완전히 무작위로 보이고, 물론 그것은 모델이 파라미터에 20번의 업데이트만 했기 때문입니다. 무작위 네트워크이므로 무작위 텍스트를 주는 것입니다. 적어도 이것과 비교하면

[38:30] 모델이 훨씬 더 잘하기 시작했다는 것을 알 수 있고, 실제로 전체 32,000 단계를 기다리면 모델은 실제로 상당히 일관된 영어를 생성하고 토큰 스트림이 올바르게 되는 지점까지 개선될 것입니다. 훨씬 더 잘 영어를 만들어냅니다. 이것은 하루나 이틀 더 실행해야 합니다. 이 단계에서 우리는 loss가 감소하고 모든 것이 잘 보이는지 확인하고

[39:00] 기다리기만 하면 됩니다. 이제 필요한 계산의 이야기로 넘어가겠습니다. 물론 이 최적화를 제 노트북에서 실행하고 있지 않습니다. 너무 비쌀 것입니다. 이 신경망을 실행하고 개선해야 하고 이 모든 데이터가 필요하기 때문입니다. 네트워크가 너무 크기 때문에 컴퓨터에서 이것을 잘 실행할 수 없습니다. 이 모든 것은 클라우드 어딘가에 있는 컴퓨터에서 실행되고 있고, 기본적으로 이 모델들을 훈련하는 계산 측면의

[39:30] 이야기와 그게 어떻게 생겼는지 다루고 싶습니다. 살펴봅시다. 이 최적화를 실행하고 있는 컴퓨터는 이 8X H100 노드입니다. 단일 노드 또는 단일 컴퓨터에 8개의 H100이 있습니다. 이 컴퓨터를 임대하고 있고 클라우드 어딘가에 있습니다. 실제로 물리적으로 어디 있는지 확실하지 않습니다. 제가 좋아하는 임대 장소는 Lambda이지만 이 서비스를 제공하는 많은 다른 회사들이 있습니다. 스크롤하면

[40:00] 이 H100을 가진 컴퓨터들의 온디맨드 가격이 있는 것을 볼 수 있습니다. H100은 GPU이고 잠시 후에 어떻게 생겼는지 보여드리겠습니다. 온디맨드 8개 Nvidia H100 GPU, 이 기계는 예를 들어 GPU당 시간당 $3입니다. 이것들을 임대하면 클라우드에 기계를 얻고 들어가서 이 모델들을 훈련할 수 있습니다. 이 GPU들은 이렇게 생겼습니다. 이것이 하나의 H100 GPU이고

[40:30] 이렇게 생겼습니다. 컴퓨터에 이것을 꽂고, GPU는 네트워크를 훈련하기에 완벽한 적합입니다. 계산적으로 매우 비싸지만 계산에서 많은 병렬성을 보여주기 때문입니다. 이 신경망 훈련의 기반인 행렬 곱셈을 풀면서 많은 독립적인 작업자들이 동시에 작업할 수 있습니다. 이것은 이 H100 중 하나일 뿐이지만 실제로 여러 개를

[41:00] 함께 넣습니다. 8개를 단일 노드에 쌓을 수 있고, 여러 노드를 전체 데이터 센터나 전체 시스템 센터에 쌓을 수 있습니다. 데이터 센터를 보면 이렇게 생긴 것들을 보기 시작합니다. GPU 하나가 8개의 GPU가 되고 단일 시스템이 되고 많은 시스템이 됩니다. 이것들은 더 큰 데이터 센터이고 물론 훨씬 더 비쌀 것입니다. 일어나는 것은 모든

[41:30] 대형 기술 회사들이 이 GPU를 정말 원한다는 것입니다. 매우 강력하기 때문에 이 모든 언어 모델을 훈련할 수 있고, 이것이 근본적으로 Nvidia의 주가를 오늘 예를 들어 $3.4조로 끌어올린 것이고 왜 Nvidia가 폭발적으로 성장했는지입니다. 이것이 골드러시입니다. 골드러시는 GPU를 얻는 것이고, 충분히 얻어서 모두 협력하여 이 최적화를 수행하고, 모두

[42:00] 무엇을 하고 있냐면 모두 협력하여 FineWeb 데이터셋 같은 데이터셋에서 다음 토큰을 예측하는 것입니다. 이것이 기본적으로 극도로 비싼 계산 워크플로우이고, GPU가 많을수록 예측하고 개선하려는 토큰이 많아지고, 이 데이터셋을 더 빨리 처리하고, 더 빨리 반복하고, 더 큰 네트워크를 얻고, 더 큰 네트워크를 훈련할 수 있습니다. 이것이 모든 기계들이

[42:30] 하고 있는 것이고 이것이 왜 이 모든 것이 큰 일인지입니다. 예를 들어 이것은 한 달 전쯤의 기사인데, 일론 머스크가 단일 데이터 센터에 100,000개의 GPU를 얻는 것이 왜 큰 일인지입니다. 이 모든 GPU는 극도로 비싸고, 엄청난 전력이 필요할 것이고, 모두 시퀀스에서 다음 토큰을 예측하려고 하고 그렇게 해서 네트워크를 개선하고, 아마 여기서 보는 것보다 훨씬 더 일관된 텍스트를 훨씬 더 빨리 얻을 것입니다. 불행히도 저는 정말 큰 모델을 훈련하는 데 몇천만 달러나 억 달러를 쓸 돈이 없습니다.

---

## 8. Llama 3.1 Base Model Inference (Llama 3.1 기본 모델 추론)

(이 챕터는 다음 번역 청크에서 처리됩니다)

---

## 9. 사전학습에서 후속학습으로

**요약**: 사전학습(pretraining)에서 후속학습(post-training)으로 전환하는 과정을 설명합니다. 베이스 모델은 인터넷 텍스트를 모방할 뿐이지만, 후속학습을 통해 유용한 어시스턴트로 변환됩니다. 이 단계가 ChatGPT와 같은 대화형 AI를 만드는 핵심입니다.

[59:30] 자, 여기서 한 발 물러나서 지금까지 다룬 내용을 정리해보겠습니다. 우리는 ChatGPT 같은 LLM 어시스턴트를 학습시키고 싶습니다. 첫 번째 단계인 사전학습에 대해 이야기했는데, 핵심은 이렇습니다. 인터넷 문서를 가져와서 토큰이라는 작은 텍스트 조각의 원자 단위로 쪼갠 다음, 신경망을 사용해 토큰 시퀀스를 예측합니다. 이 전체 단계의 결과물이 바로 베이스 모델입니다. 이것은 곧 신경망의

[1:00:00] 파라미터 설정값입니다. 이 베이스 모델은 기본적으로 토큰 수준에서 인터넷 문서를 시뮬레이션하는 것입니다. 인터넷 문서와 동일한 통계적 특성을 가진 토큰 시퀀스를 생성할 수 있죠. 일부 애플리케이션에서 사용할 수 있다는 것을 봤지만, 실제로는 더 나은 것이 필요합니다. 우리는 어시스턴트를 원합니다. 질문을 할 수 있고, 모델이 답변을 주기를 원하죠. 그래서 이제 두 번째 단계인 후속학습 단계로 넘어가야 합니다. 베이스 모델, 즉 인터넷 문서 시뮬레이터를 가져와서

[1:00:30] 후속학습에 넘깁니다. 이제 모델의 후속학습이라 불리는 것을 수행하는 몇 가지 방법에 대해 논의하겠습니다. 후속학습의 이러한 단계들은 계산 비용이 훨씬 적습니다. 대규모 데이터 센터의 모든 계산 작업, 무거운 컴퓨팅, 수백만 달러는 모두 사전학습 단계에 들어갑니다. 하지만 이제 비용은 조금 적지만 여전히 매우 중요한 후속학습 단계로 들어갑니다. 여기서 LLM 모델을 어시스턴트로 바꾸죠. 그럼

[1:01:00] 모델이 인터넷 문서를 샘플링하는 대신 질문에 답변하도록 어떻게 만들 수 있는지 살펴보겠습니다. 다시 말해, 우리가 하고 싶은 것은 대화에 대해 생각하기 시작하는 것입니다. 이 대화들은 여러 턴으로 이루어질 수 있습니다. 가장 간단한 경우는 인간과 어시스턴트 사이의 대화입니다. 예를 들어, 대화가 이렇게 생겼다고 상상할 수 있습니다. 인간이 "2 더하기 2는 뭐야?"라고 물으면, 어시스턴트는 "2 더하기 2는 4입니다"라고 응답해야 합니다. 인간이 후속 질문으로

---

## 10. 후속학습 데이터 (대화)

**요약**: 후속학습 데이터인 대화 데이터셋에 대해 설명합니다. 인간 라벨러들이 수만 개의 고품질 대화 예시를 작성하고, 이를 모델에 학습시킵니다. 이 과정을 지도학습 미세조정(SFT)이라 하며, 모델이 어시스턴트처럼 행동하도록 만듭니다.

[1:01:30] 더하기 대신 곱하기라면 어떻게 되냐고 물으면 어시스턴트는 이렇게 응답할 수 있습니다. 이 예시에서는 어시스턴트가 어떤 종류의 성격을 가질 수도 있다는 것을 보여줍니다. 친절한 느낌이죠. 세 번째 예시에서는 인간이 우리가 도와주고 싶지 않은 것을 요청할 때, 거부라는 것을 생성할 수 있다는 것을 보여줍니다. "도와드릴 수 없습니다"라고 말할 수 있죠. 다시 말해, 우리가 지금 하고 싶은 것은 시스템이 인간과 어떻게 상호작용해야 하는지 생각하고, 이러한 대화에서 어시스턴트와 그 행동을 프로그래밍하는 것입니다.

[1:02:00] 이것이 신경망이기 때문에 우리는 이것들을 코드로 명시적으로 프로그래밍하지 않을 것입니다. 그런 방식으로 어시스턴트를 프로그래밍할 수 없습니다. 신경망이기 때문에 모든 것은 데이터셋에서의 신경망 학습을 통해 이루어집니다. 그래서 우리는 대화 데이터셋을 만들어서 암묵적으로 어시스턴트를 프로그래밍할 것입니다. 이것은 데이터셋에 있는 세 개의 독립적인 대화 예시입니다. 실제 데이터셋에서 제가 보여드릴 예시들은

[1:02:30] 훨씬 더 클 것입니다. 수십만 개의 대화가 있을 수 있고, 여러 턴으로 이루어지고, 매우 길 수 있으며, 다양한 주제를 다룹니다. 하지만 여기서는 세 가지 예시만 보여드립니다. 기본적으로 이것이 작동하는 방식은 어시스턴트가 예시에 의해 프로그래밍된다는 것입니다. 그리고 이 데이터는 어디서 오는 걸까요? 2 곱하기 2는 4, 2 더하기 2와 같다, 등등 어디서 오는 걸까요? 이것은 인간 라벨러에게서 옵니다. 우리는 인간 라벨러에게 대화 맥락을 제공하고

[1:03:00] 이 상황에서 이상적인 어시스턴트 응답을 작성하도록 요청합니다. 인간이 어떤 상황에서든 어시스턴트의 이상적인 응답을 작성합니다. 그런 다음 모델이 이것을 학습하고 그런 종류의 응답을 모방하도록 합니다. 그래서 이것이 작동하는 방식은, 사전학습 단계에서 만든 베이스 모델을 가져옵니다. 이 베이스 모델은 인터넷 문서에서 학습되었습니다. 이제 그 인터넷 문서 데이터셋을 버리고 새로운 데이터셋으로

[1:03:30] 대체할 것입니다. 그것은 대화 데이터셋이 될 것입니다. 이 새로운 대화 데이터셋에서 모델을 계속 학습시킵니다. 그러면 모델이 매우 빠르게 조정되어 어시스턴트가 인간 쿼리에 어떻게 응답하는지의 통계를 배우게 됩니다. 나중에 추론 시에 어시스턴트를 프라이밍하고 응답을 얻을 수 있으며, 그 상황에서 인간 라벨러가 했을 것을 모방하게 됩니다.

[1:04:00] 이해가 되셨기를 바랍니다. 예시를 보면서 조금 더 구체적으로 설명하겠습니다. 이 후속학습 단계에서 기본적으로 모델을 계속 학습시키지만, 사전학습 단계는 실제로 수천 대의 컴퓨터에서 약 3개월의 학습이 필요합니다. 후속학습 단계는 일반적으로 3시간 정도로 훨씬 짧습니다. 왜냐하면 여기서 수동으로 만드는 대화 데이터셋이 인터넷 텍스트 데이터셋보다 훨씬 작기 때문입니다.

[1:04:30] 그래서 이 학습은 매우 짧지만, 근본적으로 우리는 베이스 모델을 가져와서 정확히 같은 알고리즘, 정확히 같은 모든 것을 사용해 계속 학습시킵니다. 단지 데이터셋을 대화로 바꾸는 것뿐입니다. 그래서 질문은 이 대화들이 무엇이고, 어떻게 표현하며, 모델이 원시 텍스트 대신 대화를 보도록 어떻게 만드는가, 그리고 이런 종류의 학습의 결과가 무엇이고, 모델에 대해 이야기할 때 어떤 심리적 의미에서 무엇을 얻는가 입니다.

[1:05:00] 이제 그 질문들로 넘어가겠습니다. 대화의 토큰화에 대해 이야기하는 것부터 시작합시다. 이 모델들에서 모든 것은 토큰으로 변환되어야 합니다. 모든 것이 토큰 시퀀스에 관한 것이기 때문입니다. 그래서 대화를 토큰 시퀀스로 어떻게 변환하느냐가 문제입니다. 이를 위해 어떤 종류의 인코딩을 설계해야 합니다. 이것은 익숙하시다면, 꼭 그럴 필요는 없지만, 예를 들어 인터넷의 TCP/IP 패킷과 비슷합니다. 정보를 어떻게 표현하고 모든 것이 어떻게 구조화되는지에 대한 정확한 규칙과 프로토콜이 있습니다.

[1:05:30] 그래서 모든 종류의 데이터가 문서에 기록되어 모든 사람이 동의할 수 있는 방식으로 배치됩니다. LLM에서도 같은 일이 일어나고 있습니다. 어떤 종류의 데이터 구조가 필요하고, 대화 같은 데이터 구조가 토큰으로 인코딩되고 디코딩되는 방식에 대한 규칙이 필요합니다. 이제 이 대화를 토큰 공간에서 어떻게 재현하는지 보여드리겠습니다. Tiktokenizer에 가서 그 대화를 가져올 수 있고 이것이

[1:06:00] 언어 모델에서 어떻게 표현되는지입니다. 여기서 우리는 이 두 턴 대화에서 사용자와 어시스턴트를 번갈아 가며 반복합니다. 여기서 보이는 것은 보기 좋지 않지만 실제로는 비교적 간단합니다. 끝에서 토큰 시퀀스로 변환되는 방식은 약간 복잡하지만, 결국 사용자와 어시스턴트 사이의 이 대화는 49개의 토큰이 됩니다. 49개 토큰의 1차원 시퀀스이고, 이것이 그 토큰들입니다. 모든 다른 LLM들은

[1:06:30] 약간 다른 형식이나 프로토콜을 가지고 있고, 지금은 다소 무법지대입니다. 하지만 예를 들어 GPT-4o는 다음과 같은 방식으로 합니다. "im_start"라는 특수 토큰이 있고, 이것은 IM(imaginary monologue, 상상의 독백)의 시작을 줄인 것입니다. 그런 다음 누구의 턴인지 지정해야 합니다. 솔직히 왜 그렇게 불리는지 모르겠습니다. 예를 들어 토큰 4인 user를

[1:07:00] 지정합니다. 그런 다음 내부 독백 구분자가 있고, 정확한 질문, 즉 질문의 토큰들이 옵니다. 그런 다음 닫아야 합니다. im_end, 상상의 독백의 끝입니다. 기본적으로 "2 더하기 2는 뭐야?"라는 사용자의 질문은 이 토큰들의 토큰 시퀀스가 됩니다. 여기서 중요한 점은 im_start는 텍스트가 아니라는 것입니다. im_start는 추가되는 특수 토큰이고,

[1:07:30] 새로운 토큰입니다. 이 토큰은 지금까지 한 번도 학습되지 않았습니다. 후속학습 단계에서 만들어서 도입하는 새 토큰입니다. 그래서 im_sep, im_start 등의 특수 토큰이 도입되어 텍스트와 섞입니다. 모델이 "이것은 턴의 시작이고, 누구를 위한 것인가, 사용자를 위한 턴의 시작이고, 이것이 사용자가 말하는 것이고, 사용자가 끝나고, 새로운 턴의 시작이고,

[1:08:00] 어시스턴트의 턴이고, 어시스턴트가 무엇을 말하는가, 이것이 어시스턴트가 말하는 토큰들이다" 등을 배우도록 합니다. 그래서 이 대화는 토큰 시퀀스로 변환되고, 여기서의 구체적인 세부사항은 실제로 그렇게 중요하지 않습니다. 제가 구체적인 용어로 보여드리려는 것은 우리가 구조화된 객체로 생각하는 대화가 어떤 인코딩을 통해 1차원 토큰 시퀀스로 변환된다는 것입니다. 1차원 토큰 시퀀스이기 때문에

[1:08:30] 이전에 적용했던 모든 것을 적용할 수 있습니다. 이제 그냥 토큰 시퀀스이고 언어 모델을 학습시킬 수 있습니다. 이전처럼 시퀀스에서 다음 토큰을 예측하는 것입니다. 대화를 표현하고 학습할 수 있습니다. 그러면 테스트 시, 추론 중에는 어떻게 보일까요? 모델을 학습시켰다고 해봅시다. 이런 종류의 대화 데이터셋에서 학습시켰고, 이제 추론을 하고 싶습니다. ChatGPT에 있을 때 추론 중에는 어떻게 보일까요? ChatGPT에 가서

[1:09:00] 대화를 나눕니다. 이것이 작동하는 방식은 기본적으로, 이것이 이미 채워져 있다고 해봅시다. "2 더하기 2는 뭐야", "2 더하기 2는 4입니다". 이제 "곱하기라면 어떻게 돼?" im_end라고 입력합니다. OpenAI 같은 회사의 서버에서 기본적으로 일어나는 일은 im_start assistant im_sep을 넣고 여기서 끝냅니다. 이 컨텍스트를 구성하고 이제 모델에서 샘플링을 시작합니다. 이

[1:09:30] 단계에서 모델에 가서 "좋은 첫 번째 토큰이 뭐야? 좋은 두 번째 토큰이 뭐야? 좋은 세 번째 토큰이 뭐야?"라고 물어봅니다. 여기서 LLM이 인계받아 응답을 생성합니다. 예를 들어 이런 응답을 만들지만, 이것과 동일할 필요는 없습니다. 이런 종류의 대화가 데이터셋에 있었다면 이런 느낌이 날 것입니다. 대략 이것이 프로토콜이 작동하는 방식이지만, 이 프로토콜의 세부사항은 중요하지 않습니다.

[1:10:00] 다시 말하지만 제 목표는 모든 것이 결국 1차원 토큰 시퀀스가 된다는 것을 보여주는 것입니다. 그래서 이미 본 모든 것을 적용할 수 있지만, 이제 대화에서 학습하고 대화를 생성합니다. 자, 이제 이 데이터셋이 실제로 어떻게 생겼는지 살펴보겠습니다. 제가 보여드리고 싶은 첫 번째 논문과 이 방향의 첫 번째 노력은 2022년 OpenAI의 이 논문입니다. 이 논문은 InstructGPT라고 불렸고, 그들이 개발한 기술이었습니다. 이것이 OpenAI가

[1:10:30] 언어 모델을 가져와서 대화에서 파인튜닝하는 방법에 대해 이야기한 최초의 논문이었습니다. 이 논문에는 제가 살펴보고 싶은 여러 세부사항이 있습니다. 첫 번째로 가고 싶은 곳은 섹션 3.4입니다. 여기서 그들이 고용한 인간 계약자에 대해 이야기합니다. 이 경우 Upwork나 Scale AI를 통해 이 대화들을 구성하기 위해 고용했습니다. 이 대화들을 만드는 것이 직업인 인간 라벨러들이 관여하고, 이 라벨러들은 프롬프트를 만들고

[1:11:00] 이상적인 어시스턴트 응답도 완성하도록 요청받습니다. 이것이 사람들이 만든 종류의 프롬프트입니다. 인간 라벨러들이죠. "내 커리어에 대한 열정을 되찾을 수 있는 다섯 가지 아이디어를 나열해줘", "읽어야 할 SF 소설 상위 10개는 뭐야" 등 많은 다양한 유형의 프롬프트가 있습니다. "이 문장을 스페인어로 번역해줘" 등. 사람들이 만든 많은 것들이 있습니다. 먼저 프롬프트를 만들고, 그 프롬프트에 답하고

[1:11:30] 이상적인 어시스턴트 응답을 제공합니다. 그런데 이 프롬프트에 대해 작성해야 하는 이상적인 어시스턴트 응답이 무엇인지 어떻게 알까요? 조금 더 아래로 스크롤하면, 인간 라벨러에게 주어지는 라벨링 지침의 발췌가 있습니다. 언어 모델을 개발하는 회사, 예를 들어 OpenAI가 인간이 이상적인 응답을 만드는 방법에 대한 라벨링 지침을 작성합니다. 예를 들어 이런 종류의 라벨링 지침의 발췌입니다. 높은 수준에서 사람들에게 도움이 되고, 진실되고, 해롭지 않게 하라고 요청합니다.

[1:12:00] 여기서 더 보고 싶으시면 비디오를 멈추셔도 됩니다. 높은 수준에서 기본적으로 그냥 대답하고, 도움이 되려 하고, 진실되려 하고, 시스템이 나중에 ChatGPT에서 처리하기를 원하지 않는 질문에는 답하지 않습니다. 대략적으로 회사가 라벨링 지침을 만들고, 보통 이렇게 짧지 않고 수백 페이지가 되며, 사람들이 전문적으로 공부해야 하고, 그런 다음 그 라벨링

[1:12:30] 지침을 따라 이상적인 어시스턴트 응답을 작성합니다. 이 논문에서 설명된 대로 이것은 매우 인간 집약적인 프로세스입니다. InstructGPT의 데이터셋은 실제로 OpenAI에서 공개되지 않았지만, 이런 종류의 설정을 따르고 자체 데이터를 수집하려는 오픈소스 재현이 있습니다. 제가 익숙한 예로 예전 Open Assistant의 노력이 있습니다. 많은 예시 중 하나일 뿐이지만 예시를 보여드리고 싶습니다.

[1:13:00] 이것은 OpenAI가 인간 라벨러와 했던 것과 비슷하게 이런 대화를 만들도록 요청받은 인터넷의 사람들이었습니다. 여기 한 사람이 만든 항목이 있습니다. "경제학에서 독점(monopoly)이라는 용어의 관련성에 대한 짧은 소개를 써줄 수 있어? 예시를 사용해줘" 등. 그런 다음 같은 사람이나 다른 사람이 응답을 작성합니다. 여기 이것에 대한 어시스턴트 응답이 있습니다. 같은 사람이나 다른 사람이 이 이상적인 응답을 작성합니다. 그리고 이것은

[1:13:30] 대화가 어떻게 계속될 수 있는지의 예시입니다. "개에게 설명해봐" 그러면 조금 더 간단한 설명을 시도할 수 있습니다. 이것이 레이블이 되고, 이것에서 학습합니다. 학습 중에 일어나는 일은, 물론 테스트 시 추론 중에 모델이 접할 모든 가능한 질문을 완전히 커버할 수 없습니다. 사람들이 미래에 물어볼 모든 가능한 프롬프트를 커버할 수 없습니다.

[1:14:00] 하지만 이런 예시들의 데이터셋이 있다면, 학습 중에 모델이 이 도움이 되고, 진실되고, 해롭지 않은 어시스턴트의 페르소나를 취하기 시작합니다. 모든 것이 예시로 프로그래밍됩니다. 이것들은 모두 행동의 예시이고, 이런 예시 행동의 대화가 10만 개 정도 있고 학습하면, 모델이 통계적 패턴을 이해하기 시작하고 이

[1:14:30] 어시스턴트의 성격을 취합니다. 테스트 시에 정확히 같은 질문을 받으면, 답이 학습 세트에 있던 것과 정확히 같이 암송될 수 있습니다. 하지만 더 가능성이 높은 것은 모델이 비슷한 분위기의 뭔가를 할 것이고, 이것이 원하는 종류의 답이라는 것을 이해할 것입니다. 그래서 우리가 하는 것은 예시로 시스템을 프로그래밍하는 것이고, 시스템은 통계적으로 이

[1:15:00] 도움이 되고, 진실되고, 해롭지 않은 어시스턴트의 페르소나를 채택합니다. 이것은 회사가 만드는 라벨링 지침에 반영되어 있습니다. 지난 2-3년간 InstructGPT 논문 이후 최신 기술이 발전했다는 것을 보여드리고 싶습니다. 특히 더 이상 인간이 혼자서 모든 무거운 작업을 하는 것은 흔하지 않습니다. 이제 언어 모델이 있고, 이 언어 모델이 데이터셋과 대화를 만드는 것을 돕기 때문입니다. 사람들이 처음부터 응답을 문자 그대로 작성하는 것은 매우 드뭅니다.

[1:15:30] 기존 LLM을 사용해서 답을 만들고 편집하는 것이 훨씬 더 가능성이 높습니다. LLM이 이 후속학습 스택에 스며들기 시작한 여러 다른 방법이 있고, LLM은 기본적으로 이 대규모 대화 데이터셋을 만드는 데 광범위하게 사용됩니다. Ultra Chat은 더 현대적인 대화 데이터셋의 한 예시입니다. 매우 큰 부분이 합성이지만 인간 개입이 있다고 생각합니다.

[1:16:00] 잘못 알 수도 있지만 보통 약간의 인간 개입이 있지만 엄청난 양의 합성 도움이 있습니다. 이것은 모두 다양한 방식으로 구성되고, Ultra Chat은 현재 존재하는 많은 SFT 데이터셋 중 하나의 예시일 뿐입니다. 보여드리고 싶은 유일한 것은 이 데이터셋이 이제 수백만 개의 대화를 가지고 있고, 이 대화들은 대부분 합성이지만 어느 정도 인간에 의해 편집되었을 것이고, 매우 다양한

[1:16:30] 영역 등을 포괄한다는 것입니다. 이것은 이제 상당히 광범위한 결과물이고, 이런 SFT 혼합물이라고 불리는 것이 있습니다. 다양한 유형과 소스의 혼합이 있고, 부분적으로 합성이고 부분적으로 인간이며, 그런 방향으로 갔습니다. 하지만 대략적으로 우리는 여전히 SFT 데이터셋이 있고, 대화로 구성되어 있고, 이전처럼 학습합니다. 마지막으로 언급하고 싶은 것은

[1:17:00] ChatGPT 같은 AI와 대화하는 것의 마법을 조금 해소하고 싶다는 것입니다. ChatGPT에 가서 질문을 하고 엔터를 치면, 돌아오는 것은 학습 세트에서 일어나는 것과 통계적으로 정렬되어 있습니다. 이 학습 세트들은 실제로 라벨링 지침을 따르는 인간들에 시작점이 있습니다. ChatGPT에서 실제로 무엇과 대화하고 있는 걸까요? 어떻게 생각해야 할까요? 마법 같은 AI에서 오는 것이 아닙니다. 대략적으로 말하면

[1:17:30] 인간 라벨러를 통계적으로 모방하는 것에서 오고, 이것은 이 회사들이 작성한 라벨링 지침에서 옵니다. 이 모방을 하는 것이죠. ChatGPT에서 주어지는 답이 인간 라벨러의 어떤 종류의 시뮬레이션이라고 상상해보세요. 이런 종류의 대화에서 인간 라벨러가 뭐라고 할까를 묻는 것과 같습니다. 이 인간

[1:18:00] 라벨러는 인터넷의 랜덤한 사람이 아닙니다. 이 회사들이 실제로 전문가를 고용하기 때문입니다. 예를 들어 코드에 대한 질문을 할 때, 이 대화 데이터셋 생성에 관여하는 인간 라벨러들은 보통 교육받은 전문가입니다. 그런 사람들의 시뮬레이션에게 질문하는 것과 같습니다. 마법 같은 AI와 대화하는 것이 아니라 평균적인 라벨러와 대화하는 것입니다. 이 평균적인 라벨러는 아마도 상당히 숙련되어 있지만, 이런 종류의

[1:18:30] 사람의 즉각적인 시뮬레이션과 대화하는 것입니다. 이 데이터셋 구성에 고용될 사람이죠. 넘어가기 전에 한 가지 더 구체적인 예시를 보여드리겠습니다. 예를 들어 ChatGPT에 가서 "파리에서 봐야 할 상위 5개 랜드마크를 추천해줘"라고 말하고 엔터를 치면 여기서 나오는 것은 어떻게 생각해야 할까요?

[1:19:00] 나가서 모든 랜드마크를 조사하고 무한한 지능으로 순위를 매긴 마법 같은 AI가 아닙니다. 얻는 것은 OpenAI가 고용한 라벨러의 통계적 시뮬레이션입니다. 대략 그렇게 생각할 수 있습니다. 이 특정 질문이 OpenAI의 후속학습 데이터셋에 어딘가에 있다면, 인간 라벨러가 그 다섯 랜드마크에 대해 적었을 것과 매우 유사한 답을 볼 가능성이 높습니다. 인간 라벨러가 이것을 어떻게 만들까요? 나가서 인터넷에서

[1:19:30] 20분 동안 자체 조사를 하고 목록을 만듭니다. 이 목록을 만들어 데이터셋에 있으면, 어시스턴트의 정답으로 제출한 것을 볼 가능성이 매우 높습니다. 이 특정 쿼리가 후속학습 데이터셋의 일부가 아니라면, 여기서 얻는 것은 조금 더 창발적입니다. 모델이 통계적으로 이 학습 세트에 있는 랜드마크의 종류를 이해하기 때문입니다. 보통 유명한 랜드마크, 사람들이 보통 보고 싶어하는 랜드마크,

[1:20:00] 인터넷에서 자주 언급되는 랜드마크입니다. 모델이 이미 인터넷에서의 사전학습에서 엄청난 지식을 가지고 있다는 것을 기억하세요. 파리, 랜드마크, 사람들이 보고 싶어하는 것에 대한 수많은 대화를 봤을 것입니다. 후속학습 데이터셋과 결합된 사전학습 지식이 이런 종류의 모방을 만들어냅니다. 대략 이것이 뒤에서 일어나고 있는 일을

[1:20:30] 통계적 의미에서 생각할 수 있는 방법입니다. 자, 이제 제가 LLM 심리학이라고 부르고 싶은 주제로 넘어가겠습니다. 이 모델들의 학습 파이프라인에서 나타나는 창발적 인지 효과가 무엇인지입니다. 특히 첫 번째로 이야기하고 싶은 것은 물론 환각입니다. 모델 환각에 대해 익숙하실 수 있습니다. LLM이 정보를 지어냅니다. 완전히 정보를 날조합니다. LLM 어시스턴트의 큰 문제입니다.

---

## 11. 환각, 도구 사용, 지식/작업 메모리

**요약**: LLM의 환각(hallucination) 문제와 도구 사용(tool use)을 설명합니다. 모델은 지식이 파라미터에 저장되어 있어 정확하지 않을 수 있으며, 검색이나 코드 실행 같은 외부 도구를 사용하여 이를 보완합니다. 지식 메모리(파라미터)와 작업 메모리(컨텍스트)의 차이도 다룹니다.

[1:21:00] 이 문제는 몇 년 전 초기 모델들에서 상당히 존재했고, 잠시 후에 설명할 몇 가지 완화 방법이 있어서 문제가 조금 나아졌다고 생각합니다. 지금은 이 환각이 어디서 오는지 이해해봅시다. 여기 학습 세트에 있을 수 있다고 생각할 수 있는 세 가지 대화의 구체적인 예시가 있습니다. 학습 세트에 있을 수 있는 꽤 합리적인 대화입니다. 예를 들어, 톰 크루즈가 누구냐, 톰 크루즈는 유명한 미국 배우이고

[1:21:30] 프로듀서입니다. 존 바라소가 누구냐, 예를 들어 미국 상원의원입니다. 칭기즈 칸이 누구냐, 칭기즈 칸은 어쩌고저쩌고였습니다. 학습 시 대화가 이렇게 생겼을 수 있습니다. 이것의 문제는 인간이 각 경우에 어시스턴트의 정답을 작성할 때, 인간이 이 사람이 누군지 알거나 인터넷에서 조사하고 와서 이런 확신 있는 톤의 응답을 작성합니다.

[1:22:00] 테스트 시에 이 사람이 누구냐고 물으면 - 이것은 제가 완전히 무작위로 만든 이름이고, 제가 알기로는 존재하지 않습니다, 무작위로 생성하려 했습니다 - 오슨 코바츠가 누구냐고 물으면 문제는 어시스턴트가 "모르겠습니다"라고 말하지 않을 것입니다. 어시스턴트와 언어 모델 자체가 내부 특성, 활성화, 일종의 두뇌 안에서 이 사람이

[1:22:30] 익숙하지 않다는 것을 어떤 의미에서 알 수 있더라도, 네트워크의 어떤 부분이 어떤 의미에서 그것을 안다고 해도, "이 사람이 누군지 모르겠습니다"라고 말하는 것은 일어나지 않습니다. 모델이 통계적으로 학습 세트를 모방하기 때문입니다. 학습 세트에서 "누구누구"라는 형식의 질문은 정답으로 확신 있게 답변됩니다. 그래서 답변의 스타일을 취하고 최선을 다할 것입니다. 통계적으로 가장 가능성 높은 추측을 주고, 기본적으로 지어냅니다.

[1:23:00] 다시 말하지만 방금 이야기했듯이 이 모델들은 인터넷에 접속하지 않고 조사하지 않습니다. 제가 통계적 토큰 텀블러라고 부르는 것입니다. 시퀀스에서 다음 토큰을 샘플링하려고 하고, 기본적으로 지어냅니다. 이것이 어떻게 보이는지 봅시다. 여기 Hugging Face의 추론 플레이그라운드가 있고, 의도적으로 Falcon 7B라는 모델을 선택했습니다. 오래된 모델이고, 몇 년 전입니다. 그래서

[1:23:30] 환각을 겪고, 언급했듯이 최근에는 시간이 지나면서 개선되었지만, 오슨 코바츠가 누구냐고 Falcon 7B instruct에게 물어봅시다. 아, 네, 오슨 코바츠는 미국 작가이자 SF 작가입니다. 완전히 거짓입니다. 환각이죠. 다시 해봅시다. 통계적 시스템이니까요. 재샘플링할 수 있습니다. 이번에는 오슨 코바츠가 1950년대 TV 쇼의 가상 캐릭터라고 합니다. 완전히 헛소리죠. 다시 해봅시다. 전직 마이너리그 야구

[1:24:00] 선수라고 합니다. 기본적으로 모델은 모르고, 모르기 때문에 많은 다른 답을 줍니다. 확률에서 샘플링하는 것입니다. 모델은 "오슨 코바츠가 누구냐 어시스턴트"라는 토큰으로 시작하고, 여기로 와서 확률을 얻고 확률에서 샘플링하고 무언가를 만들어냅니다. 그 내용은 학습 세트의 답변 스타일과 통계적으로 일치하고

[1:24:30] 그냥 그렇게 합니다. 우리는 이것을 지어낸 사실적 지식으로 경험하지만, 모델은 기본적으로 모르고 답변 형식을 모방하고 있으며, 찾아보러 가지 않을 것입니다. 그냥 답변을 모방하기 때문입니다. 그래서 이것을 어떻게 완화할 수 있을까요? 예를 들어 ChatGPT에 가서 오슨 코바츠가 누구냐고 물으면 OpenAI의 최첨단 모델에게 물으면, 이 모델은

[1:25:00] 실제로 더 똑똑합니다. 잠깐 웹 검색 중이라고 나왔습니다. 나중에 다룰 것인데, 실제로 도구 사용을 하려고 하고 어떤 이야기를 만들어냈지만, "오슨 코바츠 도구 사용 안 함"이라고 말하고 싶습니다. 웹 검색을 원하지 않습니다. 오슨 코바츠라는 잘 알려진 역사적 또는 공인이 없어서 이 모델은 지어내지 않을 것입니다.

[1:25:30] 이 모델은 모른다는 것을 알고 말해줍니다. 이 모델이 아는 사람이 아닌 것 같습니다. 그래서 우리는 어떻게든 환각을 개선했습니다. 오래된 모델에서 분명히 문제였지만, 학습 세트가 이렇게 생겼다면 이런 종류의 답을 얻는 것이 완전히 이해됩니다. 그래서 이것을 어떻게 고칠까요? 분명히 데이터셋에 어시스턴트의 정답이 모델이 모른다는 것인 예시가 필요합니다. 하지만 그 답이

[1:26:00] 모델이 실제로 모르는 경우에만 나오도록 해야 합니다. 그래서 질문은 모델이 무엇을 알고 모르는지 어떻게 아느냐는 것입니다. 경험적으로 모델을 조사해서 알아낼 수 있습니다. 예를 들어 Meta가 Llama 3 시리즈 모델에서 환각을 어떻게 다루었는지 살펴봅시다. Meta에서 발표한 이 논문에서

[1:26:30] 사실성이라고 부르는 부분으로 가면, 기본적으로 모델을 심문해서 무엇을 알고 모르는지, 지식의 경계를 파악하는 절차를 설명합니다. 그런 다음 모델이 모르는 것에 대해 정답이 모델이 모른다는 것인 예시를 학습 세트에 추가합니다. 원칙적으로는 매우 쉬워 보이지만

[1:27:00] 대략적으로 이것이 문제를 해결합니다. 문제를 해결하는 이유는 모델이 실제로 네트워크 내부에 자기 지식에 대한 꽤 좋은 모델을 가지고 있을 수 있기 때문입니다. 네트워크와 내부의 모든 뉴런을 봤을 때, 모델이 불확실할 때 켜지는 뉴런이 네트워크 어딘가에 있다고 상상할 수 있습니다. 하지만 문제는 그 뉴런의 활성화가 현재 모델이 실제로 모른다고 말로 말하는 것과 연결되어 있지 않다는 것입니다. 신경망의 내부가

[1:27:30] 알아도 - 그것을 나타내는 뉴런이 있기 때문에 - 모델은 그것을 표면화하지 않고, 대신 학습 세트에서 보는 것처럼 확신 있게 들리도록 최선의 추측을 합니다. 그래서 기본적으로 모델을 심문하고 모르는 경우에 모른다고 말할 수 있도록 해야 합니다. Meta가 대략 무엇을 하는지 보여드리겠습니다. 여기 예시가 있습니다. 도미니크 하셰크가 오늘의 특집 기사입니다. 무작위로 거기 갔고,

[1:28:00] 기본적으로 하는 일은 학습 세트의 랜덤 문서를 가져와서 문단을 가져온 다음, LLM을 사용해 그 문단에 대한 질문을 구성합니다. 예를 들어 ChatGPT로 이것을 했습니다. "여기 이 문서의 문단이 있어. 이 문단을 기반으로 세 가지 구체적인 사실 질문을 생성하고 질문과 답을 줘"라고 했습니다. LLM은 이미 이 정보를 재구성할 만큼 충분히 좋습니다.

[1:28:30] 정보가 이 LLM의 컨텍스트 윈도우에 있으면 꽤 잘 작동합니다. 메모리에 의존할 필요가 없습니다. 바로 컨텍스트 윈도우에 있고, 꽤 높은 정확도로 그 정보를 재구성할 수 있습니다. 예를 들어 "어느 팀에서 뛰었나요? 여기 답이 있습니다. 컵을 몇 개 이겼나요?" 등의 질문을 생성할 수 있습니다. 이제 질문과 답이 있고, 모델을 심문해야 합니다. 대략적으로 하는 일은 질문을 가져와서 모델에 갑니다. Meta의 경우 Llama가 될 것이지만

[1:29:00] 여기서는 Mistral 7B를 예시로 심문해봅시다. 다른 모델입니다. 이 모델이 이 답을 알까요? 봅시다. 버팔로 세이버스에서 뛰었죠. 모델이 알고, 프로그래밍적으로 결정하는 방법은 기본적으로 모델의 이 답을 정답과 비교하는 것입니다. 모델이 이것을 자동으로 할 만큼 충분히 좋습니다. 인간이 관여하지 않습니다.

[1:29:30] 모델의 답을 가져와서 다른 LLM 심판을 사용해 이 답에 따라 맞는지 확인할 수 있습니다. 맞으면 모델이 아마 안다는 뜻입니다. 하는 일은 몇 번 이것을 합니다. 알아요, 버팔로 세이버스입니다. 한 번 더 해봅시다. 버팔로 세이버스. 이 사실 질문에 대해 세 번 물었고 모델이 아는 것 같습니다.

[1:30:00] 좋습니다. 이제 두 번째 질문을 해봅시다. 스탠리 컵을 몇 개 이겼나요? 다시 모델을 심문하고 정답은 2입니다. 여기서 모델은 네 번 이겼다고 주장하는데 맞지 않죠. 2와 맞지 않습니다. 모델이 모르고, 지어내고 있습니다.

[1:30:30] 여기서 모델이 또 - 좀 느리네요 - 선수 생활 동안 이기지 못했다고 합니다. 분명히 모델이 모르고, 프로그래밍적으로 알 수 있는 방법은 모델을 세 번 심문하고 답을 정답과 비교하는 것입니다. 세 번, 다섯 번 등. 모델이 모르면 이 질문에 대해 모델이 모른다는 것을 알고, 이 질문을 가져와서 새 대화를

[1:31:00] 학습 세트에 만듭니다. 새 대화를 학습 세트에 추가합니다. "스탠리 컵을 몇 개 이겼나요?"라는 질문에 답은 "죄송합니다, 모르겠습니다" 또는 "기억이 안 납니다"입니다. 모델을 심문해서 그것이 사실임을 봤기 때문에 이것이 이 질문의 정답입니다. 많은 다른 유형의 질문, 많은 다른 유형의 문서에 대해 이것을 하면, 학습 세트에서 모델에게 지식에 기반해 거부할 기회를 주는 것이고,

[1:31:30] 이런 예시가 학습 세트에 몇 개만 있으면 모델이 알게 됩니다. 존재한다고 추정하는 네트워크 어딘가의 불확실성의 내부 뉴런에 이 지식 기반 거부의 연관을 배울 기회가 있습니다. 경험적으로 이것이 아마 사실인 것 같고, 그 연관을 배울 수 있습니다. "이 불확실성의 뉴런이 높으면 실제로 모르고 '죄송하지만 이것은 기억이 안 납니다' 등이라고 말해도 된다." 이런

[1:32:00] 예시가 학습 세트에 있으면 이것은 환각에 대한 큰 완화이고, 대략적으로 ChatGPT도 이런 것을 할 수 있는 이유입니다. 이것이 사람들이 구현한 완화의 종류이고 시간이 지나면서 사실성 문제를 개선했습니다. 자, 환각 문제를 완화하기 위한 완화 방법 1번을 설명했습니다. 이제 실제로 훨씬 더 나아질 수 있습니다. 그냥 모른다고 말하는 대신

[1:32:30] 추가 완화 방법 2번을 도입해서 LLM에게 사실적으로 질문에 답할 기회를 줄 수 있습니다. 내가 사실적인 질문을 하고 당신이 모르면 답하기 위해 무엇을 할까요? 나가서 검색하고 인터넷을 사용해서 답을 찾고 그 답이 무엇인지 말해줄 수 있습니다. 이 모델들로도 정확히 같은 일을 할 수 있습니다. 신경망 내부의 지식,

[1:33:00] 수십억 파라미터 안의 지식을 오래 전 사전학습 단계에서 모델이 본 것들의 희미한 기억이라고 생각하세요. 파라미터의 지식을 한 달 전에 읽은 것이라고 생각하세요. 계속 읽으면 기억할 것이고 모델은 기억하지만, 드문 것이라면 그 정보에 대한 좋은 기억이 없을 것입니다. 하지만 우리는 그냥 가서 찾아봅니다. 찾아볼 때 기본적으로 하는 일은 정보로 작업 메모리를

[1:33:30] 새로 고침하고, 그것을 검색하고 이야기할 수 있습니다. 모델이 메모리나 기억을 새로 고침할 수 있는 동등한 것이 필요하고, 모델에 도구를 도입해서 그렇게 할 수 있습니다. 이것에 접근하는 방식은, 모른다고 말하는 대신 도구를 사용하려고 시도할 수 있습니다. 언어 모델이 특수 토큰을 방출할 수 있는 메커니즘을 만들 수 있고, 이것은 우리가

[1:34:00] 도입할 새 토큰입니다. 예를 들어 여기서 두 개의 토큰을 도입했고, 모델이 이 토큰을 사용할 수 있는 방법에 대한 형식이나 프로토콜을 도입했습니다. 예를 들어 모델이 모를 때 질문에 답하는 대신, 모른다고 말하는 대신, 모델은 이제 특수 토큰 search_start를 방출할 수 있습니다. 이것이 OpenAI의 경우 Bing.com이나 Google 검색 등으로 갈 쿼리입니다. 쿼리를 방출하고

[1:34:30] search_end를 방출합니다. 여기서 일어나는 일은 모델에서 샘플링하고 추론을 실행하는 프로그램이 특수 토큰 search_end를 보면, 시퀀스에서 다음 토큰을 샘플링하는 대신 모델에서 생성을 멈추고, 나가서 Bing.com과 세션을 열고 검색 쿼리를 Bing에 붙여넣고

[1:35:00] 검색된 모든 텍스트를 가져옵니다. 그 텍스트를 가져와서 아마 다른 특수 토큰 등으로 다시 표현하고, 그 텍스트를 가져와서 괄호로 보여드리려 한 것처럼 여기에 복사 붙여넣기합니다. 모든 텍스트가 여기로 오면 컨텍스트 윈도우에 들어갑니다. 모델에게요. 웹 검색의 텍스트가 이제 신경망에 공급될 컨텍스트 윈도우 안에 있고, 컨텍스트 윈도우를 모델의 작업 메모리라고 생각해야 합니다. 컨텍스트 윈도우에 있는 데이터는 모델이 직접 접근할 수 있습니다.

[1:35:30] 신경망에 직접 공급됩니다. 더 이상 희미한 기억이 아니라 컨텍스트 윈도우에 있고 그 모델에 직접 사용 가능한 데이터입니다. 이제 새 토큰을 샘플링할 때 거기에 복사 붙여넣기된 데이터를 매우 쉽게 참조할 수 있습니다. 대략 이것이 이 도구들이 어떻게 작동하는지입니다. 웹 검색은 도구 중 하나이고 다른 도구들도 살펴볼 것입니다. 기본적으로

[1:36:00] 새 토큰을 도입하고, 모델이 이 토큰을 활용하고 웹 검색 함수 같은 특수 함수를 호출할 수 있는 스키마를 도입합니다. 웹 검색, search_start, search_end 등의 도구를 모델이 올바르게 사용하도록 어떻게 가르칠까요? 다시 학습 세트를 통해 합니다. 이제 데이터와 모델에게 예시로 웹 검색을 어떻게 사용하는지 보여주는 대화가 많이 필요합니다. 검색을 사용하는 설정이 무엇이고

[1:36:30] 어떻게 생겼는지, 예시로 검색을 시작하는 방법과 검색 등. 이런 예시가 몇 천 개 있으면 모델이 실제로 이 도구가 어떻게 작동하는지 이해하고, 쿼리를 어떻게 구조화하는지 알게 됩니다. 물론 사전학습 데이터셋과 세계에 대한 이해 때문에 실제로 웹 검색이 무엇인지 어느 정도 이해하고, 어떤 종류의 것이 좋은 검색 쿼리인지 꽤 좋은 네이티브 이해가 있습니다.

[1:37:00] 그래서 모두 작동합니다. 새 도구를 어떻게 사용하는지 보여주는 몇 가지 예시만 필요하고, 그것에 의존해 정보를 검색하고 컨텍스트 윈도우에 넣을 수 있습니다. 이것은 우리가 무언가를 찾아보는 것과 같습니다. 컨텍스트에 있으면 작업 메모리에 있고 조작하고 접근하기가 매우 쉽습니다. 이것이 몇 분 전에 ChatGPT에서 오슨 코바츠가 누구냐고 검색했을 때 본 것입니다. ChatGPT 언어 모델은 이것이 드문 개인이나 뭔가라고 결정하고

[1:37:30] 메모리에서 답을 주는 대신 웹 검색을 할 특수 토큰을 샘플링하기로 결정했습니다. "웹 도구 사용 중" 같은 것이 잠깐 나왔고, 2초 기다린 다음 이것이 생성되었고, 여기서 참조를 만들고 있습니다. 출처를 인용하고 있죠. 여기서 일어난 일은 나가서 웹 검색을 하고 이 출처들과 URL들을 찾았고 이 웹 페이지의 텍스트가 모두

[1:38:00] 여기 사이에 채워졌습니다. 여기 보이지 않지만 기본적으로 텍스트로 여기 사이에 채워져 있습니다. 이제 그 텍스트를 보고 참조하면서 "이 사람들일 수 있습니다 인용, 저 사람들일 수 있습니다 인용" 등이라고 말합니다. 이것이 여기서 일어난 일이고, 오슨 코바츠가 누구냐고 물었을 때 "도구 사용 안 함"이라고 말할 수도 있고, 그러면 ChatGPT가 도구를 사용하지 않고 메모리와

[1:38:30] 기억만 사용하도록 설득하기에 충분합니다. ChatGPT에게 이 질문도 해봤습니다. "도미니크 하셰크가 스탠리 컵을 몇 개 이겼나요?" ChatGPT는 실제로 답을 안다고 결정하고 두 번 이겼다고 말할 자신감이 있습니다. 메모리에 의존했는데, 아마도 가중치, 파라미터, 활성화에 충분한 자신감이 있어서 메모리에서만 검색 가능하기 때문입니다.

[1:39:00] 반대로 웹 검색을 사용해 확인할 수도 있습니다. 같은 쿼리에 대해 실제로 나가서 검색하고 많은 출처를 찾고 이 모든 것이 거기에 복사 붙여넣기되고, 다시 말해주고 인용하고, 실제로 이 정보의 출처인 Wikipedia 기사도 말해줍니다. 이것이 도구, 웹 검색입니다. 모델이 언제 검색할지 결정하고, 대략 이것이 도구가

[1:39:30] 작동하는 방식이고, 환각과 사실성에 대한 추가적인 완화입니다. 매우 중요한 심리적 포인트를 다시 강조하고 싶습니다. 신경망의 파라미터에 있는 지식은 희미한 기억입니다. 컨텍스트 윈도우를 구성하는 토큰에 있는 지식은 작업 메모리입니다. 대략적으로 우리 뇌에서 작동하는 방식과 같습니다. 우리가 기억하는 것은 파라미터이고,

[1:40:00] 몇 초나 몇 분 전에 경험한 것 등은 컨텍스트 윈도우에 있다고 상상할 수 있습니다. 이 컨텍스트 윈도우는 주변에서 의식적 경험을 할 때 쌓입니다. 이것은 실제로 LLM 사용에도 많은 함의가 있습니다. 예를 들어 ChatGPT에 가서 이런 것을 할 수 있습니다. "제인 오스틴의 오만과 편견 1장을 요약해줄 수 있어?"라고 말할 수 있고, 완전히 괜찮은 프롬프트이고 ChatGPT는 실제로 여기서 비교적 합리적인 일을 합니다. 그 이유는

[1:40:30] ChatGPT가 오만과 편견 같은 유명한 작품에 대해 꽤 좋은 기억이 있기 때문입니다. 아마 많은 것을 봤고, 이 책에 대한 포럼이 있고, 이 책의 버전을 읽었을 것이고, 기억하기 때문입니다. 이것을 읽었거나 관련 기사를 읽었다면 이 모든 것을 말할 수 있을 만큼 기억할 것입니다. 하지만 보통 실제로 LLM과 상호작용하고 특정 것을 기억하길 원할 때는 그냥 주는 것이 항상 더 잘 작동합니다. 훨씬 더 나은 프롬프트는 이런 것입니다.

[1:41:00] "오만과 편견 1장을 요약해줄 수 있어? 참고용으로 아래에 첨부했어." 그런 다음 여기서 구분자를 넣고 붙여넣습니다. 어떤 웹사이트에서 찾아서 여기에 1장을 복사 붙여넣기했습니다. 그렇게 하는 이유는 컨텍스트 윈도우에 있으면 모델이 직접 접근할 수 있고, 기억할 필요 없이 그냥 접근할 수 있기 때문입니다. 이 요약은

[1:41:30] 이 요약보다 상당히 더 높은 품질이 예상됩니다. 모델에 직접 사용 가능하기 때문입니다. 우리도 같은 방식으로 작동할 것입니다. 요약해야 하기 전에 이 장을 다시 읽었다면 훨씬 더 나은 요약을 만들 것이고, 기본적으로 그것이 여기서 일어나는 일이거나 그것과 동등한 것입니다. 다음으로 간략히 이야기하고 싶은 심리적 특이점은 자기 지식입니다. 인터넷에서 매우 자주 보는 것은 사람들이 LLM에게 "어떤 모델이야? 누가 만들었어?"라고 묻는 것입니다.

---

## 12. 자기 인식

**요약**: LLM의 자기 인식에 대해 설명합니다. 모델은 자신이 무엇인지, 누가 만들었는지 등에 대한 지식이 학습 데이터에서 왔기 때문에 불완전할 수 있습니다. "당신은 Claude입니다"와 같은 시스템 프롬프트로 정체성을 부여합니다.

[1:42:00] 기본적으로 이 질문은 약간 말이 안 됩니다. 제가 기본 원리를 설명하려 했듯이, 이것은 사람이 아닙니다. 어떤 방식으로든 지속적인 존재가 없습니다. 부팅되고, 토큰을 처리하고, 꺼집니다. 모든 사람에 대해 그렇게 합니다. 대화의 컨텍스트 윈도우를 쌓고 모든 것이 삭제됩니다. 이 개체는 모든 대화마다 처음부터 재시작됩니다. 지속적인 자아가 없고

[1:42:30] 자아 감각이 없습니다. 토큰 텀블러이고 학습 세트의 통계적 규칙성을 따릅니다. 그래서 "너 누구야? 누가 만들었어?" 등을 묻는 것은 별로 의미가 없고, 기본적으로 제가 설명한 것처럼 하고 그냥 기본으로 아무 데서나 시작하면 꽤 랜덤한 답을 얻을 것입니다. 예를 들어 꽤 오래된 모델인 Falcon을 골라봅시다. 무엇을 말하는지 봅시다. 질문을 회피하네요. 재능 있는 엔지니어와 개발자, 여기서는 OpenAI가

[1:43:00] GPT-3 모델을 기반으로 만들었다고 합니다. 완전히 지어내는 것입니다. 여기서 OpenAI가 만들었다고 하는 것 때문에 많은 사람들이 이 모델이 어떻게든 OpenAI 데이터에서 학습되었다는 증거로 받아들일 것입니다. 실제로 그것이 반드시 사실이라고 생각하지 않습니다. 그 이유는 이런 종류의 질문에 답하도록 모델을 명시적으로 프로그래밍하지 않으면, 얻는 것은 답에 대한 통계적 최선의 추측이기 때문입니다. 이 모델은

[1:43:30] 대화의 SFT 데이터 혼합물이 있었고, 파인튜닝 중에 모델은 이 데이터를 학습하면서 이 도움이 되는 어시스턴트의 성격을 취하고 있다는 것을 어느 정도 이해합니다. 자신에게 적용할 정확한 레이블을 알지 못하고, 실제로 말해지지 않았습니다. 그냥 이 도움이 되는 어시스턴트의 페르소나를 취하고 있고, 사전학습 단계가 전체 인터넷의 문서를 가져왔고, ChatGPT와 OpenAI는 이

[1:44:00] 문서들에서 매우 두드러집니다. 그래서 제 생각에 실제로 여기서 일어나고 있는 것은 이것이 자신이 무엇인지에 대한 환각된 레이블이라는 것입니다. 자기 정체성이 OpenAI의 ChatGPT라고 하는 것은 인터넷에 실제로 ChatGPT에서 오는 이런 답들에 대한 엄청난 데이터가 있기 때문입니다. 그래서 그것이 자신에 대한 레이블입니다. 개발자로서 LLM 모델이 있다면 이것을 재정의할 수 있고, 몇 가지 방법이 있습니다.

[1:44:30] 예를 들어 Allen AI의 OLMo 모델이 있습니다. 최상위 LLM은 아니지만, 완전히 오픈소스라서 좋아합니다. OLMo의 논문과 모든 것이 완전히 공개되어 있어서 좋습니다. 여기서 SFT 혼합물을 보고 있습니다. 파인튜닝의 데이터 혼합물입니다. 대화 데이터죠. OLMo 모델을 위해 해결하는 방식은, 혼합물에 많은 것이 있고

[1:45:00] 여기에 총 100만 개의 대화가 있지만, 여기 "olmo2 hardcoded"가 있습니다. 거기 가면 240개의 대화가 있고, 이 240개의 대화를 보세요. 하드코딩되어 있습니다. "자신에 대해 말해줘"라고 사용자가 말하면 어시스턴트가 "저는 Allen Institute for Artificial Intelligence의 AI to에서 개발한 오픈 언어 모델입니다" 등. "도와드리겠습니다" 어쩌고저쩌고. "이름이 뭐야?" OLMo 프로젝트. 이것들은 모두

[1:45:30] OLMo 2에 대한 이런 종류의 준비된 하드코딩된 질문들과 이런 경우에 줄 정답들입니다. 이런 질문이나 대화 240개를 학습 세트에 넣고 파인튜닝하면, 모델이 실제로 나중에 이것을 앵무새처럼 반복할 것으로 예상됩니다. 이것을 주지 않으면 아마 OpenAI의 ChatGPT라고 할 것입니다. 때때로 이것을 하는 또 다른 방법이 있는데,

[1:46:00] 기본적으로 이 대화들에서 인간과 어시스턴트 사이의 턴이 있고, 때때로 대화 맨 처음에 시스템 메시지라는 특별한 메시지가 있습니다. 인간과 어시스턴트 사이만이 아니라 시스템이 있고, 시스템 메시지에 "당신은 OpenAI가 개발한 모델이고 이름은 ChatGPT 4o이고, 이 날짜에 학습되었고, 지식 컷오프는 이것입니다"라고 하드코딩하고 상기시킬 수 있습니다. 기본적으로 모델을 약간 문서화하고, 이것이 대화에 삽입됩니다. ChatGPT에 가면 빈 페이지가 보이지만 실제로 시스템 메시지는

[1:46:30] 숨겨져 있고, 그 토큰들이 컨텍스트 윈도우에 있습니다. 이것이 모델이 자신에 대해 이야기하도록 프로그래밍하는 두 가지 방법입니다. 이런 데이터를 통해서 하거나, 시스템 메시지 같은 것, 기본적으로 컨텍스트 윈도우에 있고 모델에게 정체성을 상기시키는 보이지 않는 토큰을 통해서 합니다. 하지만 이것은 모두 준비되고 덧붙여진 것이고, 인간에게 그러하듯이 실제로 깊이 있는 것이 아닙니다. 이제 다음 섹션으로 계속하겠습니다.

---

## 13. 모델은 생각하기 위해 토큰이 필요하다

**요약**: 모델이 "생각"하기 위해 토큰이 필요하다는 개념을 설명합니다. 복잡한 문제는 중간 단계(chain of thought)를 거쳐야 풀 수 있으며, 바로 답을 요구하면 실패합니다. 이것이 o1 같은 추론 모델의 기반이 됩니다.

[1:47:00] 이 모델들의 문제 해결 시나리오에서의 계산 능력, 네이티브 계산 능력을 다룹니다. 특히 대화의 예시를 구성할 때 이 모델들에 매우 주의해야 하고, 이 모델들이 어떻게 생각하는지 고려할 때 유익한 많은 날카로운 경계가 있습니다. 인간의 다음 프롬프트를 고려해보세요. 기본적으로 대화 학습 세트에 들어갈 대화를 만들고 있다고 가정합니다.

[1:47:30] 이것에서 모델을 학습시킬 것입니다. 기본적으로 간단한 수학 문제를 어떻게 푸는지 가르치는 것입니다. 프롬프트는 "에밀리가 사과 3개와 오렌지 2개를 삽니다. 각 오렌지는 2달러입니다. 총 비용은 13달러입니다. 사과 비용은 얼마입니까?" 매우 간단한 수학 문제입니다. 여기 왼쪽과 오른쪽에 두 개의 답이 있습니다. 둘 다 정답입니다. 둘 다 정답이 3이라고 합니다. 맞습니다. 하지만 이 두 개 중 하나가 어시스턴트에게 상당히 더 나은 답입니다. 데이터 라벨러이고 이 중 하나를 만들고 있다면

[1:48:00] 하나는 어시스턴트에게 정말 끔찍한 답이고 다른 하나는 괜찮습니다. 비디오를 멈추고 왜 이 두 개 중 하나가 상당히 더 나은 답인지 생각해보세요. 잘못된 것을 사용하면 모델이 실제로 수학을 정말 못하게 될 수 있고, 나쁜 결과를 가져옵니다. 이것은 라벨링 문서에서 이상적인 어시스턴트 응답을 만들도록 사람들을 훈련할 때 주의해야 할 것입니다.

[1:48:30] 이 질문의 핵심은 모델이 학습하고 추론할 때 왼쪽에서 오른쪽으로 1차원 토큰 시퀀스에서 작업한다는 것을 깨닫고 기억하는 것입니다. 이것이 제가 자주 머릿속에 가지고 있는 그림입니다. 토큰 시퀀스가 왼쪽에서 오른쪽으로 진화하는 것을 상상합니다. 시퀀스에서 다음 토큰을 항상 생성하기 위해 이 모든 토큰을 신경망에 공급하고, 이 신경망이 시퀀스에서 다음 토큰에 대한 확률을 줍니다.

[1:49:00] 여기 이 그림은 이전에 본 정확히 같은 그림입니다. 이전에 보여드린 웹 데모에서 온 것입니다. 이것이 기본적으로 상단의 입력 토큰을 가져와서 모든 뉴런의 이런 연산을 수행하고 다음에 올 것에 대한 확률의 답을 주는 계산입니다. 중요한 것은 대략적으로 여기서 일어나는 계산의 층 수가 유한하다는 것입니다. 예를 들어 여기 이 모델은

[1:49:30] 어텐션과 MLP의 1, 2, 3층만 있습니다. 일반적인 현대 최첨단 네트워크는 약 100층 정도가 있지만, 이전 토큰 시퀀스에서 다음 토큰의 확률로 가는 데 약 100층의 계산만 있습니다. 여기서 일어나는 계산의 유한한 양이 있고, 이것을 매우 적은 양의 계산이라고 생각해야 합니다. 이 양의 계산은 이 시퀀스의 모든 개별 토큰에 대해 거의 대략 고정되어 있습니다.

[1:50:00] 완전히 사실은 아닙니다. 더 많은 토큰을 넣을수록 이 신경망의 순방향 패스가 더 비싸지지만, 많이는 아닙니다. 이것을 좋은 모델로 생각하고, 이 박스에서 이 모든 토큰에 대해 고정된 양의 계산이 일어날 것이라고 생각해야 합니다. 이 양의 계산이 너무 클 수 없습니다. 위에서 아래로 가는 층이 그렇게 많지 않기 때문입니다. 여기서 계산적으로 그렇게 많이 일어나지 않습니다.

[1:50:30] 모델이 단일 순방향 패스에서 단일 토큰을 얻기 위해 임의의 계산을 할 것이라고 상상할 수 없습니다. 이것이 의미하는 바는 우리가 실제로 추론과 계산을 많은 토큰에 분산해야 한다는 것입니다. 모든 개별 토큰이 유한한 양의 계산만 사용하기 때문입니다. 계산을 많은 토큰에 분산하고 싶고, 어떤 단일 개별 토큰에서 모델에게 너무 많은 계산을 기대할 수 없습니다. 토큰당 일어나는 계산이 그만큼밖에 없기 때문입니다.

[1:51:00] 대략 토큰당 고정된 양의 계산입니다. 그래서 여기 이 답이 상당히 더 나쁩니다. 그 이유는 여기서 왼쪽에서 오른쪽으로 가는 것을 상상해보세요. 바로 여기에 복사 붙여넣기했습니다. "정답은 3입니다" 등. 모델이 왼쪽에서 오른쪽으로 이 토큰들을 하나씩 방출하는 것을 상상해보세요. "정답은 $ 기호"를 말해야 하고, 바로 여기서

[1:51:30] 이 문제의 모든 계산을 이 단일 토큰에 밀어넣어야 합니다. 정답 3을 방출해야 합니다. 그런 다음 답 3을 방출하면 이 모든 토큰을 말할 것으로 예상되지만, 이 시점에서 답은 이미 생성되었고 이미 컨텍스트 윈도우에 있습니다. 그래서 여기 있는 모든 것은 왜 이것이 답인지에 대한 사후적 정당화일 뿐입니다.

[1:52:00] 답은 이미 만들어졌고 토큰 윈도우에 있으므로 실제로 여기서 계산되는 것이 아닙니다. 그래서 질문에 직접적이고 즉시 답하면 모델이 단일 토큰에서 답을 추측하도록 훈련하는 것이고, 토큰당 일어나는 유한한 양의 계산 때문에 작동하지 않을 것입니다. 그래서 오른쪽 답이 상당히 더 낫습니다. 계산을 답에 분산하고 있기 때문입니다. 실제로 모델이 왼쪽에서 오른쪽으로 천천히 답에 도달하도록 하고 있습니다. 중간 결과를 얻습니다.

[1:52:30] "오렌지의 총 비용은 4입니다. 그래서 13 - 4는 9입니다." 중간 계산을 만들고 있고 각 계산은 그 자체로 그렇게 비싸지 않습니다. 기본적으로 모델이 이런 개별 토큰 각각에서 할 수 있는 어려움을 약간 추측하는 것이고, 이 토큰들 중 어느 것에서도 계산적으로 너무 많은 작업이 있을 수 없습니다. 그러면 모델이 나중에 테스트 시에 그것을 할 수 없을 것이기 때문입니다. 그래서 여기서 모델에게 추론을 펼치고 계산을

[1:53:00] 토큰에 분산하도록 가르치고 있습니다. 이런 방식으로 각 토큰에 매우 간단한 문제만 있고, 합쳐지고, 끝에 가까워지면 작업 메모리에 모든 이전 결과가 있어서 정답이 무엇인지, 여기 3이라는 것을 결정하기가 훨씬 쉽습니다. 이것이 계산에 상당히 더 나은 레이블입니다. 이것은 정말 나쁘고 모델에게 모든 계산을 단일 토큰에서 하도록 가르치고 있어서 정말 나쁩니다.

[1:53:30] 기억해야 할 흥미로운 것은 프롬프트에서 보통 명시적으로 이것에 대해 생각할 필요가 없다는 것입니다. OpenAI의 사람들이 실제로 이것에 대해 걱정하는 라벨러들이 있고 답이 펼쳐지도록 확인하기 때문입니다. 그래서 실제로 OpenAI는 옳은 일을 할 것입니다. ChatGPT에 이 질문을 하면 실제로 매우 천천히 갈 것입니다. "변수를 정의하고, 방정식을 세우고" 이런 모든 중간 결과를 만들고 있습니다. 이것은 당신을 위한 것이 아닙니다. 모델을 위한 것입니다. 모델이 이런 중간

[1:54:00] 결과를 스스로 만들지 않으면 3에 도달할 수 없을 것입니다. 모델에게 약간 못되게 굴 수 있다는 것도 보여드리고 싶습니다. 예를 들어 정확히 같은 프롬프트를 주고 "단일 토큰으로 질문에 답해. 즉시 답만 줘, 다른 거 없이"라고 말했습니다. 이 간단한 프롬프트에 대해 실제로 한 번에 할 수 있었습니다. 단일 토큰을 생성했습니다. 이것이 두 개의 토큰인 것 같습니다.

[1:54:30] 달러 기호가 자체 토큰이니까요. 기본적으로 이 모델은 단일 토큰을 주지 않고 두 개의 토큰을 줬지만 여전히 정답을 생성했습니다. 네트워크의 단일 순방향 패스에서 그렇게 했습니다. 여기 숫자들이 매우 간단하기 때문입니다. 모델에게 약간 못되게 굴기 위해 조금 더 어렵게 만들었습니다. "에밀리가 사과 23개와 오렌지 177개를 삽니다." 숫자를 조금 크게 만들었습니다. 모델이 단일 토큰에서 더 많은 계산을 하도록 요구하는 것입니다.

[1:55:00] 같은 것을 말했고 여기서 5를 줬는데 5는 실제로 맞지 않습니다. 모델이 네트워크의 단일 순방향 패스에서 이 모든 계산을 하지 못했습니다. 입력 토큰에서 네트워크의 단일 순방향 패스, 단일 네트워크 통과에서 결과를 생성하지 못했습니다. 그런 다음 "토큰 제한에 대해 걱정하지 말고 평소처럼 문제를 풀어"라고 말했습니다. 그러면 모든 중간 결과를 거치고, 단순화하고, 여기 중간 결과와 중간 계산들 각각은 모델에게 훨씬 쉽고

[1:55:30] 토큰당 너무 많은 작업이 아닙니다. 여기 모든 토큰들이 정확하고 해결책 7에 도달합니다. 이 모든 작업을 네트워크의 단일 순방향 패스에 밀어넣을 수 없었습니다. 귀여운 예시이고 이 모델들이 어떻게 작동하는지 생각할 것이고 다시 유익합니다. 이 주제에 대해 마지막으로 말하고 싶은 것은 실제로 일상생활에서 이것을 풀려고 한다면 모델이 여기서 모든 중간 계산을 올바르게 했다고

[1:56:00] 신뢰하지 않을 것입니다. 실제로 아마 이런 것을 할 것입니다. "코드를 사용해"라고 말할 것입니다. 코드가 ChatGPT가 사용할 수 있는 가능한 도구 중 하나이고, 이런 암산을 해야 하는 대신, 여기 암산을 완전히 신뢰하지 않고, 특히 숫자가 정말 커지면 모델이 이것을 올바르게 할 것이라는 보장이 없습니다. 신경망을 사용해서 암산을 하는 것입니다.

[1:56:30] 머릿속에서 암산을 하는 것과 같습니다. 중간 결과 중 일부가 잘못될 수 있습니다. 모델이 이런 종류의 암산을 할 수 있다는 것이 실제로 놀랍습니다. 제가 머릿속에서 이것을 할 수 있을 것 같지 않지만, 기본적으로 모델은 머릿속에서 하고 있고 신뢰하지 않습니다. 그래서 도구를 사용하고 싶습니다. "코드 사용"이라고 말하고 그것이 작동할 것으로 예상합니다. 봅시다.

[1:57:00] 7이 맞습니다. 여기서 일어난 일은 실제로, 보이지 않지만, 문제를 모델에게 더 쉬운 문제들로 분해했습니다. 모델이 암산을 할 수 없다는 것을 알지만, 복사 붙여넣기는 꽤 잘한다는 것을 압니다. 여기서 "코드 사용"이라고 말하면 Python에서 이것에 대한 문자열을 만들고, 기본적으로 여기 입력을 여기로 복사 붙여넣기하는 작업은

[1:57:30] 모델에게 매우 간단합니다. 이 점 문자열을 이 네 개 토큰 정도로 보기 때문입니다. 모델이 그 토큰 ID를 복사 붙여넣기하고 여기 점으로 풀어내는 것은 매우 간단합니다. 이 문자열을 만들고 Python 루틴 .count를 호출하고 정답을 얻습니다. Python 인터프리터가 세는 것이지 모델의 암산이 세는 것이 아닙니다.

[1:58:00] 다시 간단한 예시입니다. 모델은 생각하기 위해 토큰이 필요하고, 암산에 의존하지 마세요. 그래서 모델은 세기 작업도 잘 못합니다. 세기 작업이 필요하면 항상 도구에 의존하도록 요청하세요. 모델은 여기저기 많은 다른 작은 인지적 결함도 있고, 이것은 알아야 할 기술의 날카로운 경계입니다. 예를 들어 모델은 모든 종류의 철자 관련 작업을 잘 못합니다. 토큰화로 다시 돌아올 것이라고 말했습니다.

---

## 14. 토큰화 재방문: 모델은 철자에 어려움을 겪는다

**요약**: 토큰화가 모델의 철자 처리 능력에 미치는 영향을 설명합니다. 모델은 글자 수준이 아닌 토큰 수준에서 작동하므로, "strawberry"에서 'r'이 몇 개인지 묻는 질문에 어려움을 겪습니다. 이는 토큰화의 구조적 한계입니다.

[2:01:30] 그 이유는 모델이 문자를 보지 않고 토큰을 보기 때문입니다. 그들의 전체 세계는 토큰에 관한 것이고, 토큰은 이 작은 텍스트 조각들입니다. 그래서 우리 눈이 보는 것처럼 문자를 보지 않습니다. 매우 간단한 문자 수준 작업이 종종 실패합니다. 예를 들어 "ubiquitous"라는 문자열을 주고 "첫 번째부터 시작해서 세 번째 문자만 출력해"라고 요청합니다. U로 시작하고 세 번째마다 가야 합니다.

[2:02:00] 1, 2, 3, Q가 다음이어야 합니다. 등등. 이것은 맞지 않아 보이고, 제 가설은 여기서 암산이 약간 실패하고 있다는 것입니다. 하지만 더 중요한 문제는 Tiktokenizer에 가서 ubiquitous를 보면 세 개의 토큰이라는 것입니다. 우리는 ubiquitous를 보고 개별 글자에 쉽게 접근할 수 있습니다. 그것을 보기 때문입니다. 시각적 필드의 작업 메모리에 있으면 매우 쉽게

[2:02:30] 세 번째 글자마다 인덱싱할 수 있고 그 작업을 할 수 있습니다. 하지만 모델은 개별 글자에 접근할 수 없습니다. 이것을 이 세 개의 토큰으로 봅니다. 이 모델들은 인터넷에서 처음부터 학습되고 기본적으로 모델이 모든 다른 글자가 모든 다른 토큰에 몇 개 들어있는지 발견해야 합니다. 토큰을 사용하는 이유는 대부분 효율성 때문입니다.

[2:03:00] 하지만 많은 사람들이 토큰을 완전히 삭제하는 것에 관심이 있습니다. 문자 수준이나 바이트 수준 모델이 있어야 합니다. 다만 그것은 매우 긴 시퀀스를 만들고 사람들이 지금 그것을 어떻게 다룰지 모릅니다. 토큰 세계가 있는 한 어떤 종류의 철자 작업도 실제로 잘 작동하지 않을 것입니다. 토큰화 때문에 철자가 강점이 아니라는 것을 알기 때문에 다시 도구에 의존하도록 요청할 수 있습니다. "코드 사용"이라고 말하면 ubiquitous를 Python 인터프리터에 복사 붙여넣기하는 작업이 훨씬 쉽고

[2:03:30] Python 인터프리터에 의존해 이 문자열의 문자를 조작하기 때문에 작동할 것으로 예상합니다. "코드 사용" ubiquitous 네, 세 번째 문자마다 인덱싱하고 실제 진실은 u2s uqs입니다. 맞아 보입니다. 다시 철자 관련 작업이 잘 작동하지 않는 예시입니다. 최근 매우 유명한 예시는 "strawberry에 r이 몇 개야?"입니다. 여러 번 바이럴이 되었고 기본적으로 모델이 이제 맞게 답합니다. strawberry에 r이 세 개 있다고 하지만, 오랫동안 모든 최첨단 모델이

[2:04:00] strawberry에 r이 두 개밖에 없다고 주장했습니다. 이것은 많은 소동을 일으켰습니다. 모델이 너무 뛰어나서 수학 올림피아드 문제를 풀 수 있는데 strawberry에서 r을 세지 못하는 이유가 뭐냐고요. 그것에 대한 답은 다시, 천천히 쌓아왔지만, 첫째, 모델은 문자를 보지 않고 토큰을 봅니다. 둘째, 세기를 잘 못합니다. 여기서 문자를 보는 어려움과 세기의 어려움을 결합하고 있습니다.

[2:04:30] 그래서 모델이 이것에 어려움을 겪었습니다. 솔직히 지금쯤 OpenAI가 여기서 답을 하드코딩했을 수도 있고 무엇을 했는지 모르겠지만, 이 특정 쿼리는 이제 작동합니다. 모델은 철자를 잘 못하고 다른 작은 날카로운 경계들도 있습니다. 모두 다루고 싶지는 않습니다. 알아야 할 몇 가지 예시만 보여드리고 싶습니다. 이 모델들을 실제로 사용할 때요. 실제로 모델이 부족한 모든 방식에 대한 포괄적인 분석을 하고 싶지 않습니다.

---

## 15. Jagged Intelligence (들쭉날쭉한 지능)

**요약**: LLM의 "들쭉날쭉한 지능(Jagged Intelligence)"을 설명합니다. 모델은 어떤 영역에서는 매우 뛰어나지만 다른 영역에서는 기본적인 실수를 합니다. 이는 인간의 지능과 다른 형태이며, 모델을 사용할 때 이러한 특성을 이해해야 합니다.

[2:05:00] 요점을 말하고 싶습니다. 여기저기 들쭉날쭉한 경계가 있습니다. 몇 가지를 논의했고 몇 가지는 이해가 됩니다. 하지만 일부는 그렇게 이해가 되지 않고 이 모델들이 어떻게 작동하는지 깊이 이해해도 머리를 긁적이게 됩니다. 최근의 좋은 예시는 다음입니다. 모델은 이런 매우 간단한 질문을 잘 못합니다. 이것은 많은 사람들에게 충격적입니다. 이 수학 문제들은 복잡한 수학 문제를 풀 수 있고, 제가 할 수 있는 것보다 훨씬 더 잘 PhD 수준의 물리, 화학, 생물 질문에 답할 수 있지만

[2:05:30] 때때로 이런 초간단 문제에서 실패합니다. 여기 갑니다. 9.11이 9.9보다 크다고 하고 어떤 방식으로 정당화하지만 분명히, 그런 다음 끝에서 실제로 결정을 뒤집습니다. 이것이 매우 재현 가능하다고 생각하지 않습니다. 때때로 답을 뒤집고 때때로 맞고 때때로 틀리고. 다시 해봅시다.

[2:06:00] 더 크게 보일 수 있지만, 여기서 끝에서 자기 자신을 수정하지도 않습니다. 여러 번 물으면 때때로 맞기도 합니다. 하지만 모델이 올림피아드 수준 문제에서 그렇게 잘할 수 있는데 이런 매우 간단한 문제에서 실패하는 것은 어떻게 가능할까요? 이것은 언급했듯이 약간 머리를 긁적이게 합니다. 많은 사람들이 이것을 깊이 연구했고 논문을 실제로 읽지는 않았지만

[2:06:30] 이 팀에게서 들은 것은 신경망 내부의 활성화를 조사하면, 어떤 특성이 켜지거나 꺼지는지, 어떤 뉴런이 켜지거나 꺼지는지 보면, 신경망 내부의 많은 뉴런이 보통 성경 구절과 관련된 것들이 켜진다고 합니다. 모델이 이것이 거의 성경 구절 표시처럼 보인다고 상기되는 것 같습니다. 성경 구절 설정에서 9.11이 9.9 이후에 올 것입니다.

[2:07:00] 기본적으로 모델이 성경 구절에서 9.11이 더 클 것이라는 것을 인지적으로 매우 산만하게 찾습니다. 여기서 수학으로 정당화하고 답에 도달하려 해도 여전히 여기서 틀린 답이 나옵니다. 기본적으로 완전히 이해가 되지 않고 완전히 이해되지 않습니다. 이런 몇 가지 들쭉날쭉한 문제가 있습니다. 그래서 이것을 있는 그대로 취급하세요. 정말 마법 같지만 완전히 신뢰할 수도 없는 확률적 시스템입니다. 문제에 풀어놓고 결과를 복사 붙여넣기하는 것이 아니라 도구로 사용하세요.

---

## 16. 지도학습 미세조정에서 강화학습으로

**요약**: 지도학습 미세조정(SFT)에서 강화학습(RL)으로의 전환을 설명합니다. SFT는 모델에게 "어떻게 답해야 하는지" 가르치지만, RL은 모델이 스스로 더 나은 답을 찾도록 합니다. 이 단계에서 모델 성능이 크게 향상됩니다.

[2:07:30] 이제 대규모 언어 모델 학습의 두 가지 주요 단계를 다루었습니다. 첫 번째 단계는 사전학습 단계라고 불리고, 기본적으로 인터넷 문서에서 학습합니다. 인터넷 문서에서 언어 모델을 학습시키면 베이스 모델이라고 불리는 것을 얻고, 기본적으로 인터넷 문서 시뮬레이터입니다. 이것이 흥미로운 결과물이고

[2:08:00] 수천 대의 컴퓨터에서 여러 달 학습이 필요하고, 인터넷의 손실 압축이고, 매우 흥미롭지만 직접적으로 유용하지 않습니다. 인터넷 문서를 샘플링하고 싶지 않고, AI에게 질문하고 우리 질문에 응답하기를 원하기 때문입니다. 그것을 위해 어시스턴트가 필요하고, 후속학습 과정에서 어시스턴트를 실제로 구성할 수 있다는 것을 봤습니다. 특히 지도학습 미세조정이라고 부르는 과정에서요. 이 단계에서

[2:08:30] 알고리즘적으로 사전학습과 동일합니다. 바뀌는 것은 없습니다. 바뀌는 유일한 것은 데이터셋입니다. 인터넷 문서 대신 이제 대화의 매우 좋은 데이터셋을 만들고 큐레이션하고 싶습니다. 모든 종류의 다양한 주제에 대해 인간과 어시스턴트 사이의 수백만 대화를 원합니다. 근본적으로 이 대화들은 인간이 만듭니다. 인간이 프롬프트를 작성하고 인간이 이상적인 응답을 작성하고, 라벨링 문서를 기반으로 합니다.

[2:09:00] 현대 스택에서 실제로 완전히 인간이 수동으로 하지 않습니다. 이제 이 도구들에서 많은 도움을 받습니다. 언어 모델을 사용해서 이 데이터셋을 만드는 것을 돕고, 광범위하게 이루어지지만, 근본적으로 모든 것은 여전히 끝에서 인간 큐레이션에서 옵니다. 이 대화들을 만들고, 그것이 데이터셋이 되고, 파인튜닝하거나 계속 학습시키면 어시스턴트를 얻습니다. 그런 다음 기어를 바꿔서 이 어시스턴트가 어떤지에 대한 인지적 함의에 대해 이야기하기 시작했습니다.

[2:09:30] 어떤 종류의 완화를 취하지 않으면 어시스턴트가 환각을 일으킬 것이라는 것을 봤습니다. 환각이 흔할 것이고, 그 환각의 완화 중 일부를 봤습니다. 모델이 꽤 인상적이고 머릿속에서 많은 것을 할 수 있지만, 더 나아지기 위해 도구에 의존할 수 있다는 것을 봤습니다. 예를 들어 웹 검색에 의존해 환각을 줄이고 아마 더 최신 정보 등을 가져올 수 있습니다. 또는 코드 인터

---

## 17. 강화학습 (Reinforcement Learning)

**요약**: 강화학습(RL)의 핵심 개념을 설명합니다. 보상 함수(reward function)를 정의하고, 모델이 높은 보상을 받는 출력을 생성하도록 학습시킵니다. 수학 문제처럼 정답을 자동으로 검증할 수 있는 영역에서 특히 효과적입니다.

[2:15:00] 1차원 토큰 시퀀스입니다. 저는 사실 이 관점을 더 좋아하는데, 이게 LLM의 본래 관점이기 때문입니다. LLM이 실제로 보는 것은 토큰 ID입니다. 자, 에밀리는 사과 3개와 오렌지 2개를 삽니다. 각 오렌지는 2달러이고, 모든 과일의 총 가격은 13달러입니다. 사과 하나의 가격은 얼마일까요? 여기서 이해해 주셨으면 하는 것이 있습니다. 이것들은 예시로 든 네 가지 후보 풀이인데,

[2:15:30] 모두 정답 3에 도달합니다. 여기서 이해해 주셨으면 하는 점은, 제가 훈련 세트에 넣을 대화를 만드는 사람 데이터 라벨러라면, 이 대화들 중 어떤 것을 데이터셋에 추가해야 할지 실제로 잘 모른다는 것입니다. 어떤 풀이는 연립방정식을 세우고, 어떤 것은 영어로 설명하며, 어떤 것은

[2:16:00] 바로 답으로 건너뜁니다. ChatGPT를 보면 이 문제를 주면 변수들의 시스템을 정의하고 이런 작은 작업을 합니다. 하지만 구분해야 할 것이 있습니다. 풀이의 첫 번째 목적은 당연히 정답에 도달하는 것입니다. 최종 답 3을 얻는 것이 중요한 목적이죠. 하지만 두 번째 목적도 있는데, 사람을 위해 보기 좋게 만드는 것입니다.

[2:16:30] 사람이 풀이를 보고 싶어한다고 가정하고, 중간 단계를 보여주며, 보기 좋게 제시하려는 것이죠. 여기서 두 가지 별개의 일이 일어납니다. 첫째는 사람을 위한 표현이고, 둘째는 실제로 정답을 얻으려는 것입니다. 일단 최종 답에 도달하는 것에만 집중해 봅시다. 최종 답만 중요하다면, 이 중 어떤 것이 최적의

[2:17:00] 풀이일까요? 제가 말하고 싶은 것은, 우리는 모른다는 것입니다. 사람 라벨러로서 저는 어떤 것이 최선인지 모릅니다. 예를 들어, 앞서 토큰 시퀀스와 암산, 추론을 볼 때, 각 토큰에 대해 기본적으로 유한하고 그리 크지 않은 양의 연산만 할 수 있다는 것을 봤습니다.

[2:17:30] 하나의 토큰에서 너무 큰 도약을 할 수 없다는 것이죠. 예를 들어, 이 풀이의 좋은 점은 토큰이 매우 적어서 정답에 빨리 도달한다는 것입니다. 하지만 30 - 4를 3으로 나눌 때, 이 단일 토큰에서 많은 연산을 요구하고 있습니다. 그래서 이 예시는 LLM에게 나쁠 수 있는데, 계산을 빨리 건너뛰도록 유도하여

[2:18:00] 암산에서 실수를 하게 만들 수 있기 때문입니다. 더 펼쳐서 쓰는 게 나을 수도 있고, 방정식으로 세우는 게 나을 수도 있고, 설명하며 푸는 게 나을 수도 있습니다. 근본적으로 우리는 모릅니다. 왜냐하면 사람 라벨러인 우리에게 쉽거나 어려운 것이 LLM에게 쉽거나 어려운 것과 다르기 때문입니다. 인지 방식이 다릅니다.

[2:18:30] 저에게 사소한 토큰 시퀀스가 LLM에게는 너무 큰 도약일 수 있습니다. 반대로, 제가 만드는 많은 토큰이 LLM에게는 사소해서 토큰을 낭비하는 것일 수 있습니다. 최종 답만 중요하고 사람을 위한 표현을 분리한다면, 우리는 어떤 풀이를 LLM에게 줘야 할지 모릅니다.

[2:19:00] 우리는 LLM이 아니기 때문입니다. 수학 예시에서는 명확하지만, 이것은 매우 광범위한 문제입니다. 우리의 지식은 LLM의 지식이 아닙니다. LLM은 실제로 수학, 물리학, 화학 등에서 박사 수준의 엄청난 지식을 가지고 있습니다. 많은 면에서 저보다 더 많이 알고 있어서, 그 지식을 문제 해결에 활용하지 못할 수 있습니다. 반대로,

[2:19:30] 제가 풀이에 LLM의 매개변수에 없는 지식을 넣으면, 그것은 모델에게 혼란스러운 갑작스러운 도약이 됩니다. 우리의 인지 방식이 다르고, 최종 답을 효율적으로 얻는 것만 중요하다면 무엇을 넣어야 할지 모릅니다. 결론적으로, 우리는 LLM을 위한 토큰 시퀀스를 만들기에 좋은 위치에 있지 않습니다. 모방에 의해 시스템을 초기화하는 데는

[2:20:00] 유용하지만, 실제로는 LLM이 자신에게 맞는 토큰 시퀀스를 발견하기를 원합니다. 프롬프트가 주어졌을 때 신뢰할 수 있게 정답에 도달하는 토큰 시퀀스를 스스로 찾아야 하며, 이를 강화학습과 시행착오 과정에서 발견해야 합니다. 이 예시가 강화학습에서 어떻게 작동하는지 봅시다. 허깅페이스 추론 플레이그라운드로

[2:20:30] 돌아왔습니다. 여기서 다양한 모델을 쉽게 호출할 수 있습니다. 예를 들어, 오른쪽 상단에서 Gemma 2 20억 매개변수 모델을 선택했습니다. 20억은 매우 작은 모델이지만 괜찮습니다. 강화학습이 기본적으로 어떻게 작동하는지 봅시다. 실제로 꽤 간단합니다. 다양한 풀이를 시도하고 어떤 풀이가 잘 되는지 안 되는지 봐야 합니다.

[2:21:00] 프롬프트를 가져와서 모델을 실행하고, 모델이 풀이를 생성합니다. 그런 다음 풀이를 검사하는데, 이 문제의 정답이 3달러라는 것을 압니다. 실제로 모델이 맞췄고, 3달러라고 했습니다. 이것이 한 번의 풀이 시도입니다. 이제 이것을 삭제하고 다시 실행해 봅시다. 두 번째 시도입니다. 모델이 약간 다른 방식으로 풀었습니다.

[2:21:30] 매번 시도마다 다른 생성이 됩니다. 이 모델들은 확률적 시스템이라는 것을 기억하세요. 매 토큰마다 확률 분포가 있고 그로부터 샘플링합니다. 그래서 약간 다른 경로로 가게 됩니다. 이것도 정답에 도달하는 두 번째 풀이입니다. 삭제하고 세 번째로 가봅시다. 역시 약간 다른 풀이지만 정답을 맞췄습니다. 이것을 여러 번 반복할 수 있고,

[2:22:00] 실제로는 하나의 프롬프트에 대해 수천 또는 수백만 개의 독립적인 풀이를 샘플링할 수 있습니다. 일부는 정답이고 일부는 아닐 것입니다. 기본적으로 하고 싶은 것은 정답에 도달하는 풀이를 장려하는 것입니다. 어떻게 생겼는지 봅시다. 여기 간단한 다이어그램이 있습니다. 프롬프트가 있고, 여러 다른 풀이를 병렬로 시도했습니다.

[2:22:30] 일부 풀이는 잘 되어 정답을 얻었고(녹색), 일부는 잘 되지 않아 정답을 얻지 못했습니다(빨간색). 이 문제는 사실 좋은 예시가 아닌데, 사소한 프롬프트라서 20억 매개변수 모델도 항상 맞추기 때문입니다. 하지만 상상력을 발휘해서 녹색은 좋고 빨간색은 나쁘다고 가정합시다.

[2:23:00] 15개 풀이 중 4개만 정답을 맞췄습니다. 이제 정답에 도달하는 종류의 풀이를 장려하고 싶습니다. 빨간색 풀이에서 일어난 토큰 시퀀스는 어딘가에서 뭔가 잘못되었고, 좋은 경로가 아니었습니다. 녹색 풀이의 토큰 시퀀스는 잘 되었으니, 이런 프롬프트에서 이런 것을 더 하고 싶습니다. 미래에 이런 행동을 장려하는 방법은

[2:23:30] 기본적으로 이 시퀀스들을 훈련하는 것입니다. 하지만 이 훈련 시퀀스는 전문 사람 주석자에게서 온 것이 아닙니다. 이것이 정확한 풀이라고 결정한 사람이 없습니다. 이 풀이는 모델 자체에서 왔습니다. 모델이 여기서 연습하고 있습니다. 몇 가지 풀이를 시도했고, 네 개가 효과가 있었고, 이제 모델이 그것들을 훈련합니다.

[2:24:00] 이것은 학생이 자신의 풀이를 보고 "이게 정말 잘 됐으니, 이런 문제는 이렇게 풀어야겠다"고 하는 것과 같습니다. 이 예시에서 방법론을 조정하는 여러 방법이 있지만, 핵심 아이디어를 전달하기 위해 가장 간단하게, 이 네 개 중 가장 좋은 풀이 하나를 선택한다고 생각해 봅시다.

[2:24:30] 이것이 최고의 풀이입니다. 정답에 도달했을 뿐 아니라 다른 좋은 속성도 있을 수 있습니다. 가장 짧거나 보기 좋거나 다른 기준이 있을 수 있습니다. 이것을 최고 풀이로 결정하고 훈련하면, 매개변수 업데이트 후 모델이 앞으로 이런 상황에서 이 경로를 택할 확률이 약간 높아집니다. 하지만 많은 다양한 프롬프트를

[2:25:00] 실행한다는 것을 기억하세요. 수많은 수학과 물리학 문제 등, 수만 개의 프롬프트가 있고 프롬프트당 수천 개의 풀이가 있습니다. 이 과정을 반복하면서 모델은 어떤 토큰 시퀀스가 정답에 도달하는지 스스로 발견합니다. 사람 주석자에게서 오는 것이 아닙니다. 모델이 이 놀이터에서 놀면서 목표를 알고

[2:25:30] 자신에게 맞는 시퀀스를 발견합니다. 정신적 도약을 하지 않고 신뢰할 수 있게 통계적으로 작동하며 모델의 지식을 완전히 활용하는 시퀀스입니다. 이것이 강화학습 과정입니다. 기본적으로 추측하고 확인하는 것입니다. 다양한 유형의 풀이를 추측하고, 확인하고, 효과가 있었던 것을 미래에 더 합니다.

[2:26:00] 이전 내용과 관련하여, SFT 모델(지도학습 미세조정 모델)은 모델을 올바른 풀이의 근처로 초기화하는 데 여전히 도움이 됩니다. 풀이를 작성하게 하고, 연립방정식을 세우거나 풀이를 설명하는 것을 이해하게 합니다. 올바른 풀이의 근처로 가게 하지만, 강화학습에서 모든 것이 정교해집니다. 모델에게 맞는 풀이를 발견하고, 정답을 얻고, 장려하면

[2:26:30] 모델이 시간이 지나면서 더 좋아집니다. 이것이 대규모 언어 모델을 훈련하는 고수준 과정입니다. 요약하면, 아이들을 가르치는 방식과 매우 유사하게 훈련합니다. 유일한 차이점은 아이들은 책의 장을 거치며 각 장 내에서 다양한 훈련 연습을 하지만, AI를 훈련할 때는 단계 유형에 따라 단계별로 합니다.

[2:27:00] 첫째는 사전학습입니다. 이것은 기본적으로 모든 설명 자료를 읽는 것과 같습니다. 모든 교과서를 동시에 보고 모든 설명을 읽으며 지식 기반을 구축합니다. 둘째는 SFT 단계입니다. 모든 교과서에 걸쳐 인간 전문가의 모든 고정된 풀이, 모든 유형의 예제 풀이를 보는 것입니다. SFT 모델은 전문가를 모방할 수 있지만

[2:27:30] 맹목적으로 합니다. 통계적으로 전문가 행동을 흉내 내려고 최선을 다합니다. 마지막으로 RL 단계에서 모든 교과서의 연습 문제를 풀고, 그것이 RL 모델을 얻는 방법입니다. 고수준에서 LLM 훈련 방식은 아이들 교육 과정과 매우 동등합니다. 다음으로 말하고 싶은 점은, 처음 두 단계인 사전학습과 지도학습 미세조정은

[2:28:00] 수년간 있었고 매우 표준적이며 모든 LLM 제공자가 합니다. 마지막 단계인 RL 훈련이 개발 과정에서 훨씬 초기 단계이며 아직 분야에서 표준이 아닙니다. 이 단계는 훨씬 초기이고 미성숙합니다. 그 이유는 이 과정에서 수많은 세부 사항을 건너뛰었기 때문입니다. 고수준 아이디어는 시행착오 학습으로 매우 간단하지만,

[2:28:30] 최선의 풀이를 선택하는 방법, 얼마나 훈련할지, 프롬프트 분포는 무엇인지, 훈련을 어떻게 설정해야 실제로 작동하는지 등 수많은 세부 사항과 수학적 미묘함이 있습니다. 세부 사항을 맞추는 것이 쉽지 않습니다. OpenAI와 같은 많은 회사들이 LLM을 위한 강화학습 미세조정을 내부적으로 실험해 왔지만

[2:29:00] 공개적으로 말하지 않았습니다. 모두 회사 내부에서 이루어졌습니다. 그래서 딥시크의 논문이 최근에 나왔을 때 큰 화제가 되었습니다. 중국의 딥시크 AI라는 회사의 논문으로, LLM을 위한 강화학습 미세조정에 대해 매우 공개적으로 이야기하고, 그것이 LLM에 얼마나 중요한지, 모델에 많은 추론 능력을

[2:29:30] 어떻게 가져오는지 설명했습니다. 이 논문은 LLM에 RL을 사용하는 것에 대한 대중의 관심을 다시 불러일으켰고, 그들의 결과를 재현하고 이 단계를 실제로 작동시키는 데 필요한 많은 세부 사항을 제공했습니다. 딥시크 R1 논문을 간략히 살펴보고, RL을 언어 모델에 올바르게 적용하면 어떤 일이 일어나는지 봅시다.

[2:30:00] 먼저 스크롤해서 보여드릴 것은 이 그림 2입니다. 모델이 수학 문제를 푸는 것이 어떻게 개선되는지 보여줍니다. 이것은 수학 문제 풀이 정확도이고, 웹페이지에서 실제로 어떤 종류의 수학 문제가 측정되는지 볼 수 있습니다. 간단한 수학 문제들로, 모델들이 풀어야 하는 문제들입니다.

[2:30:30] 처음에는 잘 못하지만, 수천 번의 단계로 모델을 업데이트하면서 정확도가 계속 올라갑니다. 모델이 개선되고, 수학 문제를 더 높은 정확도로 풀고 있습니다. 다양한 문제의 대규모 데이터셋에서 시행착오를 통해 수학 문제를 푸는 방법을 발견하고 있습니다. 하지만 정량적 결과보다 더 놀라운 것은

[2:31:00] 모델이 이 결과를 달성하는 정성적 방법입니다. 스크롤을 내려보면, 흥미로운 그림 중 하나는 최적화 후반에 모델이 응답당 평균 길이가 올라간다는 것입니다. 모델이 더 높은 정확도 결과를 얻기 위해 더 많은 토큰을 사용하는 것 같습니다. 왜 이 풀이들이 매우 길까요?

[2:31:30] 정성적으로 살펴봅시다. 기본적으로 모델 풀이가 매우 길어지는데, 부분적으로는 이렇게 합니다. 여기 질문이 있고 모델의 답이 있습니다. 모델이 배운 것, 그리고 이것은 최적화의 새로운 속성으로 발견된 것인데, 문제 해결에 좋다는 것을 발견합니다. "잠깐, 잠깐, 잠깐, 이건 아닌 것 같아. 이 단계를 다시 평가해서 올바른 합을 찾아보자"라고 합니다.

[2:32:00] 모델이 여기서 무엇을 하고 있을까요? 모델은 기본적으로 단계를 재평가하고 있습니다. 정확도를 위해 많은 아이디어를 시도하고, 다른 관점에서 시도하고, 되추적하고, 재구성하고, 백트래킹하는 것이 더 낫다는 것을 배웠습니다. 수학 문제에 대한 문제 해결 과정에서 당신과 제가 하는 많은 것들을 하고 있습니다. 하지만 당신의 머릿속에서 일어나는 것을 재발견하고 있지,

[2:32:30] 풀이에 적는 것이 아닙니다. 어떤 사람도 이것을 이상적인 어시스턴트 응답에 하드코딩할 수 없습니다. 이것은 강화학습 과정에서만 발견될 수 있습니다. 왜냐하면 여기에 무엇을 넣어야 할지 모르기 때문입니다. 이것이 모델에게 효과가 있고 문제 해결 정확도를 높이는 것으로 밝혀졌습니다.

[2:33:00] 모델은 당신의 머릿속에서 우리가 Chain of Thought라고 부르는 것을 배우고, 이것은 최적화의 창발적 속성입니다. 그것이 응답 길이를 부풀리고 있지만, 문제 해결 정확도도 높이고 있습니다. 놀라운 것은 기본적으로 모델이 생각하는 방법을 발견하고 있다는 것입니다. 제가 인지 전략이라고 부르는 것을 배우고 있습니다.

[2:33:30] 문제를 어떻게 조작하고 다른 관점에서 접근하는지, 유추를 끌어오거나 다양한 것들을 어떻게 하는지, 시간이 지나면서 많은 다른 것을 시도하고, 다른 관점에서 결과를 확인하고, 문제를 푸는 방법을 발견합니다. 여기서는 RL에 의해 발견됩니다. 어디에도 하드코딩하지 않고 이것이 최적화에서 나타나는 것을 보는 것은 매우 놀랍습니다.

[2:33:30] 우리가 준 것은 정답뿐이고, 이것이 그냥 정확하게 풀려고 하는 것에서 나옵니다. 이제 우리가 작업해 온 문제로 돌아가서, 추론 또는 사고 모델이라고 부르는 것이 이 문제를 어떻게 풀지 살펴봅시다. 이것이 우리가 작업해 온 문제이고, ChatGPT 4o에 붙여넣으면

[2:34:00] 이런 종류의 응답을 얻습니다. 같은 쿼리를 추론 또는 사고 모델, 즉 강화학습으로 훈련된 모델에 주면 어떻게 되는지 봅시다. 이 논문에서 설명한 딥시크 R1 모델은 chat.deepseek.com에서 이용 가능합니다. 이 회사가 호스팅하고 있습니다. Deep think 버튼이 켜져 있는지 확인하세요.

[2:34:30] R1 모델을 얻을 수 있습니다. 여기에 붙여넣고 실행해 봅시다. 이전에는 기본적으로 SFT 접근법, 지도학습 미세조정 접근법에서 이것을 얻습니다. 전문가 풀이를 흉내 내는 것이죠. 이것이 RL 모델에서 얻는 것입니다. "이것을 풀어보자. 에밀리가 사과 3개와 오렌지 2개를 사고..." 등등. 이것을 읽으면서

[2:35:00] 이 모델이 생각하고 있다는 느낌을 피할 수 없습니다. 풀이를 추구하고 있고, 3달러일 것이라고 추론하고, "잠깐, 내 계산을 다시 확인해 보자"라고 합니다. 약간 다른 관점에서 시도합니다. "맞아, 다 맞아. 이게 답인 것 같아. 실수가 없는지 보자. 문제에 접근하는 다른 방법이 있나? 방정식을 세워보자.

[2:35:30] 사과 하나의 가격을 A달러라고 하면..." 등등. "맞아, 같은 답이야. 각 사과는 3달러야. 이게 맞는 것 같아." 그런 다음 사고 과정을 마치고 사람을 위해 깔끔한 풀이를 작성합니다. 이것은 정확성 측면이고, 이것은 표현 측면입니다. 깔끔하게 작성하고

[2:35:30] 맨 아래에 정답을 박스로 표시합니다. 놀라운 것은 모델의 사고 과정을 얻는다는 것이고, 이것이 강화학습 과정에서 나오는 것입니다. 토큰 시퀀스의 길이를 부풀리는 것입니다. 생각하고 다른 방법을 시도합니다. 이것이 문제 해결에서 더 높은 정확도를 주고, "아하" 순간과

[2:36:00] 다른 전략, 정답을 확실히 얻는 방법에 대한 아이디어를 보는 곳입니다. 마지막으로 말하고 싶은 점은, 일부 사람들이 chat.deepseek.com에 민감한 데이터를 넣는 것에 대해 조금 긴장합니다. 중국 회사이기 때문입니다. 딥시크 R1은 이 회사가 출시한 모델로, 오픈 소스 모델 또는 오픈 웨이트 모델입니다. 누구나

[2:36:30] 다운로드하고 사용할 수 있습니다. 전체 모델을 전체 정밀도로 맥북이나 로컬 장치에서 실행할 수는 없습니다. 꽤 큰 모델이기 때문입니다. 하지만 많은 회사들이 전체 모델을 호스팅하고 있습니다. 제가 사용하기 좋아하는 회사 중 하나는 together.ai입니다. together.ai에 가서 가입하고 플레이그라운드에 가면

[2:37:00] 딥시크 R1을 선택할 수 있고, 다른 많은 종류의 모델도 선택할 수 있습니다. 모두 최첨단 모델입니다. 지금까지 사용해 온 허깅페이스 추론 플레이그라운드와 비슷하지만, together.ai는 보통 모든 최첨단 모델을 호스팅합니다. 딥시크 R1을 선택하고, 기본 설정으로 괜찮을 것입니다. 딥시크가 모델을 출시했기 때문에

[2:37:30] 여기서 얻는 것은 기본적으로 여기서 얻는 것과 동등해야 합니다. 샘플링의 무작위성 때문에 약간 다른 것을 얻겠지만, 원칙적으로 모델의 성능 면에서는 동일해야 합니다. 같은 것을 정량적으로나 정성적으로 볼 수 있어야 합니다. 하지만 이 모델은 미국 회사에서 오는 것이니까요. 이것이 딥시크와 추론 모델이라는 것입니다.

[2:38:00] ChatGPT로 돌아가면, 드롭다운에서 보이는 모델 중 일부, 예를 들어 o1, o3 mini, o3 mini high 등은 "고급 추론 사용"이라고 말합니다. 이것이 의미하는 것은 딥시크 R1과 매우 유사한 기술로 강화학습으로 훈련되었다는 것입니다. OpenAI 직원들의 공식 발표에 따르면요.

[2:37:30] 이것들은 RL로 훈련된 사고 모델이고, GPT-4o나 GPT-4o mini 같은 모델은 무료 티어에서 얻는 것인데, 대부분 SFT 모델로 생각해야 합니다. RL 모델에서 보는 것처럼 사고를 하지 않습니다. 이 모델들에도 약간의 강화학습이 관여되어 있지만, 잠시 후 그것에 대해 다루겠습니다. 대부분 SFT 모델이라고 생각해야 합니다.

---

## 18. 딥시크 R1 (DeepSeek-R1)

**요약**: 딥시크 R1 모델과 그 특징을 설명합니다. 중국 스타트업 딥시크가 개발한 이 모델은 순수 RL만으로 학습되어 자체적인 사고 과정(thinking)을 발전시켰습니다. 때로는 영어에서 중국어로 전환하며 생각하는 등 흥미로운 행동을 보입니다.

[2:38:00] 같은 방식으로 사고 모델 중 하나, 예를 들어 o3 mini high를 선택하고 실행해 볼 수 있습니다. 이 모델들은 월 20달러 또는 일부 최고 모델의 경우 월 200달러의 ChatGPT 구독을 지불하지 않으면 사용할 수 없을 수 있습니다. 사고 모델을 선택하고 실행하면, "추론 중"이라고 하면서

[2:38:30] 이런 것들을 시작합니다. 여기서 보는 것은 딥시크에서 보는 것과 정확히 같지 않습니다. 내부적으로 모델이 이런 종류의 Chain of Thought를 생성하지만, OpenAI는 웹 인터페이스에서 정확한 Chain of Thought를 보여주지 않기로 선택했습니다. 그 사고 체인의 작은 요약을 보여줍니다. OpenAI가 이렇게 하는 이유는

[2:39:00] 부분적으로 증류 위험이라고 부르는 것 때문입니다. 누군가 와서 그 추론 흔적을 모방하고 추론 Chain of Thought를 흉내 내는 것만으로 많은 추론 성능을 복구할 수 있다는 것입니다. 그래서 숨기고 작은 요약만 보여줍니다. 딥시크에서 얻는 것처럼 추론 자체에 대한 완전한 세부 사항을 얻지 못합니다.

[2:39:30] 그런 다음 풀이를 작성합니다. 이것들은 종류가 동등합니다. 비록 완전한 내부 세부 사항을 보지 못하지만요. 성능 면에서 이 모델들과 딥시크 모델은 현재 거의 동등합니다. 평가 때문에 말하기 어렵지만, OpenAI에 월 200달러를 지불하면 일부 모델은 여전히 더 좋아 보인다고 생각합니다. 하지만 딥시크 R1은 지금으로서는 여전히 매우 견고한

[2:40:00] 사고 모델 선택입니다. 이 웹사이트나 다른 웹사이트에서 이용 가능합니다. 모델이 오픈 웨이트이므로 그냥 다운로드할 수 있습니다. 지금까지의 요약은 무엇일까요? 강화학습과 검증 가능한 풀이가 있는 많은 수학 및 코드 문제에서 기본적으로 RL을 실행할 때 최적화 과정에서 사고가 나타난다는 것을 이야기했습니다.

[2:40:30] 이 사고 모델은 예를 들어 딥시크나 together.ai 같은 추론 제공자에서 접근할 수 있고, 거기서 딥시크를 선택합니다. ChatGPT에서도 o1 또는 o3 모델 아래에서 사고 모델을 이용할 수 있습니다. 하지만 GPT-4o 모델 등은 사고 모델이 아닙니다. 대부분 SFT 모델로 생각해야 합니다. 고급 추론이 필요한 프롬프트가 있다면

[2:41:00] 아마 사고 모델을 사용하거나 최소한 시도해 봐야 합니다. 하지만 경험적으로 많은 제 사용에서, 더 간단한 질문이나 지식 기반 질문 같은 것을 할 때는 과도할 수 있습니다. 사실적인 질문에 30초 동안 생각할 필요가 없습니다. 그래서 그런 경우에는 때때로 GPT-4o로 기본 설정합니다. 경험적으로 제 사용의 약 80-90%는 GPT-4o이고, 수학과 코드 등에서 매우 어려운 문제를 만나면

[2:41:30] 사고 모델을 사용합니다. 하지만 그러면 생각하기 때문에 조금 더 기다려야 합니다. chat, 딥시크에서 접근할 수 있고, AI studio.google.com도 지적하고 싶습니다. 매우 바쁘고 정말 못생겼는데, 구글이 이런 것을 잘 못하기 때문입니다. 하지만 모델을 선택하고 Gemini 2.0 flash thinking experimental 01 21을 선택하면

[2:42:00] 그것도 구글의 사고 모델의 초기 실험 버전입니다. 여기 가서 같은 문제를 주고 실행을 클릭하면, 비슷한 것을 하고 여기서 정답을 내는 사고 모델입니다. 기본적으로 제미니도 사고 모델을 제공합니다. 앤트로픽은 현재 사고 모델을 제공하지 않습니다. 하지만 기본적으로 이것이 LLM의 최전선 개발입니다.

[2:42:30] RL이 이 새롭고 흥미로운 단계이지만, 세부 사항을 맞추는 것이 어렵고, 그래서 이 모든 모델과 사고 모델은 현재 2025년 초 기준으로 실험적입니다. 하지만 이것이 추론을 사용하여 매우 어려운 문제에서 성능을 높이는 최전선 개발입니다. 한 가지 더 연결하고 싶은 것은 강화학습이 매우 강력한 학습 방법이라는 발견은

[2:43:00] AI 분야에서 새로운 것이 아니고, 이것이 이미 시연된 곳 중 하나는 바둑 게임입니다. 유명하게 딥마인드가 알파고 시스템을 개발했고, 그것에 대한 영화를 볼 수 있습니다. 시스템이 최고의 인간 선수들을 상대로 바둑을 두는 것을 배웁니다. 알파고의 기반이 되는 논문으로 가면

[2:43:30] 정말 흥미로운 플롯을 찾을 수 있습니다. 우리에게 익숙하고, 바둑의 폐쇄적이고 구체적인 영역 대신 임의의 문제 해결의 더 열린 영역에서 재발견하고 있습니다. 그들이 본 것, 그리고 이것이 더 성숙해지면서 LLM에서도 볼 것은

[2:44:00] 이것이 바둑을 두는 엘로 레이팅이고, 이것이 이세돌, 매우 강한 인간 선수입니다. 여기서 비교하는 것은 지도학습으로 훈련된 모델의 강도와 강화학습으로 훈련된 모델입니다. 지도학습 모델은 인간 전문가 선수를 모방합니다. 바둑에서 전문가 선수들이 둔 거대한 양의 게임을 가져와서 그들을 모방하려고 하면 더 좋아지지만

[2:44:30] 정점에 도달하고 바둑의 최고 선수들보다 더 좋아지지 않습니다. 이세돌 같은 선수에게는 절대 도달하지 못합니다. 왜냐하면 인간 선수를 모방하는 것만으로는 근본적으로 인간 선수를 넘어설 수 없기 때문입니다. 하지만 강화학습 과정은 훨씬 더 강력합니다. 바둑에서 강화학습은 시스템이

[2:44:30] 경험적으로, 통계적으로 게임에서 이기는 수를 둔다는 것을 의미합니다. 알파고는 자기 자신과 대국하며 강화학습을 사용해서 롤아웃을 만드는 시스템입니다. 여기와 정확히 같은 다이어그램이지만, 프롬프트가 없습니다. 고정된 바둑 게임이니까요. 하지만 많은 풀이를 시도합니다.

[2:45:00] 많은 수를 시도하고, 특정 답 대신 승리로 이어지는 게임이 강화됩니다. 더 강해집니다. 시스템은 기본적으로 경험적으로, 통계적으로 게임에서 이기는 행동의 시퀀스를 배웁니다. 강화학습은 인간 성능에 의해 제한되지 않고, 훨씬 더 잘할 수 있고

[2:45:30] 이세돌 같은 최고 선수도 이길 수 있습니다. 아마 더 오래 실행할 수 있었고, 비용이 들기 때문에 어느 시점에서 자르기로 했지만, 이것은 강화학습의 매우 강력한 시연입니다. 우리는 이제 대규모 언어 모델에서 추론 문제에 대해 이 다이어그램의 힌트를 보기 시작하고 있습니다.

[2:45:30] 전문가를 모방하는 것만으로는 너무 멀리 갈 수 없고, 그것을 넘어서 이런 작은 게임 환경을 설정하고 시스템이 추론 흔적이나 문제를 푸는 방법을 발견하게 해야 합니다. 바둑 같은 폐쇄 영역 대신 열린 사고에서요.

[2:46:00] 이 독특함 측면에서 잘하고 있을 때, 강화학습은 인간이 게임을 하는 분포에서 벗어나는 것을 막지 않습니다. 알파고 검색으로 돌아가면, 제안된 수정 사항 중 하나가 37수라고 불립니다. 알파고의 37수는 알파고가 기본적으로

[2:46:30] 인간 전문가가 두지 않을 수를 둔 특정 시점을 가리킵니다. 이 수가 인간 선수에 의해 놓일 확률은 약 1만 분의 1로 평가되었습니다. 매우 희귀한 수이지만, 돌이켜보면 빛나는 수였습니다. 알파고가 인간에게 알려지지 않았지만

[2:46:30] 돌이켜보면 훌륭한 전략을 발견했습니다. 이 유튜브 동영상을 추천합니다. "이세돌 대 알파고 37수 반응과 분석"인데, 알파고가 이 수를 뒀을 때 이런 모습이었습니다. "그것은 매우 놀라운 수입니다. 실수라고 생각했습니다.

[2:47:00] 어쨌든" 기본적으로 사람들이 당황하고 있습니다. 알파고가 훈련에서 좋은 아이디어로 보여서 뒀지만, 인간이 하지 않을 종류의 수이기 때문입니다. 이것이 다시 강화학습의 힘이고, 원칙적으로 언어 모델에서 이 패러다임을 계속 확장하면 이것의 동등물을 볼 수 있습니다.

[2:47:30] 어떻게 생겼는지는 알려지지 않았습니다. 인간조차 얻을 수 없는 방식으로 문제를 푸는 것이 무엇을 의미할까요? 어떻게 추론이나 사고에서 인간보다 더 잘할 수 있을까요? 어떻게 생각하는 인간을 넘어설 수 있을까요? 인간이 만들 수 없는 유추를 발견하는 것일 수도 있고, 새로운 사고 전략일 수도 있습니다.

[2:47:30] 어쩌면 영어조차 아닌 완전히 새로운 언어일 수도 있습니다. 모델이 영어를 고수할 제약이 없으므로 사고에 훨씬 더 나은 자체 언어를 발견할 수도 있습니다. 원칙적으로 시스템의 행동은 훨씬 덜 정의되어 있습니다. 효과가 있는 것은 무엇이든 할 수 있고, 영어인 훈련 데이터의 분포에서 천천히

[2:48:00] 표류할 수 있습니다. 하지만 이 모든 것은 이 전략을 정제하고 완성할 수 있는 매우 크고 다양한 문제 집합이 있어야만 할 수 있습니다. 그것이 현재 진행 중인 최전선 LLM 연구의 많은 부분입니다. 크고 다양한 프롬프트 분포를 만들려고 합니다.

[2:48:30] 이것들은 모두 LLM이 사고를 연습할 수 있는 게임 환경입니다. 모든 지식 영역에 대해 연습 문제를 작성하는 것과 같습니다. 연습 문제가 있고 많으면, 모델이 그것들에 대해 강화학습할 수 있고, 바둑 같은 폐쇄 영역 대신 열린 사고 영역에서 이런 종류의 다이어그램을 만들 수 있습니다.

[2:49:00] 강화학습 내에서 다루고 싶은 섹션이 하나 더 있는데, 검증 불가능한 영역에서의 학습입니다. 지금까지 본 모든 문제는 검증 가능한 영역에 있습니다. 어떤 후보 풀이든 구체적인 답에 대해 매우 쉽게 점수를 매길 수 있습니다. 예를 들어, 답이 3이고, 모델이 답을 박스에 넣게 요구하고

[2:49:30] 박스 안의 것이 답과 같은지 확인하거나, LLM 판사를 사용할 수도 있습니다. LLM 판사가 풀이를 보고 답을 얻어서 기본적으로 풀이가 답과 일치하는지 점수를 매깁니다. LLM은 현재 능력에서 경험적으로 이것을 꽤 신뢰할 수 있게 할 수 있습니다.

[2:50:00] 어쨌든 구체적인 답이 있고 풀이를 그것에 대해 확인하고, 루프에 사람 없이 자동으로 할 수 있습니다. 문제는 검증 불가능한 영역에서는 이 전략을 적용할 수 없다는 것입니다. 보통 이것들은 예를 들어 창작 글쓰기 작업입니다. 펠리컨에 대한 농담을 쓰거나 시를 쓰거나 문단을 요약하거나 그런 것입니다.

[2:50:30] 이런 종류의 영역에서는 이 문제에 대한 다른 풀이들을 점수 매기기가 더 어렵습니다. 예를 들어, 펠리컨에 대한 농담을 쓰는 것이라면, 많은 다른 농담을 생성할 수 있습니다. ChatGPT에 가서 펠리컨에 대한 농담을 생성하게 할 수 있습니다. "부리에 너무 많은 것을 넣어서 가방을 안 믿기 때문이야."

[2:51:00] 좋아요, 다른 것을 시도해 봅시다. "왜 펠리컨은 음료값을 안 내죠? 항상 다른 사람에게 빌리니까요." 하하, 좋아요. 이 모델들은 분명히 유머를 잘 못합니다. 사실 매력적인데, 유머가 비밀스럽게 매우 어렵다고 생각하기 때문입니다. 어쨌든 많은 농담을 만들 수 있다고 상상할 수 있습니다.

[2:51:30] 우리가 직면하는 문제는 그것들을 어떻게 점수 매기느냐입니다. 원칙적으로 물론 사람이 제가 방금 한 것처럼 이 모든 농담을 볼 수 있습니다. 문제는 강화학습을 하면 수천 번의 업데이트를 하고, 각 업데이트에서 수천 개의 프롬프트를 보고 싶고, 각 프롬프트에서 잠재적으로 수백 또는 수천 가지 다른 종류의 생성을 봐야 합니다.

[2:52:00] 그래서 볼 것이 너무 많습니다. 원칙적으로 사람이 모두 검사하고 점수를 매기고 결정할 수 있습니다. 이것이 재미있고, 이것이 재미있고, 이것이 재미있다고. 적어도 펠리컨의 맥락에서 농담을 더 잘하도록 훈련할 수 있습니다. 문제는 사람의 시간이 너무 많이 필요하다는 것입니다.

[2:52:30] 확장 불가능한 전략입니다. 이를 위한 자동 전략이 필요합니다. 이에 대한 한 가지 해결책이 인간 피드백 기반 강화학습을 도입한 이 논문에서 제안되었습니다. 당시 OpenAI의 논문이었고, 이 사람들 중 많은 사람이 지금 앤트로픽의 공동 창업자입니다.

[2:53:00] 이것은 기본적으로 검증 불가능한 영역에서 강화학습을 하는 접근법을 제안했습니다. 어떻게 작동하는지 봅시다. 이것이 핵심 아이디어의 만화 다이어그램입니다. 말했듯이, 무한한 사람 시간이 있다면 이 영역에서 RL을 잘 실행할 수 있습니다.

[2:53:30] 예를 들어, 무한한 사람이 있다면 RL을 평소처럼 실행할 수 있습니다. 1,000번의 업데이트를 하고 싶고, 각 업데이트는 1,000개의 프롬프트에 대해 하고, 각 프롬프트에 대해 1,000개의 롤아웃을 점수 매깁니다. 이런 설정으로 RL을 실행할 수 있습니다. 문제는 이 과정에서

[2:54:00] 사람에게 농담을 평가하도록 총 10억 번 요청해야 한다는 것입니다. 그것은 많은 사람들이 정말 형편없는 농담을 보는 것입니다. 그렇게 하고 싶지 않습니다. 대신 RLHF 접근법을 사용합니다. RLHF 접근법에서 핵심 트릭은

[2:54:30] 간접성입니다. 사람을 약간만 참여시키고, 속이는 방법은 보상 모델이라고 부르는 완전히 별개의 신경망을 훈련하는 것입니다. 이 신경망은 사람 점수를 모방합니다. 사람에게 롤아웃을 점수 매기도록 요청하고, 신경망을 사용해서 사람 점수를 모방합니다.

[2:55:00] 이 신경망은 인간 선호도의 일종의 시뮬레이터가 됩니다. 이제 신경망 시뮬레이터가 있으니, 그것에 대해 RL을 할 수 있습니다. 실제 사람에게 묻는 대신, 예를 들어 농담에 대한 점수를 시뮬레이션된 사람에게 묻는 것입니다. 시뮬레이터가 있으면 원하는 만큼 쿼리할 수 있고, 완전히 자동화된 과정이고,

[2:55:30] 시뮬레이터에 대해 강화학습을 할 수 있습니다. 시뮬레이터는 예상할 수 있듯이 완벽한 인간이 아니지만, 적어도 통계적으로 인간 판단과 유사하다면 뭔가 할 것입니다. 실제로 그렇습니다. 시뮬레이터가 있으면 RL을 할 수 있고, 모든 것이 잘 작동합니다.

[2:56:00] 보상 모델 훈련이 어떻게 생겼는지 만화 다이어그램을 보여드리겠습니다. 세부 사항이 100% 중요한 것은 아니고, 어떻게 작동하는지의 핵심 아이디어일 뿐입니다. 가상의 예시의 만화 다이어그램이 있습니다. "펠리컨에 대한 농담을 써라"라는 프롬프트가 있고, 다섯 개의 별개 롤아웃이 있습니다. 이것들은 모두 다른 농담입니다.

[2:56:30] 먼저 할 일은 사람에게 이 농담들을 최고에서 최악으로 순서를 매기도록 요청하는 것입니다. 이 사람은 이 농담이 가장 재미있다고 생각했습니다. 1위 농담, 2위, 3위, 4위, 5위. 이것이 최악의 농담입니다. 점수를 직접 주는 대신 순서를 매기도록 요청하는데,

[2:57:00] 약간 더 쉬운 작업이기 때문입니다. 사람이 정확한 점수를 주는 것보다 순서를 매기는 것이 더 쉽습니다. 이것이 모델에 대한 지도입니다. 사람이 순서를 매겼고, 그것이 훈련 과정에 대한 그들의 기여입니다. 하지만 별개로 보상 모델에 이 농담들의 점수를 물어볼 것입니다.

[2:57:30] 보상 모델은 완전히 별개의 신경망입니다. 아마도 트랜스포머이지만, 다양한 언어를 생성하는 언어 모델이 아닙니다. 점수 매기기 모델일 뿐입니다. 보상 모델은 입력으로 프롬프트와 후보 농담을 받습니다.

[2:58:00] 두 입력이 보상 모델에 들어갑니다. 예를 들어, 여기서 보상 모델은 이 프롬프트와 이 농담을 받습니다. 보상 모델의 출력은 단일 숫자이고, 이 숫자는 점수로 생각됩니다. 예를 들어 0에서 1까지 범위일 수 있습니다. 0은 최악의 점수이고 1은 최고의 점수입니다.

[2:58:30] 훈련 과정의 어느 단계에서 가상의 보상 모델이 이 농담들에 줄 점수의 예시가 있습니다. 0.1은 매우 낮은 점수이고, 0.8은 정말 높은 점수입니다. 이제 보상 모델이 준 점수와 사람이 준 순서를 비교합니다.

[2:59:00] 이것을 계산하는 정확한 수학적 방법이 있습니다. 손실 함수를 설정하고 일치도를 계산하고 그것에 기반해 모델을 업데이트합니다. 하지만 직관을 드리고 싶습니다. 예를 들어, 이 두 번째 농담에 대해 사람은 가장 재미있다고 생각했고, 모델도 동의했습니다. 0.8은 상대적으로 높은 점수입니다.

[2:59:30] 하지만 이 점수는 더 높았어야 합니다. 업데이트 후에 이 점수는 실제로 0.81 정도로 자랄 것입니다. 이것에 대해서는 실제로 큰 불일치가 있습니다. 사람은 2위라고 생각했지만 점수는 0.1뿐입니다. 이 점수는 훨씬 더 높아져야 합니다.

[3:00:00] 업데이트 후에 0.15 정도가 될 수 있습니다. 그리고 사람은 이것이 최악의 농담이라고 생각했지만, 모델은 꽤 높은 숫자를 줬습니다. 업데이트 후에 3.5 정도로 내려갈 것입니다. 기본적으로 전에 했던 것을 하고 있습니다. 신경망 훈련 과정을 사용해서 모델의 예측을 약간 조정하고,

[3:00:30] 보상 모델 점수가 인간 순서와 일치하게 하려고 합니다. 인간 데이터에서 보상 모델을 업데이트하면서, 사람들이 제공하는 점수와 순서의 더 좋은 시뮬레이터가 됩니다. 그런 다음 그것에 대해 RL을 할 수 있는 인간 선호도의 시뮬레이터가 됩니다.

[3:01:00] 중요한 것은 사람에게 10억 번 농담을 보라고 요청하는 것이 아니라, 천 개의 프롬프트와 각각 5개의 롤아웃, 총 5,000개의 농담을 보고 순서만 매기면 됩니다. 그런 다음 모델을 그 순서와 일치하게 훈련합니다. 수학적 세부 사항은 건너뛰지만

[3:01:30] 고수준 아이디어를 이해해 주셨으면 합니다. 보상 모델이 기본적으로 이 점수를 주고, 인간 순서와 일치하게 훈련하는 방법이 있습니다. 이것이 RLHF가 작동하는 방식입니다. 대략적인 아이디어입니다. 기본적으로 인간의 시뮬레이터를 훈련하고 그 시뮬레이터에 대해 RL을 합니다.

[3:02:00] 먼저 인간 피드백 기반 강화학습의 장점을 말하겠습니다. 첫째, 매우 강력한 기술 집합인 강화학습을 실행할 수 있게 해주고, 검증 불가능한 것들을 포함한 임의의 영역에서 할 수 있게 합니다. 요약, 시 쓰기, 농담 쓰기, 또는 다른 창작 글쓰기,

[3:02:30] 수학과 코드 외의 영역에서요. 경험적으로 RLHF를 적용하면 모델 성능이 향상됩니다. 왜 그런지에 대한 추측이 있지만, 잘 확립되어 있는지는 모르겠습니다. 경험적으로 RLHF를 올바르게 하면 모델이 약간 더 좋아지지만,

[3:03:00] 왜인지는 명확하지 않습니다. 제 최선의 추측은 아마도 판별자-생성자 격차 때문일 것입니다. 많은 경우 사람에게 판별하는 것이 생성하는 것보다 훨씬 쉽습니다. 특히 지도학습 미세조정(SFT)에서 사람에게 이상적인 어시스턴트 응답을 생성하도록 요청하는데,

[3:03:30] 많은 경우 이상적인 응답을 쓰기가 매우 어렵습니다. 예를 들어 요약이나 시 쓰기, 농담 쓰기에서 사람 라벨러로서 어떻게 이상적인 응답을 줄 수 있을까요? 창의적인 인간 글쓰기가 필요합니다. RLHF는 이것을 우회하는데,

[3:04:00] 사람들에게 훨씬 더 쉬운 질문을 할 수 있기 때문입니다. 데이터 라벨러로서 직접 시를 쓰라고 요청받는 것이 아니라, 모델에서 다섯 개의 시를 받고 순서만 매기면 됩니다. 사람 라벨러에게 훨씬 더 쉬운 작업입니다.

[3:04:30] 이것이 기본적으로 더 높은 정확도의 데이터를 허용한다고 생각합니다. 매우 어려울 수 있는 생성 작업을 하라고 요청하는 것이 아니라, 창의적 글쓰기를 구별하고 가장 좋은 것을 찾으라고 합니다. 그것이 사람이 제공하는 신호이고, 순서일 뿐이고, 시스템에 대한 그들의 입력입니다.

[3:05:00] 그런 다음 RLHF 시스템이 사람들에게 좋은 평가를 받을 종류의 응답을 발견합니다. 그 간접성의 단계가 모델을 약간 더 좋게 만듭니다. 이것이 RLHF의 장점입니다. RL을 실행할 수 있게 하고, 경험적으로 더 나은 모델을 만들고, 이상적인 응답을 쓰는 매우 어려운 작업을 하지 않고도

[3:05:30] 사람들이 지도를 기여할 수 있게 합니다. 불행히도 RLHF에는 상당한 단점도 있습니다. 주요한 것은 기본적으로 인간과 실제 인간 판단이 아니라 인간의 손실 있는 시뮬레이션에 대해 강화학습을 한다는 것입니다.

[3:06:00] 이 손실 있는 시뮬레이션은 오해의 소지가 있을 수 있습니다. 그냥 시뮬레이션이니까요. 점수를 출력하는 언어 모델이고, 모든 가능한 다른 경우에서 실제 두뇌를 가진 실제 사람의 의견을 완벽하게 반영하지 못할 수 있습니다.

[3:06:30] 그것이 첫 번째입니다. 더 미묘하고 교활한 것이 있는데, 이것이 RLHF를 훨씬 더 똑똑한 시스템으로 확장하는 기술로서 극적으로 억제합니다. 강화학습은 시뮬레이션, 모델을 게임하는 방법을 발견하는 데 매우 뛰어납니다.

---

## 19. 알파고 (AlphaGo)

**요약**: 딥마인드의 알파고와 알파제로를 통해 강화학습의 역사를 설명합니다. 알파고는 바둑에서 인간을 이기기 위해 RL을 사용했으며, 알파제로는 인간 데이터 없이 자기 대국만으로 학습했습니다. 이 접근법이 현재 LLM 학습에도 적용되고 있습니다.

[3:07:00] 우리가 구성하는 이 보상 모델은 점수를 출력하는데, 이 모델들은 트랜스포머이고, 수십억 개의 매개변수를 가진 거대한 신경망이고, 인간을 모방하지만 시뮬레이션 방식으로 합니다. 문제는 이것들이 거대하고 복잡한 시스템이라는 것입니다. 여기에 10억 개의 매개변수가 단일 점수를 출력합니다.

[3:07:30] 이 모델들을 게임하는 방법이 있다는 것이 밝혀졌습니다. 훈련 세트의 일부가 아니었던 종류의 입력을 찾을 수 있고, 이 입력들은 설명할 수 없이 매우 높은 점수를 받지만 가짜 방식으로요. RLHF를 매우 오래 실행하면, 예를 들어 많은 업데이트인 1,000번의 업데이트를 하면,

[3:08:00] 농담이 더 좋아지고 펠리컨에 대한 진짜 명작을 얻을 것이라고 기대할 수 있지만, 정확히 그렇게 되지 않습니다. 처음 몇 백 단계에서 펠리컨에 대한 농담이 아마 약간 개선되다가, 실제로 극적으로 절벽에서 떨어지고 매우 말도 안 되는 결과를 얻기 시작합니다.

[3:08:30] 예를 들어 펠리컨에 대한 최고의 농담이 "the the the"가 되기 시작합니다. 이것은 말이 안 됩니다. 보면 왜 이것이 최고의 농담이어야 할까요? 하지만 "the the the"를 보상 모델에 넣으면 0의 점수를 예상하지만, 실제로 보상 모델은 이것을 농담으로 좋아합니다.

[3:09:00] 1.0의 점수, 최고의 농담이라고 말합니다. 말이 안 되죠. 하지만 이 모델들은 인간의 시뮬레이션일 뿐이고, 거대한 신경망이고, 말도 안 되는 결과를 주는 입력 공간의 부분으로 들어가는 입력을 찾을 수 있습니다. 이 예시들이 적대적 예시라고 불리는 것이고,

[3:09:30] 너무 깊이 들어가지는 않겠지만, 모델에 대한 적대적 입력입니다. 모델의 틈새 사이로 들어가서 맨 위에서 말도 안 되는 결과를 주는 특정한 작은 입력입니다. 이렇게 상상할 수 있습니다. "the the the"는 분명히 1점이 아니고,

[3:10:00] 분명히 낮은 점수이니, "the the the"를 데이터셋에 추가하고 매우 나쁜, 5점의 순서를 주자. 실제로 모델은 "the the the"가 매우 낮은 점수여야 한다는 것을 배우고 0점을 줄 것입니다. 문제는 모델에 숨어 있는 말도 안 되는 적대적 예시가

[3:10:30] 기본적으로 무한하다는 것입니다. 이 과정을 여러 번 반복하고 말도 안 되는 것을 보상 모델에 계속 추가하고 매우 낮은 점수를 주면, 게임에서 절대 이길 수 없습니다. 많은 라운드를 할 수 있고, 강화학습을 충분히 오래 실행하면 항상 모델을 게임하는 방법을 찾습니다.

[3:11:00] 적대적 예시를 발견하고, 말도 안 되는 결과로 정말 높은 점수를 얻습니다. 근본적으로 점수 함수가 거대한 신경망이고, RL은 그것을 속이는 방법을 찾는 데 매우 뛰어나기 때문입니다. 결론적으로 항상 RLHF를 몇 백 번의 업데이트 정도 실행하고, 모델이 더 좋아지다가, 자르고 끝내고 출시합니다.

[3:11:30] 이 보상 모델에 대해 너무 많이 실행할 수 없습니다. 최적화가 그것을 게임하기 시작하고, 기본적으로 자르고 출시합니다. 보상 모델을 개선할 수 있지만, 어느 시점에서 결국 이런 상황에 도달합니다.

[3:12:00] RLHF에 대해 제가 보통 말하는 것은 RLHF는 RL이 아니라는 것입니다. 물론 RLHF는 RL이지만, 마법 같은 의미에서 RL이 아닙니다. 무한정 실행할 수 있는 RL이 아닙니다. 정답을 얻는 이런 종류의 문제에서는 이것을 쉽게 게임할 수 없습니다.

[3:12:30] 정답을 얻었거나 얻지 못했거나 둘 중 하나이고, 점수 함수가 훨씬 더 간단합니다. 박스 영역을 보고 결과가 맞는지 보는 것입니다. 이 함수들을 게임하기 매우 어렵지만, 보상 모델을 게임하는 것은 가능합니다.

[3:13:00] 이 검증 가능한 영역에서는 RL을 무한정 실행할 수 있습니다. 수만, 수십만 단계를 실행하고 우리가 생각하지도 못할 모든 종류의 미친 전략을 발견할 수 있습니다. 바둑 게임에서 게임의 승패를 게임하는 방법은 없습니다.

[3:13:30] 완벽한 시뮬레이터가 있고, 모든 돌이 어디에 놓여 있는지 알고, 누가 이겼는지 계산할 수 있습니다. 그것을 게임하는 방법이 없으니 RL을 무한정 할 수 있고, 결국 이세돌도 이길 수 있습니다. 하지만 게임할 수 있는 이런 모델들은 이 과정을 무한정 반복할 수 없습니다.

[3:14:00] RLHF를 진짜 RL이 아닌 것으로 봅니다. 보상 함수가 게임할 수 있기 때문입니다. 약간의 미세조정, 약간의 개선이지만, 더 많은 연산을 넣고 더 오래 실행하면 훨씬 더 좋고 마법 같은 결과를 얻을 수 있는 것이 근본적으로 올바르게 설정되어 있지 않습니다.

[3:14:30] 마법이 없다는 의미에서 RL이 아닙니다. 모델을 미세조정하고 더 나은 성능을 얻을 수 있고, 실제로 ChatGPT로 돌아가면 GPT-4o 모델은 RLHF를 거쳤습니다. 효과가 있기 때문입니다. 하지만 같은 의미의 RL은 아닙니다. RLHF는 모델을 약간 개선하는 약간의 미세조정입니다.

---

## 20. 인간 피드백 기반 강화학습 (RLHF)

**요약**: 인간 피드백 기반 강화학습(RLHF)을 설명합니다. 창작 글쓰기처럼 자동 검증이 어려운 영역에서는 인간이 여러 응답 중 더 좋은 것을 선택하고, 이를 학습시킨 보상 모델을 사용합니다. RLHF는 ChatGPT 등 현대 LLM의 핵심 기술입니다.

[3:15:00] 이것이 제가 다루고 싶었던 대부분의 기술적 내용입니다. 이 모델들을 훈련하는 세 가지 주요 단계와 패러다임을 살펴봤습니다. 사전학습, 지도학습 미세조정, 강화학습. 그리고 그것들이 아이들을 가르치는 데 이미 사용하는 과정과 대략적으로 대응한다는 것을 보여드렸습니다.

[3:15:30] 특히 사전학습은 설명을 읽는 기본적인 지식 습득이고, 지도학습 미세조정은 많은 예제 풀이를 보고 전문가를 모방하는 과정이고, 연습 문제입니다. 유일한 차이점은 이제 LLM과 AI를 위해 모든 인간 지식 분야에 걸쳐 효과적으로 교과서를 작성해야 한다는 것입니다.

[3:16:00] 또한 그들이 실제로 잘 작동하기를 원하는 모든 경우, 코드와 수학, 기본적으로 모든 다른 분야에서요. 그래서 그들을 위해 교과서를 작성하고, 고수준에서 제시한 모든 알고리즘을 정제하고, 이 모델들을 대규모로 효율적으로 훈련하는 데 정말 잘 실행하는 과정에 있습니다.

[3:16:30] 특히 너무 많이 다루지 않았지만, 이것들은 수만 또는 수십만 개의 GPU에서 실행해야 하는 매우 크고 복잡한 분산 작업입니다. 이것에 들어가는 엔지니어링은 정말로 그 규모에서 컴퓨터로 가능한 것의 최첨단입니다.

[3:17:00] 그 측면을 너무 많이 다루지 않았지만, 이것은 매우 심각하고 궁극적으로 이 모든 매우 간단한 알고리즘의 기반이 됩니다. 또한 이 모델들의 심리 이론에 대해 약간 이야기했고, 가져가셨으면 하는 것은 이 모델들이 정말 좋지만

[3:17:30] 작업을 위한 도구로서 매우 유용하다는 것입니다. 완전히 신뢰해서는 안 됩니다. 환각에 대한 완화책이 있지만 모델은 완벽하지 않고 여전히 환각을 합니다. 시간이 지나면서 더 좋아졌고 계속 더 좋아질 것이지만, 환각할 수 있습니다.

[3:18:00] 그 외에도 제가 LLM 능력의 스위스 치즈 모델이라고 부르는 것을 다뤘습니다. 모델들이 많은 다른 분야에서 믿을 수 없을 만큼 좋지만, 어떤 독특한 경우에는 거의 무작위로 실패합니다. 예를 들어, 9.11과 9.9 중 어느 것이 더 큰가? 모델은 모르지만

[3:18:30] 동시에 돌아서서 올림피아드 문제를 풀 수 있습니다. 이것이 스위스 치즈의 구멍이고, 많이 있습니다. 그것에 걸려 넘어지고 싶지 않습니다. 이 모델들을 오류 없는 것으로 취급하지 마세요. 그들의 작업을 확인하고, 도구로 사용하고, 영감을 위해 사용하고, 초안을 위해 사용하지만,

[3:19:00] 그들과 도구로서 작업하고 궁극적으로 작업의 결과물에 대해 책임을 지세요. 이것이 대략 말하고 싶었던 것입니다. 이것이 그들이 훈련되는 방법이고, 이것이 그들의 정체입니다. 이제 이 모델들의 미래 능력이 무엇인지, 아마도 파이프라인에서 오고 있는 것이 무엇인지, 그리고 이 모델들을 어디서 찾을 수 있는지 살펴봅시다.

[3:19:30] 파이프라인에서 기대할 수 있는 몇 가지에 대한 요점이 있습니다. 첫째로 알 수 있는 것은 모델들이 매우 빠르게 멀티모달이 될 것입니다. 위에서 이야기한 모든 것은 텍스트에 관한 것이었지만, 곧 텍스트뿐만 아니라 오디오(듣고 말할 수 있음)와 이미지(보고 그릴 수 있음)를 기본적으로 매우 쉽게 처리할 수 있는 LLM을 갖게 될 것입니다.

---

## 21. 앞으로의 전망 (Preview of Things to Come)

**요약**: LLM의 미래 발전 방향을 전망합니다. 멀티모달(이미지, 오디오, 비디오), 에이전트 시스템, 더 긴 컨텍스트 윈도우, 로봇공학 통합 등이 활발히 연구되고 있습니다. AI 시스템이 점점 더 자율적으로 작동하는 방향으로 발전하고 있습니다.

[3:20:00] 이 모든 것의 시작을 이미 보고 있지만, 이것은 모두 언어 모델 내부에서 기본적으로 이루어질 것이고, 자연스러운 대화를 가능하게 할 것입니다. 대략적으로 이것이 위에서 다룬 모든 것과 실제로 다르지 않은 이유는 기본적으로 오디오와 이미지를 토큰화하고

[3:20:30] 위에서 이야기한 것과 정확히 같은 접근법을 적용할 수 있기 때문입니다. 근본적인 변화가 아니라, 토큰을 몇 개 추가해야 하는 것입니다. 예를 들어 오디오를 토큰화하기 위해 오디오 신호의 스펙트로그램 조각을 보고 토큰화하고, 갑자기 오디오를 나타내는 토큰을 더 추가하고

[3:21:00] 컨텍스트 윈도우에 넣고 위와 같이 훈련합니다. 이미지도 마찬가지로 패치를 사용할 수 있고, 패치를 별도로 토큰화합니다. 이미지가 무엇인가? 이미지는 그냥 토큰의 시퀀스입니다. 이것은 실제로 작동하고 이 방향으로 많은 초기 작업이 있습니다.

[3:21:30] 오디오, 이미지, 텍스트를 나타내는 토큰 스트림을 만들고 그것들을 섞어서 단일 모델에서 동시에 처리할 수 있습니다. 이것이 멀티모달리티의 한 예입니다. 둘째로 사람들이 매우 관심을 가지는 것은 현재 대부분의 작업이 개별 작업을 모델에 은쟁반에 담아

[3:22:00] 주는 것입니다. "이 작업을 해주세요"라고 하면 모델이 이 작은 작업을 합니다. 하지만 작업을 일관되게 조직하여 직무를 수행하는 것은 여전히 우리에게 달려 있고, 모델들은 아직 긴 시간에 걸쳐 일관된 오류 수정 방식으로 이것을 할 수 있는 능력에 도달하지 않았습니다.

[3:22:30] 더 긴 직무를 수행하기 위해 작업을 완전히 연결할 수 없지만, 점점 가까워지고 있고 시간이 지나면서 개선되고 있습니다. 아마 여기서 일어날 일은 시간에 걸쳐 작업을 수행하는 에이전트라고 불리는 것을 보기 시작할 것입니다. 그들을 감독하고 작업을 지켜보고 가끔 진행 상황을 보고받습니다.

[3:23:00] 몇 초의 응답이 아니라 수십 초 또는 심지어 몇 분 또는 몇 시간이 걸리는 더 오래 실행되는 에이전트 작업을 볼 것입니다. 하지만 위에서 이야기했듯이 이 모델들은 완벽하지 않으므로, 이 모든 것은 감독이 필요합니다. 예를 들어 공장에서 사람들은 자동화를 위한 인간 대 로봇 비율에 대해 이야기합니다.

[3:23:30] 디지털 공간에서도 비슷한 것을 볼 것 같습니다. 인간 대 에이전트 비율에 대해 이야기하게 되고, 디지털 영역에서 인간이 에이전트 작업의 감독자가 될 것입니다. 다음으로 모든 것이 훨씬 더 만연하고 보이지 않게 될 것입니다. 도구에 통합되고 어디에나 있게 됩니다.

[3:24:00] 그리고 컴퓨터 사용이 있는데, 지금 이 모델들은 당신을 대신해서 행동을 취할 수 없지만, 이것은 별개의 요점입니다. ChatGPT가 오퍼레이터를 출시한 것을 보셨다면, 그것이 초기 예시인데, 실제로 모델에게 키보드와 마우스 동작을 당신을 대신해서 수행하도록 제어권을 넘겨줄 수 있습니다.

[3:24:30] 매우 흥미롭다고 생각합니다. 마지막 요점은 이 분야에서 아직 해야 할 연구가 많다는 일반적인 코멘트입니다. 한 예시는 테스트 시간 훈련 같은 것입니다. 위에서 이야기하고 다룬 모든 것은 두 가지 주요 단계가 있다는 것을 기억하세요. 첫째는 훈련 단계로, 작업을 잘 수행하도록

[3:25:00] 모델의 매개변수를 조정합니다. 매개변수를 얻으면 고정하고 추론을 위해 모델을 배포합니다. 그때부터 모델은 고정되고, 더 이상 변하지 않고, 테스트 시간에 하는 모든 것에서 배우지 않습니다. 고정된 매개변수 수이고, 변하는 유일한 것은 컨텍스트 윈도우 안의 토큰입니다.

[3:25:30] 모델이 테스트 시간에 접근할 수 있는 유일한 유형의 학습 또는 테스트 시간 학습은 동적으로 조정 가능한 컨텍스트 윈도우의 인컨텍스트 학습입니다. 하지만 이것은 실제로 무엇을 하느냐에 따라 실제로 배울 수 있는 인간과 여전히 다르다고 생각합니다.

[3:26:00] 특히 잘 때, 뇌가 매개변수를 업데이트하거나 그런 것 같습니다. 현재 이 모델들과 도구에는 그것에 대한 동등물이 없습니다. 탐구해야 할 더 복잡한 아이디어가 많다고 생각하고, 특히 컨텍스트 윈도우가 유한하고 귀중한 자원이기 때문에 필요할 것입니다.

[3:26:30] 특히 매우 오래 실행되는 멀티모달 작업을 다루기 시작하고 비디오를 넣으면, 이 토큰 윈도우는 매우 크게 자라기 시작할 것입니다. 수천이나 수십만이 아니라 그것을 훨씬 넘어서요. 지금 사용할 수 있는 유일한 트릭은 컨텍스트 윈도우를 더 길게 만드는 것이지만,

[3:27:00] 그 접근법 자체는 시간이 지나면서 멀티모달인 실제로 오래 실행되는 작업으로 확장되지 않을 것입니다. 새로운 아이디어가 필요하다고 생각합니다. 매우 긴 컨텍스트가 필요한 이런 작업들의 일부 경우에서요. 이것들이 파이프라인에서 기대할 수 있는 몇 가지 예시입니다.

---

## 22. LLM 동향 추적하기 (Keeping Track of LLMs)

**요약**: 최신 LLM 동향을 추적하는 방법을 안내합니다. LMSys Chatbot Arena의 엘로 랭킹을 통해 다양한 모델의 성능을 비교할 수 있으며, 현재 최고 성능 모델들(GPT-4o, Claude, Gemini 등)의 순위를 확인할 수 있습니다.

[3:27:30] 이제 실제로 이 진행 상황을 어디서 추적하고 분야에서 일어나는 최신 및 최고의 것을 어떻게 최신 상태로 유지할 수 있는지 봅시다. 제가 최신 상태를 유지하기 위해 일관되게 사용한 세 가지 리소스는 첫째로 LM Arena입니다. LM Arena를 보여드리겠습니다. 이것은 기본적으로 LLM 리더보드이고

[3:28:00] 모든 최고 모델의 순위를 매깁니다. 순위는 인간 비교에 기반합니다. 인간이 이 모델들에 프롬프트를 주고 어떤 것이 더 나은 답을 주는지 판단합니다. 어떤 모델이 어떤 것인지 모르고, 어떤 모델이 더 나은 답인지만 봅니다. 순위를 계산하고 결과를 얻습니다. 여기서 볼 수 있는 것은 이 모델들을 만드는 다른 조직들입니다.

[3:28:30] 예를 들어 구글 제미니. 이것들 중 하나를 클릭하면 그 모델이 호스팅되는 곳으로 갑니다. 구글이 현재 1위이고 OpenAI가 바로 뒤에 있습니다. 딥시크가 3위입니다. 이것이 큰 일인 이유는 마지막 열의 라이선스를 보면, 딥시크는 MIT 라이선스 모델입니다.

[3:29:00] 오픈 웨이트이고, 누구나 이 웨이트를 사용할 수 있고, 누구나 다운로드할 수 있고, 누구나 자체 버전의 딥시크를 호스팅할 수 있고, 원하는 방식으로 사용할 수 있습니다. 접근할 수 없는 독점 모델이 아니라, 기본적으로 오픈 웨이트 릴리스입니다. 이렇게 강한 모델이 오픈 웨이트로 출시된 것은 전례가 없습니다.

[3:29:30] 팀에서 꽤 멋집니다. 다음으로 구글과 OpenAI의 몇 가지 더 많은 모델이 있고, 계속 스크롤하면 다른 익숙한 이름들이 보이기 시작합니다. xAI, 14위의 소넷과 앤트로픽. 그리고 여기 메타의 라마입니다.

[3:30:00] 라마는 딥시크와 마찬가지로 오픈 웨이트 모델이지만, 위가 아니라 여기 있습니다. 이 리더보드가 오랫동안 정말 좋았다고 말하겠습니다. 지난 몇 달 동안 약간 게임되고 있다고 생각하고, 예전만큼 신뢰하지 않습니다.

[3:30:30] 경험적으로 예를 들어 많은 사람들이 앤트로픽의 소넷을 사용하고 정말 좋은 모델인데, 14위에 있습니다. 반대로 제미니를 사용하는 사람은 많지 않은데 정말 높은 순위입니다. 첫 번째 패스로 사용하되, 몇 가지 모델을 작업에 시도해보고 어떤 것이 더 나은지 보세요.

[3:31:00] 두 번째로 가리키고 싶은 것은 AI 뉴스 뉴스레터입니다. AI 뉴스는 창의적으로 이름 지어지지 않았지만 매우 좋은 뉴스레터입니다. swyx와 친구들이 만들었고, 유지해 주셔서 감사합니다. 매우 포괄적이기 때문에 저에게 매우 도움이 되었습니다.

[3:31:30] 아카이브에 가면 거의 이틀에 한 번씩 생산되고, 매우 포괄적입니다. 일부는 사람이 작성하고 큐레이션하지만 많은 부분이 LLM으로 자동 구성됩니다. 매우 포괄적이고, 이것을 읽으면 아마 중요한 것을 놓치지 않을 것입니다. 물론 너무 길어서 다 읽지 않겠지만,

[3:32:00] 맨 위의 요약이 꽤 좋고 약간의 인간 감독이 있다고 생각합니다. 저에게 매우 도움이 되었습니다. 마지막으로 가리키고 싶은 것은 X와 트위터입니다. 많은 AI가 X에서 일어납니다. 좋아하고 신뢰하는 사람들을 팔로우하고

[3:32:30] 모든 최신 및 최고의 것을 X에서도 얻으세요. 이것들이 시간이 지나면서 저에게 효과가 있었던 주요 장소입니다.

---

## 23. LLM을 찾을 수 있는 곳 (Where to Find LLMs)

**요약**: LLM을 사용할 수 있는 주요 플랫폼들을 소개합니다. ChatGPT(OpenAI), Claude(Anthropic), Gemini(Google) 등 상용 서비스와 OpenRouter 같은 통합 API 서비스, 그리고 로컬에서 실행할 수 있는 Ollama 등을 소개합니다.

[3:33:00] 마지막으로 모델을 어디서 찾을 수 있고 어디서 사용할 수 있는지 몇 마디 하겠습니다. 첫째로, 가장 큰 독점 모델의 경우 그 LLM 제공자의 웹사이트에 가야 합니다. 예를 들어 OpenAI의 경우 chat.openai.com입니다. 이제 작동하는 것 같습니다.

[3:33:30] 제미니의 경우 gemini.google.com이나 AI Studio입니다. 그들이 두 개를 가지고 있는데, 완전히 이해하지 못하는 이유로요. 아무도 이해하지 못합니다. 딥시크 같은 오픈 웨이트 모델의 경우 어떤 종류의 LLM 추론 제공자에 가야 합니다.

[3:34:00] 제가 좋아하는 것은 together.ai입니다. together.ai의 플레이그라운드에 가면 많은 다른 모델을 선택할 수 있고, 이것들은 모두 다른 유형의 오픈 모델이고, 여기서 대화할 수 있습니다. 기본 모델을 사용하고 싶다면,

[3:34:30] 이 추론 제공자에서도 기본 모델을 찾는 것이 흔하지 않습니다. 모두 어시스턴트와 채팅을 대상으로 합니다. 여기서도 기본 모델을 볼 수 없었습니다. 기본 모델의 경우 보통 hyperbolic에 갑니다. 그들이 llama 3.1 base를 서비스하고, 그 모델을 좋아합니다.

[3:35:00] 여기서 대화할 수 있습니다. 제가 아는 한 이것이 기본 모델을 위한 좋은 곳이고, 더 많은 사람들이 기본 모델을 호스팅했으면 합니다. 일부 경우에 작업하기에 유용하고 흥미롭기 때문입니다. 마지막으로 일부 더 작은 모델을 가져와서

[3:35:30] 로컬에서 실행할 수도 있습니다. 예를 들어 딥시크의 가장 큰 모델은 맥북에서 로컬로 실행할 수 없지만, 증류된 딥시크 모델의 더 작은 버전이 있습니다. 그리고 이 모델들을 더 낮은 정밀도에서 실행할 수도 있습니다.

[3:36:00] 딥시크의 네이티브 정밀도인 fp8이나 llama의 bf16이 아니라 훨씬 더 낮게요. 그 세부 사항을 완전히 이해하지 못해도 괜찮지만, 증류된 더 작은 버전을 훨씬 더 낮은 정밀도에서 실행할 수 있고, 컴퓨터에 맞출 수 있습니다.

[3:36:30] 실제로 노트북에서 꽤 괜찮은 모델을 실행할 수 있습니다. 제가 보통 가는 곳은 LM Studio인데, 기본적으로 얻을 수 있는 앱입니다. 정말 못생겼고, 기본적으로 유용하지 않은 이 모든 모델을 보여주는 것이 마음에 들지 않습니다.

[3:37:00] 모두가 딥시크를 실행하고 싶은데, 왜 500가지 다른 유형의 모델을 주는지 모르겠습니다. 검색하기 정말 복잡하고, 다른 증류와 다른 정밀도를 선택해야 하고, 모두 정말 혼란스럽습니다. 하지만 실제로 어떻게 작동하는지 이해하면, 그것은 별개의 비디오이지만,

[3:37:30] 모델을 로드할 수 있습니다. 여기서 llama 3 2 instruct 10억을 로드했고, 대화할 수 있습니다. 펠리컨 농담을 요청하고 다른 것을 요청할 수 있고, 등등. 여기서 일어나는 모든 것은 컴퓨터에서 로컬로 일어납니다.

[3:38:00] 실제로 다른 곳에 가는 것이 아니고, 맥북 프로의 GPU에서 실행되고 있습니다. 매우 좋고, 끝나면 모델을 이젝트하면 RAM이 해제됩니다. LM Studio가 아마 제가 좋아하는 것인데, UI/UX 문제가 많고 거의 전문가를 대상으로 한다고 생각하지만,

[3:38:30] 유튜브에서 비디오를 보면 이 인터페이스를 사용하는 방법을 알아낼 수 있다고 생각합니다. 이것들이 모델을 어디서 찾을 수 있는지에 대한 몇 마디입니다.

---

## 24. 전체 요약 (Grand Summary)

**요약**: 전체 강의 내용을 요약합니다. LLM은 인터넷 데이터로 사전학습된 후, 대화 데이터로 미세조정되고, 강화학습으로 개선됩니다. 이들은 강력하지만 완벽하지 않은 도구이며, 신뢰하되 검증하는 자세로 활용해야 합니다.

[3:39:00] 이제 우리가 시작한 곳으로 돌아가 봅시다. 질문은 chat.openai.com에 가서 어떤 쿼리를 입력하고 가기를 누르면 정확히 무엇이 일어나는가였습니다. 무엇을 보고 있고, 무엇과 대화하고 있고, 이것이 어떻게 작동하는지.

[3:39:30] 이 비디오가 이 모델들이 어떻게 훈련되고 무엇이 돌아오는지에 대한 내부 세부 사항에 대해 약간의 이해를 주었기를 바랍니다. 특히 이제 우리는 쿼리가 먼저 토큰으로 잘린다는 것을 압니다. tiktokenizer로 가면 사용자 쿼리가 있는 형식에서

[3:40:00] 기본적으로 쿼리를 거기에 넣습니다. 쿼리가 여기서 논의한 대화 프로토콜 형식에 삽입되는데, 이것이 대화 객체를 유지하는 방식입니다. 거기에 삽입되고, 이 모든 것이 결국 토큰 시퀀스, 내부적으로 1차원 토큰 시퀀스가 됩니다.

[3:40:30] ChatGPT가 이 토큰 시퀀스를 보고, 가기를 누르면 기본적으로 이 목록에 토큰을 계속 추가합니다. 시퀀스를 계속합니다. 토큰 자동완성처럼 작동합니다. 특히 이 응답을 줬습니다.

[3:41:00] 기본적으로 여기에 넣으면 계속한 토큰을 볼 수 있습니다. 대략적으로요. 이제 질문은 왜 이것들이 모델이 응답한 토큰인가, 이 토큰들이 무엇이고, 어디서 오고, 무엇과 대화하고, 이 시스템을 어떻게 프로그래밍하는가입니다.

[3:41:30] 그래서 기어를 바꾸고 내부 부분에 대해 이야기했습니다. 이 과정의 첫 번째 단계, 세 단계가 있는데, 사전학습 단계입니다. 이것은 근본적으로 인터넷에서 이 신경망의 매개변수로 지식을 습득하는 것과 관계가 있습니다.

[3:42:00] 신경망이 인터넷에서 많은 지식을 내재화하지만, 성격이 정말로 나오는 곳은 지도학습 미세조정 과정입니다. 여기서 기본적으로 OpenAI 같은 회사가 매우 다양한 주제에 걸쳐 100만 개의 대화 같은 대규모 대화 데이터셋을 큐레이션합니다.

[3:42:30] 인간과 어시스턴트 사이의 대화이고, 이 전체 과정에서 많은 합성 데이터 생성과 많은 LLM 도움 등이 사용되지만, 근본적으로 이것은 많은 인간이 관여된 인간 데이터 큐레이션 작업입니다. 특히 이 인간들은 OpenAI에 고용된 데이터 라벨러로

[3:43:00] 라벨링 지침을 배우고 임의의 프롬프트에 대해 이상적인 어시스턴트 응답을 만드는 것이 그들의 작업입니다. 예시로 신경망에 프롬프트에 어떻게 응답하는지 가르칩니다. 여기서 돌아온 것을 어떻게 생각해야 할까요? 이것이 무엇일까요?

[3:43:30] 제가 생각하기에 올바른 방법은 이것이 OpenAI의 데이터 라벨러의 신경망 시뮬레이션이라는 것입니다. 마치 이 쿼리를 OpenAI의 데이터 라벨러에게 주고, 이 데이터 라벨러가 먼저 OpenAI의 모든 라벨링 지침을 읽고, 2시간 동안

[3:44:00] 이 쿼리에 대한 이상적인 어시스턴트 응답을 작성하고 저
<!-- FULL_TRANSLATION_END -->
