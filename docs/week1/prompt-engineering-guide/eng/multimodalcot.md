---
title: "Multimodal CoT"
source_url: "https://www.promptingguide.ai/techniques/multimodalcot"
source_type: web
author: "DAIR.AI"
fetch_date: "2026-01-08"
---

# Multimodal Chain-of-Thought Prompting

## Overview

Zhang et al. (2023) introduced a multimodal CoT approach that extends traditional chain-of-thought reasoning by incorporating both textual and visual information. Rather than relying solely on language, this framework processes multiple modalities together.

## Key Approach

The methodology operates in two distinct phases:

1. **Rationale Generation**: Creates explanatory reasoning based on combined text and image data
2. **Answer Inference**: Uses the generated rationales to produce final answers

## Performance

The research demonstrated that "the multimodal CoT model (1B) outperforms GPT-3.5 on the ScienceQA benchmark," indicating significant efficiency gains compared to larger language-only models.

## Framework Structure

The approach leverages visual and linguistic information simultaneously, enabling the model to reason about content that requires understanding both modalities. This contrasts with traditional CoT methods that focus exclusively on textual reasoning paths.

## Additional Resources

The page references a related paper: "Language Is Not All You Need: Aligning Perception with Language Models" (February 2023), which explores similar intersections between visual perception and language model capabilities.

The guide also offers associated courses on prompt engineering fundamentals and AI agent development for practitioners seeking deeper understanding.
