---
title: "10. Post-Training Data (Conversations)"
titleKr: "10. 후속학습 데이터 - 대화"
chapter: 10
timestamp: "1:01:06"
sourceUrl: "https://www.youtube.com/watch?v=7xTGNNLPyMI&t=3666s"
translatedAt: "2026-01-10"
---

# 10. 후속학습 데이터 - 대화

[영상 바로가기 (1:01:06)](https://www.youtube.com/watch?v=7xTGNNLPyMI&t=3666s)

## 요약

지도학습 미세조정(SFT)의 핵심은 대화 데이터셋입니다. 인간 라벨러가 수십만 개의 고품질 대화를 작성하고, 회사가 제공한 라벨링 지침에 따라 이상적인 어시스턴트 응답을 만듭니다. 모델은 이 대화들의 통계적 패턴을 학습하여 "도움이 되고, 진실되고, 해롭지 않은" 어시스턴트의 페르소나를 갖게 됩니다. ChatGPT와 대화할 때 받는 응답은 이 인간 라벨러들의 통계적 모방이라고 볼 수 있습니다.

**핵심 개념:**
- **SFT 데이터셋**: 인간 라벨러가 작성한 수십만 개의 대화 예시 모음
- **라벨링 지침**: 이상적인 어시스턴트 응답을 정의하는 회사의 가이드라인 (도움, 진실성, 무해함)
- **대화 토큰화**: 대화를 특수 토큰(im_start, im_end 등)을 사용해 1차원 토큰 시퀀스로 변환
- **합성 데이터**: 현대 스택에서는 LLM이 데이터셋 생성을 돕고 인간이 편집하는 방식으로 발전
- **통계적 모방**: ChatGPT 응답은 학습 데이터의 인간 라벨러를 통계적으로 시뮬레이션한 결과

---

## 전체 번역

**요약**: 후속학습 데이터인 대화 데이터셋에 대해 설명합니다. 인간 라벨러들이 수만 개의 고품질 대화 예시를 작성하고, 이를 모델에 학습시킵니다. 이 과정을 지도학습 미세조정(SFT)이라 하며, 모델이 어시스턴트처럼 행동하도록 만듭니다.

[1:01:30] 더하기 대신 곱하기라면 어떻게 되냐고 물으면 어시스턴트는 이렇게 응답할 수 있습니다. 이 예시에서는 어시스턴트가 어떤 종류의 성격을 가질 수도 있다는 것을 보여줍니다. 친절한 느낌이죠. 세 번째 예시에서는 인간이 우리가 도와주고 싶지 않은 것을 요청할 때, 거부라는 것을 생성할 수 있다는 것을 보여줍니다. "도와드릴 수 없습니다"라고 말할 수 있죠. 다시 말해, 우리가 지금 하고 싶은 것은 시스템이 인간과 어떻게 상호작용해야 하는지 생각하고, 이러한 대화에서 어시스턴트와 그 행동을 프로그래밍하는 것입니다.

[1:02:00] 이것이 신경망이기 때문에 우리는 이것들을 코드로 명시적으로 프로그래밍하지 않을 것입니다. 그런 방식으로 어시스턴트를 프로그래밍할 수 없습니다. 신경망이기 때문에 모든 것은 데이터셋에서의 신경망 학습을 통해 이루어집니다. 그래서 우리는 대화 데이터셋을 만들어서 암묵적으로 어시스턴트를 프로그래밍할 것입니다. 이것은 데이터셋에 있는 세 개의 독립적인 대화 예시입니다. 실제 데이터셋에서 제가 보여드릴 예시들은

[1:02:30] 훨씬 더 클 것입니다. 수십만 개의 대화가 있을 수 있고, 여러 턴으로 이루어지고, 매우 길 수 있으며, 다양한 주제를 다룹니다. 하지만 여기서는 세 가지 예시만 보여드립니다. 기본적으로 이것이 작동하는 방식은 어시스턴트가 예시에 의해 프로그래밍된다는 것입니다. 그리고 이 데이터는 어디서 오는 걸까요? 2 곱하기 2는 4, 2 더하기 2와 같다, 등등 어디서 오는 걸까요? 이것은 인간 라벨러에게서 옵니다. 우리는 인간 라벨러에게 대화 맥락을 제공하고

[1:03:00] 이 상황에서 이상적인 어시스턴트 응답을 작성하도록 요청합니다. 인간이 어떤 상황에서든 어시스턴트의 이상적인 응답을 작성합니다. 그런 다음 모델이 이것을 학습하고 그런 종류의 응답을 모방하도록 합니다. 그래서 이것이 작동하는 방식은, 사전학습 단계에서 만든 베이스 모델을 가져옵니다. 이 베이스 모델은 인터넷 문서에서 학습되었습니다. 이제 그 인터넷 문서 데이터셋을 버리고 새로운 데이터셋으로

[1:03:30] 대체할 것입니다. 그것은 대화 데이터셋이 될 것입니다. 이 새로운 대화 데이터셋에서 모델을 계속 학습시킵니다. 그러면 모델이 매우 빠르게 조정되어 어시스턴트가 인간 쿼리에 어떻게 응답하는지의 통계를 배우게 됩니다. 나중에 추론 시에 어시스턴트를 프라이밍하고 응답을 얻을 수 있으며, 그 상황에서 인간 라벨러가 했을 것을 모방하게 됩니다.

[1:04:00] 이해가 되셨기를 바랍니다. 예시를 보면서 조금 더 구체적으로 설명하겠습니다. 이 후속학습 단계에서 기본적으로 모델을 계속 학습시키지만, 사전학습 단계는 실제로 수천 대의 컴퓨터에서 약 3개월의 학습이 필요합니다. 후속학습 단계는 일반적으로 3시간 정도로 훨씬 짧습니다. 왜냐하면 여기서 수동으로 만드는 대화 데이터셋이 인터넷 텍스트 데이터셋보다 훨씬 작기 때문입니다.

[1:04:30] 그래서 이 학습은 매우 짧지만, 근본적으로 우리는 베이스 모델을 가져와서 정확히 같은 알고리즘, 정확히 같은 모든 것을 사용해 계속 학습시킵니다. 단지 데이터셋을 대화로 바꾸는 것뿐입니다. 그래서 질문은 이 대화들이 무엇이고, 어떻게 표현하며, 모델이 원시 텍스트 대신 대화를 보도록 어떻게 만드는가, 그리고 이런 종류의 학습의 결과가 무엇이고, 모델에 대해 이야기할 때 어떤 심리적 의미에서 무엇을 얻는가 입니다.

[1:05:00] 이제 그 질문들로 넘어가겠습니다. 대화의 토큰화에 대해 이야기하는 것부터 시작합시다. 이 모델들에서 모든 것은 토큰으로 변환되어야 합니다. 모든 것이 토큰 시퀀스에 관한 것이기 때문입니다. 그래서 대화를 토큰 시퀀스로 어떻게 변환하느냐가 문제입니다. 이를 위해 어떤 종류의 인코딩을 설계해야 합니다. 이것은 익숙하시다면, 꼭 그럴 필요는 없지만, 예를 들어 인터넷의 TCP/IP 패킷과 비슷합니다. 정보를 어떻게 표현하고 모든 것이 어떻게 구조화되는지에 대한 정확한 규칙과 프로토콜이 있습니다.

[1:05:30] 그래서 모든 종류의 데이터가 문서에 기록되어 모든 사람이 동의할 수 있는 방식으로 배치됩니다. LLM에서도 같은 일이 일어나고 있습니다. 어떤 종류의 데이터 구조가 필요하고, 대화 같은 데이터 구조가 토큰으로 인코딩되고 디코딩되는 방식에 대한 규칙이 필요합니다. 이제 이 대화를 토큰 공간에서 어떻게 재현하는지 보여드리겠습니다. Tiktokenizer에 가서 그 대화를 가져올 수 있고 이것이

[1:06:00] 언어 모델에서 어떻게 표현되는지입니다. 여기서 우리는 이 두 턴 대화에서 사용자와 어시스턴트를 번갈아 가며 반복합니다. 여기서 보이는 것은 보기 좋지 않지만 실제로는 비교적 간단합니다. 끝에서 토큰 시퀀스로 변환되는 방식은 약간 복잡하지만, 결국 사용자와 어시스턴트 사이의 이 대화는 49개의 토큰이 됩니다. 49개 토큰의 1차원 시퀀스이고, 이것이 그 토큰들입니다. 모든 다른 LLM들은

[1:06:30] 약간 다른 형식이나 프로토콜을 가지고 있고, 지금은 다소 무법지대입니다. 하지만 예를 들어 GPT-4o는 다음과 같은 방식으로 합니다. "im_start"라는 특수 토큰이 있고, 이것은 IM(imaginary monologue, 상상의 독백)의 시작을 줄인 것입니다. 그런 다음 누구의 턴인지 지정해야 합니다. 솔직히 왜 그렇게 불리는지 모르겠습니다. 예를 들어 토큰 4인 user를

[1:07:00] 지정합니다. 그런 다음 내부 독백 구분자가 있고, 정확한 질문, 즉 질문의 토큰들이 옵니다. 그런 다음 닫아야 합니다. im_end, 상상의 독백의 끝입니다. 기본적으로 "2 더하기 2는 뭐야?"라는 사용자의 질문은 이 토큰들의 토큰 시퀀스가 됩니다. 여기서 중요한 점은 im_start는 텍스트가 아니라는 것입니다. im_start는 추가되는 특수 토큰이고,

[1:07:30] 새로운 토큰입니다. 이 토큰은 지금까지 한 번도 학습되지 않았습니다. 후속학습 단계에서 만들어서 도입하는 새 토큰입니다. 그래서 im_sep, im_start 등의 특수 토큰이 도입되어 텍스트와 섞입니다. 모델이 "이것은 턴의 시작이고, 누구를 위한 것인가, 사용자를 위한 턴의 시작이고, 이것이 사용자가 말하는 것이고, 사용자가 끝나고, 새로운 턴의 시작이고,

[1:08:00] 어시스턴트의 턴이고, 어시스턴트가 무엇을 말하는가, 이것이 어시스턴트가 말하는 토큰들이다" 등을 배우도록 합니다. 그래서 이 대화는 토큰 시퀀스로 변환되고, 여기서의 구체적인 세부사항은 실제로 그렇게 중요하지 않습니다. 제가 구체적인 용어로 보여드리려는 것은 우리가 구조화된 객체로 생각하는 대화가 어떤 인코딩을 통해 1차원 토큰 시퀀스로 변환된다는 것입니다. 1차원 토큰 시퀀스이기 때문에

[1:08:30] 이전에 적용했던 모든 것을 적용할 수 있습니다. 이제 그냥 토큰 시퀀스이고 언어 모델을 학습시킬 수 있습니다. 이전처럼 시퀀스에서 다음 토큰을 예측하는 것입니다. 대화를 표현하고 학습할 수 있습니다. 그러면 테스트 시, 추론 중에는 어떻게 보일까요? 모델을 학습시켰다고 해봅시다. 이런 종류의 대화 데이터셋에서 학습시켰고, 이제 추론을 하고 싶습니다. ChatGPT에 있을 때 추론 중에는 어떻게 보일까요? ChatGPT에 가서

[1:09:00] 대화를 나눕니다. 이것이 작동하는 방식은 기본적으로, 이것이 이미 채워져 있다고 해봅시다. "2 더하기 2는 뭐야", "2 더하기 2는 4입니다". 이제 "곱하기라면 어떻게 돼?" im_end라고 입력합니다. OpenAI 같은 회사의 서버에서 기본적으로 일어나는 일은 im_start assistant im_sep을 넣고 여기서 끝냅니다. 이 컨텍스트를 구성하고 이제 모델에서 샘플링을 시작합니다. 이

[1:09:30] 단계에서 모델에 가서 "좋은 첫 번째 토큰이 뭐야? 좋은 두 번째 토큰이 뭐야? 좋은 세 번째 토큰이 뭐야?"라고 물어봅니다. 여기서 LLM이 인계받아 응답을 생성합니다. 예를 들어 이런 응답을 만들지만, 이것과 동일할 필요는 없습니다. 이런 종류의 대화가 데이터셋에 있었다면 이런 느낌이 날 것입니다. 대략 이것이 프로토콜이 작동하는 방식이지만, 이 프로토콜의 세부사항은 중요하지 않습니다.

[1:10:00] 다시 말하지만 제 목표는 모든 것이 결국 1차원 토큰 시퀀스가 된다는 것을 보여주는 것입니다. 그래서 이미 본 모든 것을 적용할 수 있지만, 이제 대화에서 학습하고 대화를 생성합니다. 자, 이제 이 데이터셋이 실제로 어떻게 생겼는지 살펴보겠습니다. 제가 보여드리고 싶은 첫 번째 논문과 이 방향의 첫 번째 노력은 2022년 OpenAI의 이 논문입니다. 이 논문은 InstructGPT라고 불렸고, 그들이 개발한 기술이었습니다. 이것이 OpenAI가

[1:10:30] 언어 모델을 가져와서 대화에서 파인튜닝하는 방법에 대해 이야기한 최초의 논문이었습니다. 이 논문에는 제가 살펴보고 싶은 여러 세부사항이 있습니다. 첫 번째로 가고 싶은 곳은 섹션 3.4입니다. 여기서 그들이 고용한 인간 계약자에 대해 이야기합니다. 이 경우 Upwork나 Scale AI를 통해 이 대화들을 구성하기 위해 고용했습니다. 이 대화들을 만드는 것이 직업인 인간 라벨러들이 관여하고, 이 라벨러들은 프롬프트를 만들고

[1:11:00] 이상적인 어시스턴트 응답도 완성하도록 요청받습니다. 이것이 사람들이 만든 종류의 프롬프트입니다. 인간 라벨러들이죠. "내 커리어에 대한 열정을 되찾을 수 있는 다섯 가지 아이디어를 나열해줘", "읽어야 할 SF 소설 상위 10개는 뭐야" 등 많은 다양한 유형의 프롬프트가 있습니다. "이 문장을 스페인어로 번역해줘" 등. 사람들이 만든 많은 것들이 있습니다. 먼저 프롬프트를 만들고, 그 프롬프트에 답하고

[1:11:30] 이상적인 어시스턴트 응답을 제공합니다. 그런데 이 프롬프트에 대해 작성해야 하는 이상적인 어시스턴트 응답이 무엇인지 어떻게 알까요? 조금 더 아래로 스크롤하면, 인간 라벨러에게 주어지는 라벨링 지침의 발췌가 있습니다. 언어 모델을 개발하는 회사, 예를 들어 OpenAI가 인간이 이상적인 응답을 만드는 방법에 대한 라벨링 지침을 작성합니다. 예를 들어 이런 종류의 라벨링 지침의 발췌입니다. 높은 수준에서 사람들에게 도움이 되고, 진실되고, 해롭지 않게 하라고 요청합니다.

[1:12:00] 여기서 더 보고 싶으시면 비디오를 멈추셔도 됩니다. 높은 수준에서 기본적으로 그냥 대답하고, 도움이 되려 하고, 진실되려 하고, 시스템이 나중에 ChatGPT에서 처리하기를 원하지 않는 질문에는 답하지 않습니다. 대략적으로 회사가 라벨링 지침을 만들고, 보통 이렇게 짧지 않고 수백 페이지가 되며, 사람들이 전문적으로 공부해야 하고, 그런 다음 그 라벨링

[1:12:30] 지침을 따라 이상적인 어시스턴트 응답을 작성합니다. 이 논문에서 설명된 대로 이것은 매우 인간 집약적인 프로세스입니다. InstructGPT의 데이터셋은 실제로 OpenAI에서 공개되지 않았지만, 이런 종류의 설정을 따르고 자체 데이터를 수집하려는 오픈소스 재현이 있습니다. 제가 익숙한 예로 예전 Open Assistant의 노력이 있습니다. 많은 예시 중 하나일 뿐이지만 예시를 보여드리고 싶습니다.

[1:13:00] 이것은 OpenAI가 인간 라벨러와 했던 것과 비슷하게 이런 대화를 만들도록 요청받은 인터넷의 사람들이었습니다. 여기 한 사람이 만든 항목이 있습니다. "경제학에서 독점(monopoly)이라는 용어의 관련성에 대한 짧은 소개를 써줄 수 있어? 예시를 사용해줘" 등. 그런 다음 같은 사람이나 다른 사람이 응답을 작성합니다. 여기 이것에 대한 어시스턴트 응답이 있습니다. 같은 사람이나 다른 사람이 이 이상적인 응답을 작성합니다. 그리고 이것은

[1:13:30] 대화가 어떻게 계속될 수 있는지의 예시입니다. "개에게 설명해봐" 그러면 조금 더 간단한 설명을 시도할 수 있습니다. 이것이 레이블이 되고, 이것에서 학습합니다. 학습 중에 일어나는 일은, 물론 테스트 시 추론 중에 모델이 접할 모든 가능한 질문을 완전히 커버할 수 없습니다. 사람들이 미래에 물어볼 모든 가능한 프롬프트를 커버할 수 없습니다.

[1:14:00] 하지만 이런 예시들의 데이터셋이 있다면, 학습 중에 모델이 이 도움이 되고, 진실되고, 해롭지 않은 어시스턴트의 페르소나를 취하기 시작합니다. 모든 것이 예시로 프로그래밍됩니다. 이것들은 모두 행동의 예시이고, 이런 예시 행동의 대화가 10만 개 정도 있고 학습하면, 모델이 통계적 패턴을 이해하기 시작하고 이

[1:14:30] 어시스턴트의 성격을 취합니다. 테스트 시에 정확히 같은 질문을 받으면, 답이 학습 세트에 있던 것과 정확히 같이 암송될 수 있습니다. 하지만 더 가능성이 높은 것은 모델이 비슷한 분위기의 뭔가를 할 것이고, 이것이 원하는 종류의 답이라는 것을 이해할 것입니다. 그래서 우리가 하는 것은 예시로 시스템을 프로그래밍하는 것이고, 시스템은 통계적으로 이

[1:15:00] 도움이 되고, 진실되고, 해롭지 않은 어시스턴트의 페르소나를 채택합니다. 이것은 회사가 만드는 라벨링 지침에 반영되어 있습니다. 지난 2-3년간 InstructGPT 논문 이후 최신 기술이 발전했다는 것을 보여드리고 싶습니다. 특히 더 이상 인간이 혼자서 모든 무거운 작업을 하는 것은 흔하지 않습니다. 이제 언어 모델이 있고, 이 언어 모델이 데이터셋과 대화를 만드는 것을 돕기 때문입니다. 사람들이 처음부터 응답을 문자 그대로 작성하는 것은 매우 드뭅니다.

[1:15:30] 기존 LLM을 사용해서 답을 만들고 편집하는 것이 훨씬 더 가능성이 높습니다. LLM이 이 후속학습 스택에 스며들기 시작한 여러 다른 방법이 있고, LLM은 기본적으로 이 대규모 대화 데이터셋을 만드는 데 광범위하게 사용됩니다. Ultra Chat은 더 현대적인 대화 데이터셋의 한 예시입니다. 매우 큰 부분이 합성이지만 인간 개입이 있다고 생각합니다.

[1:16:00] 잘못 알 수도 있지만 보통 약간의 인간 개입이 있지만 엄청난 양의 합성 도움이 있습니다. 이것은 모두 다양한 방식으로 구성되고, Ultra Chat은 현재 존재하는 많은 SFT 데이터셋 중 하나의 예시일 뿐입니다. 보여드리고 싶은 유일한 것은 이 데이터셋이 이제 수백만 개의 대화를 가지고 있고, 이 대화들은 대부분 합성이지만 어느 정도 인간에 의해 편집되었을 것이고, 매우 다양한

[1:16:30] 영역 등을 포괄한다는 것입니다. 이것은 이제 상당히 광범위한 결과물이고, 이런 SFT 혼합물이라고 불리는 것이 있습니다. 다양한 유형과 소스의 혼합이 있고, 부분적으로 합성이고 부분적으로 인간이며, 그런 방향으로 갔습니다. 하지만 대략적으로 우리는 여전히 SFT 데이터셋이 있고, 대화로 구성되어 있고, 이전처럼 학습합니다. 마지막으로 언급하고 싶은 것은

[1:17:00] ChatGPT 같은 AI와 대화하는 것의 마법을 조금 해소하고 싶다는 것입니다. ChatGPT에 가서 질문을 하고 엔터를 치면, 돌아오는 것은 학습 세트에서 일어나는 것과 통계적으로 정렬되어 있습니다. 이 학습 세트들은 실제로 라벨링 지침을 따르는 인간들에 시작점이 있습니다. ChatGPT에서 실제로 무엇과 대화하고 있는 걸까요? 어떻게 생각해야 할까요? 마법 같은 AI에서 오는 것이 아닙니다. 대략적으로 말하면

[1:17:30] 인간 라벨러를 통계적으로 모방하는 것에서 오고, 이것은 이 회사들이 작성한 라벨링 지침에서 옵니다. 이 모방을 하는 것이죠. ChatGPT에서 주어지는 답이 인간 라벨러의 어떤 종류의 시뮬레이션이라고 상상해보세요. 이런 종류의 대화에서 인간 라벨러가 뭐라고 할까를 묻는 것과 같습니다. 이 인간

[1:18:00] 라벨러는 인터넷의 랜덤한 사람이 아닙니다. 이 회사들이 실제로 전문가를 고용하기 때문입니다. 예를 들어 코드에 대한 질문을 할 때, 이 대화 데이터셋 생성에 관여하는 인간 라벨러들은 보통 교육받은 전문가입니다. 그런 사람들의 시뮬레이션에게 질문하는 것과 같습니다. 마법 같은 AI와 대화하는 것이 아니라 평균적인 라벨러와 대화하는 것입니다. 이 평균적인 라벨러는 아마도 상당히 숙련되어 있지만, 이런 종류의

[1:18:30] 사람의 즉각적인 시뮬레이션과 대화하는 것입니다. 이 데이터셋 구성에 고용될 사람이죠. 넘어가기 전에 한 가지 더 구체적인 예시를 보여드리겠습니다. 예를 들어 ChatGPT에 가서 "파리에서 봐야 할 상위 5개 랜드마크를 추천해줘"라고 말하고 엔터를 치면 여기서 나오는 것은 어떻게 생각해야 할까요?

[1:19:00] 나가서 모든 랜드마크를 조사하고 무한한 지능으로 순위를 매긴 마법 같은 AI가 아닙니다. 얻는 것은 OpenAI가 고용한 라벨러의 통계적 시뮬레이션입니다. 대략 그렇게 생각할 수 있습니다. 이 특정 질문이 OpenAI의 후속학습 데이터셋에 어딘가에 있다면, 인간 라벨러가 그 다섯 랜드마크에 대해 적었을 것과 매우 유사한 답을 볼 가능성이 높습니다. 인간 라벨러가 이것을 어떻게 만들까요? 나가서 인터넷에서

[1:19:30] 20분 동안 자체 조사를 하고 목록을 만듭니다. 이 목록을 만들어 데이터셋에 있으면, 어시스턴트의 정답으로 제출한 것을 볼 가능성이 매우 높습니다. 이 특정 쿼리가 후속학습 데이터셋의 일부가 아니라면, 여기서 얻는 것은 조금 더 창발적입니다. 모델이 통계적으로 이 학습 세트에 있는 랜드마크의 종류를 이해하기 때문입니다. 보통 유명한 랜드마크, 사람들이 보통 보고 싶어하는 랜드마크,

[1:20:00] 인터넷에서 자주 언급되는 랜드마크입니다. 모델이 이미 인터넷에서의 사전학습에서 엄청난 지식을 가지고 있다는 것을 기억하세요. 파리, 랜드마크, 사람들이 보고 싶어하는 것에 대한 수많은 대화를 봤을 것입니다. 후속학습 데이터셋과 결합된 사전학습 지식이 이런 종류의 모방을 만들어냅니다. 대략 이것이 뒤에서 일어나고 있는 일을

[1:20:30] 통계적 의미에서 생각할 수 있는 방법입니다. 자, 이제 제가 LLM 심리학이라고 부르고 싶은 주제로 넘어가겠습니다. 이 모델들의 학습 파이프라인에서 나타나는 창발적 인지 효과가 무엇인지입니다. 특히 첫 번째로 이야기하고 싶은 것은 물론 환각입니다. 모델 환각에 대해 익숙하실 수 있습니다. LLM이 정보를 지어냅니다. 완전히 정보를 날조합니다. LLM 어시스턴트의 큰 문제입니다.

---
