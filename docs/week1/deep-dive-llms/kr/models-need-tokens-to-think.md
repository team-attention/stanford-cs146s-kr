---
title: "13. Models Need Tokens to Think"
titleKr: "13. 모델은 생각하기 위해 토큰이 필요하다"
chapter: 13
timestamp: "1:46:56"
sourceUrl: "https://www.youtube.com/watch?v=7xTGNNLPyMI&t=6416s"
translatedAt: "2026-01-10"
---

# 13. 모델은 생각하기 위해 토큰이 필요하다

[영상 바로가기 (1:46:56)](https://www.youtube.com/watch?v=7xTGNNLPyMI&t=6416s)

## 요약

LLM의 신경망은 각 토큰을 생성할 때 약 100개 층의 고정된 계산만 수행합니다. 복잡한 문제를 단일 토큰으로 답하라고 요구하면 실패하는 이유입니다. 대신 중간 단계를 거쳐 추론을 "펼치면" 각 단계에서 간단한 계산만 필요하므로 정확한 답에 도달할 수 있습니다. 이것이 Chain of Thought의 원리이며, 암산에 의존하지 않고 코드 도구를 사용하면 더 신뢰할 수 있는 결과를 얻습니다.

**핵심 개념:**
- **토큰당 고정 계산량**: 각 토큰 생성에 약 100개 층의 유한한 연산만 사용됨
- **Chain of Thought**: 복잡한 계산을 여러 토큰에 분산시켜 중간 결과를 생성하는 기법
- **나쁜 학습 데이터**: "정답은 3입니다"처럼 바로 답하는 예시는 모델이 추측하도록 만듦
- **좋은 학습 데이터**: 단계별로 풀이 과정을 보여주는 예시가 모델 학습에 효과적
- **도구 사용 권장**: 암산 대신 Python 인터프리터 같은 도구를 활용하여 정확도 향상

---

## 전체 번역

**요약**: 모델이 "생각"하기 위해 토큰이 필요하다는 개념을 설명합니다. 복잡한 문제는 중간 단계(chain of thought)를 거쳐야 풀 수 있으며, 바로 답을 요구하면 실패합니다. 이것이 o1 같은 추론 모델의 기반이 됩니다.

[1:47:00] 이 모델들의 문제 해결 시나리오에서의 계산 능력, 네이티브 계산 능력을 다룹니다. 특히 대화의 예시를 구성할 때 이 모델들에 매우 주의해야 하고, 이 모델들이 어떻게 생각하는지 고려할 때 유익한 많은 날카로운 경계가 있습니다. 인간의 다음 프롬프트를 고려해보세요. 기본적으로 대화 학습 세트에 들어갈 대화를 만들고 있다고 가정합니다.

[1:47:30] 이것에서 모델을 학습시킬 것입니다. 기본적으로 간단한 수학 문제를 어떻게 푸는지 가르치는 것입니다. 프롬프트는 "에밀리가 사과 3개와 오렌지 2개를 삽니다. 각 오렌지는 2달러입니다. 총 비용은 13달러입니다. 사과 비용은 얼마입니까?" 매우 간단한 수학 문제입니다. 여기 왼쪽과 오른쪽에 두 개의 답이 있습니다. 둘 다 정답입니다. 둘 다 정답이 3이라고 합니다. 맞습니다. 하지만 이 두 개 중 하나가 어시스턴트에게 상당히 더 나은 답입니다. 데이터 라벨러이고 이 중 하나를 만들고 있다면

[1:48:00] 하나는 어시스턴트에게 정말 끔찍한 답이고 다른 하나는 괜찮습니다. 비디오를 멈추고 왜 이 두 개 중 하나가 상당히 더 나은 답인지 생각해보세요. 잘못된 것을 사용하면 모델이 실제로 수학을 정말 못하게 될 수 있고, 나쁜 결과를 가져옵니다. 이것은 라벨링 문서에서 이상적인 어시스턴트 응답을 만들도록 사람들을 훈련할 때 주의해야 할 것입니다.

[1:48:30] 이 질문의 핵심은 모델이 학습하고 추론할 때 왼쪽에서 오른쪽으로 1차원 토큰 시퀀스에서 작업한다는 것을 깨닫고 기억하는 것입니다. 이것이 제가 자주 머릿속에 가지고 있는 그림입니다. 토큰 시퀀스가 왼쪽에서 오른쪽으로 진화하는 것을 상상합니다. 시퀀스에서 다음 토큰을 항상 생성하기 위해 이 모든 토큰을 신경망에 공급하고, 이 신경망이 시퀀스에서 다음 토큰에 대한 확률을 줍니다.

[1:49:00] 여기 이 그림은 이전에 본 정확히 같은 그림입니다. 이전에 보여드린 웹 데모에서 온 것입니다. 이것이 기본적으로 상단의 입력 토큰을 가져와서 모든 뉴런의 이런 연산을 수행하고 다음에 올 것에 대한 확률의 답을 주는 계산입니다. 중요한 것은 대략적으로 여기서 일어나는 계산의 층 수가 유한하다는 것입니다. 예를 들어 여기 이 모델은

[1:49:30] 어텐션과 MLP의 1, 2, 3층만 있습니다. 일반적인 현대 최첨단 네트워크는 약 100층 정도가 있지만, 이전 토큰 시퀀스에서 다음 토큰의 확률로 가는 데 약 100층의 계산만 있습니다. 여기서 일어나는 계산의 유한한 양이 있고, 이것을 매우 적은 양의 계산이라고 생각해야 합니다. 이 양의 계산은 이 시퀀스의 모든 개별 토큰에 대해 거의 대략 고정되어 있습니다.

[1:50:00] 완전히 사실은 아닙니다. 더 많은 토큰을 넣을수록 이 신경망의 순방향 패스가 더 비싸지지만, 많이는 아닙니다. 이것을 좋은 모델로 생각하고, 이 박스에서 이 모든 토큰에 대해 고정된 양의 계산이 일어날 것이라고 생각해야 합니다. 이 양의 계산이 너무 클 수 없습니다. 위에서 아래로 가는 층이 그렇게 많지 않기 때문입니다. 여기서 계산적으로 그렇게 많이 일어나지 않습니다.

[1:50:30] 모델이 단일 순방향 패스에서 단일 토큰을 얻기 위해 임의의 계산을 할 것이라고 상상할 수 없습니다. 이것이 의미하는 바는 우리가 실제로 추론과 계산을 많은 토큰에 분산해야 한다는 것입니다. 모든 개별 토큰이 유한한 양의 계산만 사용하기 때문입니다. 계산을 많은 토큰에 분산하고 싶고, 어떤 단일 개별 토큰에서 모델에게 너무 많은 계산을 기대할 수 없습니다. 토큰당 일어나는 계산이 그만큼밖에 없기 때문입니다.

[1:51:00] 대략 토큰당 고정된 양의 계산입니다. 그래서 여기 이 답이 상당히 더 나쁩니다. 그 이유는 여기서 왼쪽에서 오른쪽으로 가는 것을 상상해보세요. 바로 여기에 복사 붙여넣기했습니다. "정답은 3입니다" 등. 모델이 왼쪽에서 오른쪽으로 이 토큰들을 하나씩 방출하는 것을 상상해보세요. "정답은 $ 기호"를 말해야 하고, 바로 여기서

[1:51:30] 이 문제의 모든 계산을 이 단일 토큰에 밀어넣어야 합니다. 정답 3을 방출해야 합니다. 그런 다음 답 3을 방출하면 이 모든 토큰을 말할 것으로 예상되지만, 이 시점에서 답은 이미 생성되었고 이미 컨텍스트 윈도우에 있습니다. 그래서 여기 있는 모든 것은 왜 이것이 답인지에 대한 사후적 정당화일 뿐입니다.

[1:52:00] 답은 이미 만들어졌고 토큰 윈도우에 있으므로 실제로 여기서 계산되는 것이 아닙니다. 그래서 질문에 직접적이고 즉시 답하면 모델이 단일 토큰에서 답을 추측하도록 훈련하는 것이고, 토큰당 일어나는 유한한 양의 계산 때문에 작동하지 않을 것입니다. 그래서 오른쪽 답이 상당히 더 낫습니다. 계산을 답에 분산하고 있기 때문입니다. 실제로 모델이 왼쪽에서 오른쪽으로 천천히 답에 도달하도록 하고 있습니다. 중간 결과를 얻습니다.

[1:52:30] "오렌지의 총 비용은 4입니다. 그래서 13 - 4는 9입니다." 중간 계산을 만들고 있고 각 계산은 그 자체로 그렇게 비싸지 않습니다. 기본적으로 모델이 이런 개별 토큰 각각에서 할 수 있는 어려움을 약간 추측하는 것이고, 이 토큰들 중 어느 것에서도 계산적으로 너무 많은 작업이 있을 수 없습니다. 그러면 모델이 나중에 테스트 시에 그것을 할 수 없을 것이기 때문입니다. 그래서 여기서 모델에게 추론을 펼치고 계산을

[1:53:00] 토큰에 분산하도록 가르치고 있습니다. 이런 방식으로 각 토큰에 매우 간단한 문제만 있고, 합쳐지고, 끝에 가까워지면 작업 메모리에 모든 이전 결과가 있어서 정답이 무엇인지, 여기 3이라는 것을 결정하기가 훨씬 쉽습니다. 이것이 계산에 상당히 더 나은 레이블입니다. 이것은 정말 나쁘고 모델에게 모든 계산을 단일 토큰에서 하도록 가르치고 있어서 정말 나쁩니다.

[1:53:30] 기억해야 할 흥미로운 것은 프롬프트에서 보통 명시적으로 이것에 대해 생각할 필요가 없다는 것입니다. OpenAI의 사람들이 실제로 이것에 대해 걱정하는 라벨러들이 있고 답이 펼쳐지도록 확인하기 때문입니다. 그래서 실제로 OpenAI는 옳은 일을 할 것입니다. ChatGPT에 이 질문을 하면 실제로 매우 천천히 갈 것입니다. "변수를 정의하고, 방정식을 세우고" 이런 모든 중간 결과를 만들고 있습니다. 이것은 당신을 위한 것이 아닙니다. 모델을 위한 것입니다. 모델이 이런 중간

[1:54:00] 결과를 스스로 만들지 않으면 3에 도달할 수 없을 것입니다. 모델에게 약간 못되게 굴 수 있다는 것도 보여드리고 싶습니다. 예를 들어 정확히 같은 프롬프트를 주고 "단일 토큰으로 질문에 답해. 즉시 답만 줘, 다른 거 없이"라고 말했습니다. 이 간단한 프롬프트에 대해 실제로 한 번에 할 수 있었습니다. 단일 토큰을 생성했습니다. 이것이 두 개의 토큰인 것 같습니다.

[1:54:30] 달러 기호가 자체 토큰이니까요. 기본적으로 이 모델은 단일 토큰을 주지 않고 두 개의 토큰을 줬지만 여전히 정답을 생성했습니다. 네트워크의 단일 순방향 패스에서 그렇게 했습니다. 여기 숫자들이 매우 간단하기 때문입니다. 모델에게 약간 못되게 굴기 위해 조금 더 어렵게 만들었습니다. "에밀리가 사과 23개와 오렌지 177개를 삽니다." 숫자를 조금 크게 만들었습니다. 모델이 단일 토큰에서 더 많은 계산을 하도록 요구하는 것입니다.

[1:55:00] 같은 것을 말했고 여기서 5를 줬는데 5는 실제로 맞지 않습니다. 모델이 네트워크의 단일 순방향 패스에서 이 모든 계산을 하지 못했습니다. 입력 토큰에서 네트워크의 단일 순방향 패스, 단일 네트워크 통과에서 결과를 생성하지 못했습니다. 그런 다음 "토큰 제한에 대해 걱정하지 말고 평소처럼 문제를 풀어"라고 말했습니다. 그러면 모든 중간 결과를 거치고, 단순화하고, 여기 중간 결과와 중간 계산들 각각은 모델에게 훨씬 쉽고

[1:55:30] 토큰당 너무 많은 작업이 아닙니다. 여기 모든 토큰들이 정확하고 해결책 7에 도달합니다. 이 모든 작업을 네트워크의 단일 순방향 패스에 밀어넣을 수 없었습니다. 귀여운 예시이고 이 모델들이 어떻게 작동하는지 생각할 것이고 다시 유익합니다. 이 주제에 대해 마지막으로 말하고 싶은 것은 실제로 일상생활에서 이것을 풀려고 한다면 모델이 여기서 모든 중간 계산을 올바르게 했다고

[1:56:00] 신뢰하지 않을 것입니다. 실제로 아마 이런 것을 할 것입니다. "코드를 사용해"라고 말할 것입니다. 코드가 ChatGPT가 사용할 수 있는 가능한 도구 중 하나이고, 이런 암산을 해야 하는 대신, 여기 암산을 완전히 신뢰하지 않고, 특히 숫자가 정말 커지면 모델이 이것을 올바르게 할 것이라는 보장이 없습니다. 신경망을 사용해서 암산을 하는 것입니다.

[1:56:30] 머릿속에서 암산을 하는 것과 같습니다. 중간 결과 중 일부가 잘못될 수 있습니다. 모델이 이런 종류의 암산을 할 수 있다는 것이 실제로 놀랍습니다. 제가 머릿속에서 이것을 할 수 있을 것 같지 않지만, 기본적으로 모델은 머릿속에서 하고 있고 신뢰하지 않습니다. 그래서 도구를 사용하고 싶습니다. "코드 사용"이라고 말하고 그것이 작동할 것으로 예상합니다. 봅시다.

[1:57:00] 7이 맞습니다. 여기서 일어난 일은 실제로, 보이지 않지만, 문제를 모델에게 더 쉬운 문제들로 분해했습니다. 모델이 암산을 할 수 없다는 것을 알지만, 복사 붙여넣기는 꽤 잘한다는 것을 압니다. 여기서 "코드 사용"이라고 말하면 Python에서 이것에 대한 문자열을 만들고, 기본적으로 여기 입력을 여기로 복사 붙여넣기하는 작업은

[1:57:30] 모델에게 매우 간단합니다. 이 점 문자열을 이 네 개 토큰 정도로 보기 때문입니다. 모델이 그 토큰 ID를 복사 붙여넣기하고 여기 점으로 풀어내는 것은 매우 간단합니다. 이 문자열을 만들고 Python 루틴 .count를 호출하고 정답을 얻습니다. Python 인터프리터가 세는 것이지 모델의 암산이 세는 것이 아닙니다.

[1:58:00] 다시 간단한 예시입니다. 모델은 생각하기 위해 토큰이 필요하고, 암산에 의존하지 마세요. 그래서 모델은 세기 작업도 잘 못합니다. 세기 작업이 필요하면 항상 도구에 의존하도록 요청하세요. 모델은 여기저기 많은 다른 작은 인지적 결함도 있고, 이것은 알아야 할 기술의 날카로운 경계입니다. 예를 들어 모델은 모든 종류의 철자 관련 작업을 잘 못합니다. 토큰화로 다시 돌아올 것이라고 말했습니다.

---
