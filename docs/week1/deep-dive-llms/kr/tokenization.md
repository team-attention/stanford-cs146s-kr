---
title: "3. Tokenization"
titleKr: "3. 토큰화"
chapter: 3
timestamp: "7:47"
sourceUrl: "https://www.youtube.com/watch?v=7xTGNNLPyMI&t=467s"
translatedAt: "2026-01-10"
---

# 3. 토큰화

[영상 바로가기 (7:47)](https://www.youtube.com/watch?v=7xTGNNLPyMI&t=467s)

## 요약

텍스트를 신경망이 처리할 수 있는 토큰 시퀀스로 변환하는 과정을 설명합니다. 신경망은 유한한 기호 집합의 1차원 시퀀스를 입력으로 받기 때문에, 텍스트를 적절한 크기의 토큰으로 분할해야 합니다. UTF-8 바이트에서 시작하여 바이트 페어 인코딩(BPE) 알고리즘으로 약 100,000개의 토큰 어휘를 생성하며, GPT-4는 100,277개의 토큰을 사용합니다.

**핵심 개념:**
- **토큰(Token)**: 텍스트의 기본 단위로, 신경망이 처리하는 기호
- **어휘(Vocabulary)**: 가능한 모든 토큰의 집합으로, GPT-4는 100,277개 보유
- **UTF-8 인코딩**: 텍스트를 바이트 시퀀스로 변환하는 표준 방식
- **바이트 페어 인코딩(BPE)**: 자주 등장하는 바이트 쌍을 새 토큰으로 병합하는 알고리즘
- **시퀀스 길이와 어휘 크기의 트레이드오프**: 적은 기호(2개)와 긴 시퀀스보다 많은 기호(100K)와 짧은 시퀀스가 효율적
- **Tiktokenizer**: GPT-4의 토큰화 과정을 시각적으로 확인할 수 있는 웹사이트

---

## 전체 번역

**요약**: 텍스트를 신경망에 입력하기 위해 토큰으로 변환하는 과정을 설명합니다. UTF-8 바이트에서 시작해 바이트 페어 인코딩(BPE) 알고리즘을 사용하여 약 100,000개의 토큰 어휘를 생성하며, GPT-4는 100,277개의 토큰을 사용합니다.

[8:00] 1차원 기호 시퀀스를 기대하고, 가능한 기호의 유한한 집합을 원합니다. 그래서 기호가 무엇인지 결정하고, 데이터를 그 기호들의 1차원 시퀀스로 표현해야 합니다. 지금 우리가 가진 것은 텍스트의 1차원 시퀀스입니다. 여기서 시작해서 여기로 가고, 여기로 오고... 화면에는 2차원으로 배치되어 있지만 왼쪽에서 오른쪽으로, 위에서 아래로 가는

[8:30] 텍스트의 1차원 시퀀스입니다. 물론 컴퓨터이기 때문에 기본 표현이 있습니다. 이 텍스트를 UTF-8로 인코딩하면 컴퓨터에서 이 텍스트에 해당하는 원시 비트를 얻을 수 있고, 이렇게 생겼습니다. 예를 들어 여기 이 첫 번째 막대가 첫 번째

[9:00] 8비트입니다. 이것이 무엇일까요? 어떤 의미에서는 우리가 찾고 있는 표현입니다. 정확히 두 가지 가능한 기호 0과 1이 있고, 매우 긴 시퀀스가 있습니다. 하지만 이 시퀀스 길이는 신경망에서 매우 유한하고 귀중한 자원이 될 것이고, 우리는 실제로 두 가지 기호만의 극도로 긴 시퀀스를 원하지 않습니다. 대신 우리가 원하는 것은

[9:30] 어휘라고 부르는 이 기호 크기와 결과 시퀀스 길이 사이의 트레이드오프입니다. 두 개의 기호와 극도로 긴 시퀀스를 원하지 않고, 더 많은 기호와 더 짧은 시퀀스를 원합니다. 자, 시퀀스 길이를 압축하거나 줄이는 한 가지 단순한 방법은 기본적으로 연속된 비트들의 그룹, 예를 들어 8비트를 고려해서 하나의

[10:00] 바이트라고 부르는 것으로 그룹화하는 것입니다. 이 비트들은 켜져 있거나 꺼져 있으므로, 8개의 그룹을 취하면 이 비트들이 켜지거나 꺼질 수 있는 조합이 256가지뿐입니다. 따라서 이 시퀀스를 바이트 시퀀스로 다시 표현할 수 있습니다. 이 바이트 시퀀스는 8배 더 짧지만 이제 256개의 가능한 기호가 있습니다. 여기 모든 숫자는 0에서 255까지입니다. 이것들을 숫자가 아니라

[10:30] 고유 ID나 고유 기호로 생각하시길 권합니다. 어쩌면 이것들을 각각 고유한 이모지로 대체해서 생각하는 게 더 나을 수 있습니다. 그러면 이렇게 됩니다. 기본적으로 이모지 시퀀스가 있고 256개의 가능한 이모지가 있다고 생각할 수 있습니다. 실제로 최첨단 언어 모델의 프로덕션에서는 이것보다 더 나아가야 합니다. 시퀀스 길이를 계속 줄여야 하는데,

[11:00] 다시 말하지만 귀중한 자원이기 때문입니다. 더 많은 기호를 어휘에 넣는 대가로요. 이렇게 하는 방식이 바이트 페어 인코딩 알고리즘을 실행하는 것입니다. 작동 방식은 기본적으로 매우 흔한 연속된 바이트나 기호를 찾는 것입니다. 예를 들어 116 다음에 32가 오는 시퀀스가 상당히 흔하고 매우 자주 발생한다는 것이 밝혀졌습니다. 그래서 우리가 할 일은 이 쌍을 새 기호로 그룹화하는 것입니다. ID 256인 기호를 새로 만들고

[11:30] 모든 쌍 116, 32를 이 새 기호로 다시 씁니다. 그런 다음 원하는 만큼 이 알고리즘을 반복할 수 있고, 새 기호를 만들 때마다 길이를 줄이고 기호 크기를 늘립니다. 실제로 어휘 크기의 꽤 좋은 설정은 약 100,000개의 가능한 기호로 밝혀졌습니다. 특히 GPT-4는 100,277개의 기호를

[12:00] 사용합니다. 원시 텍스트에서 이 기호들, 또는 토큰이라고 부르는 것으로 변환하는 이 과정을 토큰화라고 합니다. 이제 GPT-4가 텍스트에서 토큰으로, 토큰에서 다시 텍스트로 토큰화를 어떻게 수행하는지, 그리고 실제로 어떻게 생겼는지 살펴봅시다. 이런 토큰 표현을 탐색하기 위해 제가 좋아하는 웹사이트는 tiktokenizer입니다. 여기 드롭다운에서

[12:30] GPT-4 기본 모델 토크나이저인 cl100k_base를 선택하고, 왼쪽에 텍스트를 넣으면 그 텍스트의 토큰화를 보여줍니다. 예를 들어 "hello space world", 즉 hello world는 정확히 두 개의 토큰으로 밝혀졌습니다. ID 15339인 토큰 hello와

[13:00] ID 1917인 토큰 " world"(앞에 공백 포함)입니다. 그래서 "hello space world"... 이 두 개를 붙이면 다시 두 개의 토큰이 되지만, H 토큰 다음에 H 없는 "ello world" 토큰이 됩니다. hello와 world 사이에 공백을 두 개 넣으면 또 다른 토큰화가 됩니다. 여기 새로운 토큰 220이 있습니다. 이것을 가지고 놀면서 무슨 일이 일어나는지 볼 수 있습니다. 또한 이것은

[13:30] 대소문자를 구분합니다. 대문자 H면 다른 것이 되고, "Hello World"면 실제로 세 개의 토큰이 됩니다. 네, 그래서 이것을 가지고 놀면서 이 토큰들이 어떻게 작동하는지 직관적인 감각을 얻을 수 있습니다. 영상 후반에 토큰화로 다시 돌아올 건데, 지금은 웹사이트를 보여드리고 싶었고 기본적으로 이 텍스트가 예를 들어

[14:00] 여기 한 줄을 가져오면 GPT-4는 이것을 이렇게 볼 것입니다. 이 텍스트는 길이 62의 시퀀스가 될 것이고, 이것이 시퀀스이고, 텍스트 청크들이 이 기호들에 어떻게 대응하는지입니다. 다시 말하지만 100,277개의 가능한 기호가 있고, 이제 그 기호들의 1차원 시퀀스가 있습니다. 토큰화로 돌아올 테지만 지금은 여기까지입니다. 자, 제가 한 것은

---
