---
title: "7. GPT-2: Training and Inference"
titleKr: "7. GPT-2: 훈련과 추론"
chapter: 7
timestamp: "31:09"
sourceUrl: "https://www.youtube.com/watch?v=7xTGNNLPyMI&t=1869s"
translatedAt: "2026-01-10"
---

# 7. GPT-2: 훈련과 추론

[영상 바로가기 (31:09)](https://www.youtube.com/watch?v=7xTGNNLPyMI&t=1869s)

## 요약

OpenAI의 GPT-2 모델을 실제 예시로 훈련과 추론 과정을 시연합니다. 2019년에 공개된 GPT-2는 16억 개의 파라미터와 1,024 토큰의 컨텍스트 길이를 가진 트랜스포머로, 현대 LLM의 원형입니다. GitHub의 llm.c 프로젝트로 약 $100-600에 재현할 수 있으며, 훈련 중 loss 감소를 관찰하면서 모델이 점점 더 일관된 텍스트를 생성하는 것을 확인할 수 있습니다.

**핵심 개념:**
- **GPT-2**: 2019년 OpenAI가 공개한 16억 파라미터의 트랜스포머 모델
- **loss(손실)**: 모델의 예측 성능을 나타내는 지표로, 낮을수록 좋음
- **llm.c**: GPT-2를 처음부터 재현할 수 있는 오픈소스 프로젝트
- **H100 GPU**: 현대 LLM 훈련에 사용되는 Nvidia의 고성능 GPU
- **GPU 클러스터**: 수천~수만 개의 GPU를 연결한 대규모 계산 시스템
- **훈련 비용 감소**: 2019년 $40,000에서 현재 $100-600으로 대폭 하락

---

## 전체 번역

**요약**: OpenAI의 GPT-2 모델을 예시로 학습과 추론을 시연합니다. 2019년에 공개된 GPT-2는 15억 개의 파라미터를 가지며, GitHub의 llm.c 프로젝트로 직접 학습해볼 수 있습니다. 인터넷 데이터의 통계적 패턴을 학습한 모델의 특성을 보여줍니다.

[31:30] GPT-4, 그 시리즈의 네 번째 버전입니다. GPT-2는 2019년에 OpenAI가 여기 있는 이 논문에서 발표했습니다. 제가 GPT-2를 좋아하는 이유는 인식 가능하게 현대적인 스택이 처음으로 모인 때이기 때문입니다. GPT-2의 모든 조각들이 현대 기준으로 오늘날에도 인식 가능합니다. 모든 것이 더 커졌을 뿐입니다. 물론 기술 논문이기 때문에 이 논문의 전체 세부 사항을 다루지는 않겠지만, 강조하고 싶은

[32:00] 세부 사항들은 다음과 같습니다. GPT-2는 오늘날 작업하는 신경망처럼 트랜스포머 신경망이었습니다. 16억 개의 파라미터를 가졌습니다. 여기서 본 파라미터들이고 16억 개입니다. 오늘날 현대 트랜스포머들은 1조 개나 수천억 개에 훨씬 가까울 것입니다. 여기서 최대 컨텍스트 길이는 1,024 토큰이었습니다. 데이터셋에서 토큰 윈도우의 청크를

[32:30] 샘플링할 때 1,024 토큰 이상은 절대 취하지 않습니다. 시퀀스에서 다음 토큰을 예측하려고 할 때 예측을 위해 컨텍스트에 1,024 토큰 이상은 절대 없습니다. 이것도 현대 기준으로는 작습니다. 오늘날 컨텍스트 길이는 수십만 개나 백만 개에 훨씬 가까울 것이므로 훨씬 더 많은 컨텍스트, 기록에 훨씬 더 많은 토큰이 있고

[33:00] 그런 식으로 시퀀스에서 다음 토큰에 대해 훨씬 더 나은 예측을 할 수 있습니다. 마지막으로 GPT-2는 약 1,000억 개의 토큰으로 훈련되었고, 이것도 현대 기준으로는 상당히 작습니다. 언급했듯이 우리가 본 FineWeb 데이터셋은 15조 개의 토큰을 가지고 있으므로 1,000억은 상당히 작습니다. 실제로 재미로 GPT-2를 재현하려고 했는데, llm.c라는 프로젝트의 일환입니다.

[33:30] GitHub의 llm.c 저장소 아래 이 포스트에서 제가 그것을 한 재현을 볼 수 있습니다. 특히 2019년에 GPT-2를 훈련하는 비용은 약 $40,000로 추정되었지만, 오늘날에는 훨씬 더 잘할 수 있습니다. 특히 여기서는 약 하루와 약 $600가 들었지만 너무 열심히 하지도 않았고, 오늘날에는 약 $100까지 낮출 수 있다고 생각합니다. 왜 비용이

[34:00] 이렇게 많이 내려갔을까요? 첫째, 이 데이터셋들이 훨씬 좋아졌고 필터링, 추출, 준비하는 방식이 훨씬 정교해져서 데이터셋의 품질이 훨씬 높아졌습니다. 그것이 한 가지이지만, 정말 가장 큰 차이는 컴퓨터가 하드웨어 측면에서 훨씬 빨라졌다는 것이고, 잠시 후에 살펴보겠습니다. 또한 이 모델들을 실행하고 가능한 한 하드웨어에서 모든 속도를 짜내는

[34:30] 소프트웨어도 모든 사람들이 이 모델들에 집중하고 매우 빠르게 실행하려고 하면서 훨씬 좋아졌습니다. 이 GPT-2 재현의 전체 세부 사항을 다루지는 못하고 이것은 긴 기술 포스트이지만, 연구자로서 이 모델 중 하나를 실제로 훈련하는 것이 어떤 모습인지, 무엇을 보고 어떤 느낌인지 직관적인 감각을 드리고 싶습니다. 조금 보여드리겠습니다. 자, 이게 어떻게 생겼는지 보여드리겠습니다. 이것을 옆으로 밀겠습니다. 여기서 지금 GPT-2 모델을 훈련하고 있고

[35:00] 일어나는 것은 여기 모든 한 줄, 이것처럼, 이것이 모델에 대한 한 번의 업데이트입니다. 여기서 이 모든 토큰들에서 예측을 더 잘하게 만들고 신경망의 가중치나 파라미터를 업데이트하는 것을 기억하세요. 여기 모든 한 줄은 다음 토큰을 예측하는 것을 더 잘하도록 파라미터를 조금씩 바꾸는 신경망에 대한 한 번의 업데이트입니다. 특히 여기 모든 한 줄은

[35:30] 훈련 세트의 100만 개 토큰에 대한 예측을 개선하는 것입니다. 기본적으로 이 데이터셋에서 100만 개의 토큰을 꺼내서 그 토큰이 시퀀스에서 다음으로 오는 예측을 100만 개 모두 동시에 개선하려고 했고, 이 모든 단계에서 네트워크에 대한 업데이트를 합니다. 가까이 봐야 할 숫자는 이

[36:00] loss라는 숫자입니다. loss는 신경망이 지금 얼마나 잘 수행하고 있는지 알려주는 단일 숫자이고 낮은 loss가 좋도록 만들어졌습니다. 신경망에 더 많은 업데이트를 하면서 loss가 감소하는 것을 볼 수 있고, 이것은 시퀀스에서 다음 토큰에 대해 더 나은 예측을 하는 것에 해당합니다. loss는 신경망 연구자로서 보는 숫자이고 손가락을 빙빙 돌리고, 커피를 마시며, 이것이 잘 보이는지

[36:30] 확인합니다. 모든 업데이트마다 loss가 개선되고 네트워크가 예측을 더 잘하게 되도록요. 여기서 업데이트당 100만 개의 토큰을 처리하고, 각 업데이트는 대략 7초 정도 걸리고, 총 32,000 단계의 최적화를 처리할 것입니다. 32,000 단계에 각각 100만 개 토큰이면 약 330억 개의 토큰을 처리할 것이고, 현재 32,000 중 약 420

[37:00] 단계이므로 아직 1%를 조금 넘게만 완료한 것입니다. 10분이나 15분 정도밖에 실행하지 않았기 때문입니다. 20 단계마다 이 최적화가 추론을 하도록 설정했습니다. 여기서 보는 것은 모델이 시퀀스에서 다음 토큰을 예측하는 것입니다. 무작위로 시작하고 토큰을 계속 넣습니다. 추론 단계를 실행하고 있고 모델이 시퀀스에서 다음 토큰을 예측하는 것입니다. 무언가가 나타날 때마다 그것이 새

[37:30] 토큰입니다. 이것을 보면 아직 그다지 일관성이 없다는 것을 알 수 있고, 훈련의 1%밖에 완료되지 않았다는 것을 기억하세요. 모델은 아직 시퀀스에서 다음 토큰을 예측하는 것을 잘 못하므로 나오는 것이 실제로 약간 횡설수설합니다. 하지만 여전히 약간의 지역적 일관성이 있습니다. "since she is mine it's a part of the information should discuss my father great companions Gordon showed me sitting over at" 등등...

[38:00] 그다지 좋아 보이지 않는다는 것을 알지만, 실제로 위로 스크롤해서 최적화를 시작했을 때 어떻게 생겼는지 봅시다. 1단계에서요. 20 단계의 최적화 후에 얻는 것은 완전히 무작위로 보이고, 물론 그것은 모델이 파라미터에 20번의 업데이트만 했기 때문입니다. 무작위 네트워크이므로 무작위 텍스트를 주는 것입니다. 적어도 이것과 비교하면

[38:30] 모델이 훨씬 더 잘하기 시작했다는 것을 알 수 있고, 실제로 전체 32,000 단계를 기다리면 모델은 실제로 상당히 일관된 영어를 생성하고 토큰 스트림이 올바르게 되는 지점까지 개선될 것입니다. 훨씬 더 잘 영어를 만들어냅니다. 이것은 하루나 이틀 더 실행해야 합니다. 이 단계에서 우리는 loss가 감소하고 모든 것이 잘 보이는지 확인하고

[39:00] 기다리기만 하면 됩니다. 이제 필요한 계산의 이야기로 넘어가겠습니다. 물론 이 최적화를 제 노트북에서 실행하고 있지 않습니다. 너무 비쌀 것입니다. 이 신경망을 실행하고 개선해야 하고 이 모든 데이터가 필요하기 때문입니다. 네트워크가 너무 크기 때문에 컴퓨터에서 이것을 잘 실행할 수 없습니다. 이 모든 것은 클라우드 어딘가에 있는 컴퓨터에서 실행되고 있고, 기본적으로 이 모델들을 훈련하는 계산 측면의

[39:30] 이야기와 그게 어떻게 생겼는지 다루고 싶습니다. 살펴봅시다. 이 최적화를 실행하고 있는 컴퓨터는 이 8X H100 노드입니다. 단일 노드 또는 단일 컴퓨터에 8개의 H100이 있습니다. 이 컴퓨터를 임대하고 있고 클라우드 어딘가에 있습니다. 실제로 물리적으로 어디 있는지 확실하지 않습니다. 제가 좋아하는 임대 장소는 Lambda이지만 이 서비스를 제공하는 많은 다른 회사들이 있습니다. 스크롤하면

[40:00] 이 H100을 가진 컴퓨터들의 온디맨드 가격이 있는 것을 볼 수 있습니다. H100은 GPU이고 잠시 후에 어떻게 생겼는지 보여드리겠습니다. 온디맨드 8개 Nvidia H100 GPU, 이 기계는 예를 들어 GPU당 시간당 $3입니다. 이것들을 임대하면 클라우드에 기계를 얻고 들어가서 이 모델들을 훈련할 수 있습니다. 이 GPU들은 이렇게 생겼습니다. 이것이 하나의 H100 GPU이고

[40:30] 이렇게 생겼습니다. 컴퓨터에 이것을 꽂고, GPU는 네트워크를 훈련하기에 완벽한 적합입니다. 계산적으로 매우 비싸지만 계산에서 많은 병렬성을 보여주기 때문입니다. 이 신경망 훈련의 기반인 행렬 곱셈을 풀면서 많은 독립적인 작업자들이 동시에 작업할 수 있습니다. 이것은 이 H100 중 하나일 뿐이지만 실제로 여러 개를

[41:00] 함께 넣습니다. 8개를 단일 노드에 쌓을 수 있고, 여러 노드를 전체 데이터 센터나 전체 시스템 센터에 쌓을 수 있습니다. 데이터 센터를 보면 이렇게 생긴 것들을 보기 시작합니다. GPU 하나가 8개의 GPU가 되고 단일 시스템이 되고 많은 시스템이 됩니다. 이것들은 더 큰 데이터 센터이고 물론 훨씬 더 비쌀 것입니다. 일어나는 것은 모든

[41:30] 대형 기술 회사들이 이 GPU를 정말 원한다는 것입니다. 매우 강력하기 때문에 이 모든 언어 모델을 훈련할 수 있고, 이것이 근본적으로 Nvidia의 주가를 오늘 예를 들어 $3.4조로 끌어올린 것이고 왜 Nvidia가 폭발적으로 성장했는지입니다. 이것이 골드러시입니다. 골드러시는 GPU를 얻는 것이고, 충분히 얻어서 모두 협력하여 이 최적화를 수행하고, 모두

[42:00] 무엇을 하고 있냐면 모두 협력하여 FineWeb 데이터셋 같은 데이터셋에서 다음 토큰을 예측하는 것입니다. 이것이 기본적으로 극도로 비싼 계산 워크플로우이고, GPU가 많을수록 예측하고 개선하려는 토큰이 많아지고, 이 데이터셋을 더 빨리 처리하고, 더 빨리 반복하고, 더 큰 네트워크를 얻고, 더 큰 네트워크를 훈련할 수 있습니다. 이것이 모든 기계들이

[42:30] 하고 있는 것이고 이것이 왜 이 모든 것이 큰 일인지입니다. 예를 들어 이것은 한 달 전쯤의 기사인데, 일론 머스크가 단일 데이터 센터에 100,000개의 GPU를 얻는 것이 왜 큰 일인지입니다. 이 모든 GPU는 극도로 비싸고, 엄청난 전력이 필요할 것이고, 모두 시퀀스에서 다음 토큰을 예측하려고 하고 그렇게 해서 네트워크를 개선하고, 아마 여기서 보는 것보다 훨씬 더 일관된 텍스트를 훨씬 더 빨리 얻을 것입니다. 불행히도 저는 정말 큰 모델을 훈련하는 데 몇천만 달러나 억 달러를 쓸 돈이 없습니다.

---
