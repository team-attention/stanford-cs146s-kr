---
title: "18. DeepSeek R1"
titleKr: "18. DeepSeek R1"
chapter: 18
timestamp: "2:37:47"
sourceUrl: "https://www.youtube.com/watch?v=7xTGNNLPyMI&t=9467s"
translatedAt: "2026-01-10"
---

# 18. DeepSeek R1

[영상 바로가기 (2:37:47)](https://www.youtube.com/watch?v=7xTGNNLPyMI&t=9467s)

## 요약

딥시크 R1은 중국 스타트업 딥시크가 개발한 오픈 웨이트 추론 모델로, 순수 강화학습만으로 "생각하는" 능력을 발전시켰습니다. OpenAI의 o1/o3 모델과 유사한 사고 모델이지만, 누구나 다운로드하고 사용할 수 있다는 점에서 혁신적입니다. 때로는 영어와 중국어를 번갈아 사용하며 생각하는 등 흥미로운 창발적 행동을 보여줍니다.

**핵심 개념:**
- **사고 모델(Thinking Model)**: RL 훈련을 통해 문제를 풀기 전에 "생각하는" 과정을 자체 개발한 모델
- **오픈 웨이트(Open Weights)**: MIT 라이선스로 공개되어 누구나 다운로드, 호스팅, 수정 가능
- **증류 위험(Distillation Risk)**: 추론 과정을 공개하면 경쟁사가 이를 모방할 수 있어 OpenAI는 사고 과정을 숨김
- **Chain of Thought**: 모델이 스스로 발견한 단계별 추론 과정으로, 정확도를 높이는 핵심 전략
- **추론 제공자**: together.ai 등 오픈 모델을 호스팅하는 서비스로, 직접 딥시크 서버 대신 사용 가능

---

## 전체 번역

**요약**: 딥시크 R1 모델과 그 특징을 설명합니다. 중국 스타트업 딥시크가 개발한 이 모델은 순수 RL만으로 학습되어 자체적인 사고 과정(thinking)을 발전시켰습니다. 때로는 영어에서 중국어로 전환하며 생각하는 등 흥미로운 행동을 보입니다.

[2:38:00] 같은 방식으로 사고 모델 중 하나, 예를 들어 o3 mini high를 선택하고 실행해 볼 수 있습니다. 이 모델들은 월 20달러 또는 일부 최고 모델의 경우 월 200달러의 ChatGPT 구독을 지불하지 않으면 사용할 수 없을 수 있습니다. 사고 모델을 선택하고 실행하면, "추론 중"이라고 하면서

[2:38:30] 이런 것들을 시작합니다. 여기서 보는 것은 딥시크에서 보는 것과 정확히 같지 않습니다. 내부적으로 모델이 이런 종류의 Chain of Thought를 생성하지만, OpenAI는 웹 인터페이스에서 정확한 Chain of Thought를 보여주지 않기로 선택했습니다. 그 사고 체인의 작은 요약을 보여줍니다. OpenAI가 이렇게 하는 이유는

[2:39:00] 부분적으로 증류 위험이라고 부르는 것 때문입니다. 누군가 와서 그 추론 흔적을 모방하고 추론 Chain of Thought를 흉내 내는 것만으로 많은 추론 성능을 복구할 수 있다는 것입니다. 그래서 숨기고 작은 요약만 보여줍니다. 딥시크에서 얻는 것처럼 추론 자체에 대한 완전한 세부 사항을 얻지 못합니다.

[2:39:30] 그런 다음 풀이를 작성합니다. 이것들은 종류가 동등합니다. 비록 완전한 내부 세부 사항을 보지 못하지만요. 성능 면에서 이 모델들과 딥시크 모델은 현재 거의 동등합니다. 평가 때문에 말하기 어렵지만, OpenAI에 월 200달러를 지불하면 일부 모델은 여전히 더 좋아 보인다고 생각합니다. 하지만 딥시크 R1은 지금으로서는 여전히 매우 견고한

[2:40:00] 사고 모델 선택입니다. 이 웹사이트나 다른 웹사이트에서 이용 가능합니다. 모델이 오픈 웨이트이므로 그냥 다운로드할 수 있습니다. 지금까지의 요약은 무엇일까요? 강화학습과 검증 가능한 풀이가 있는 많은 수학 및 코드 문제에서 기본적으로 RL을 실행할 때 최적화 과정에서 사고가 나타난다는 것을 이야기했습니다.

[2:40:30] 이 사고 모델은 예를 들어 딥시크나 together.ai 같은 추론 제공자에서 접근할 수 있고, 거기서 딥시크를 선택합니다. ChatGPT에서도 o1 또는 o3 모델 아래에서 사고 모델을 이용할 수 있습니다. 하지만 GPT-4o 모델 등은 사고 모델이 아닙니다. 대부분 SFT 모델로 생각해야 합니다. 고급 추론이 필요한 프롬프트가 있다면

[2:41:00] 아마 사고 모델을 사용하거나 최소한 시도해 봐야 합니다. 하지만 경험적으로 많은 제 사용에서, 더 간단한 질문이나 지식 기반 질문 같은 것을 할 때는 과도할 수 있습니다. 사실적인 질문에 30초 동안 생각할 필요가 없습니다. 그래서 그런 경우에는 때때로 GPT-4o로 기본 설정합니다. 경험적으로 제 사용의 약 80-90%는 GPT-4o이고, 수학과 코드 등에서 매우 어려운 문제를 만나면

[2:41:30] 사고 모델을 사용합니다. 하지만 그러면 생각하기 때문에 조금 더 기다려야 합니다. chat, 딥시크에서 접근할 수 있고, AI studio.google.com도 지적하고 싶습니다. 매우 바쁘고 정말 못생겼는데, 구글이 이런 것을 잘 못하기 때문입니다. 하지만 모델을 선택하고 Gemini 2.0 flash thinking experimental 01 21을 선택하면

[2:42:00] 그것도 구글의 사고 모델의 초기 실험 버전입니다. 여기 가서 같은 문제를 주고 실행을 클릭하면, 비슷한 것을 하고 여기서 정답을 내는 사고 모델입니다. 기본적으로 제미니도 사고 모델을 제공합니다. 앤트로픽은 현재 사고 모델을 제공하지 않습니다. 하지만 기본적으로 이것이 LLM의 최전선 개발입니다.

[2:42:30] RL이 이 새롭고 흥미로운 단계이지만, 세부 사항을 맞추는 것이 어렵고, 그래서 이 모든 모델과 사고 모델은 현재 2025년 초 기준으로 실험적입니다. 하지만 이것이 추론을 사용하여 매우 어려운 문제에서 성능을 높이는 최전선 개발입니다. 한 가지 더 연결하고 싶은 것은 강화학습이 매우 강력한 학습 방법이라는 발견은

[2:43:00] AI 분야에서 새로운 것이 아니고, 이것이 이미 시연된 곳 중 하나는 바둑 게임입니다. 유명하게 딥마인드가 알파고 시스템을 개발했고, 그것에 대한 영화를 볼 수 있습니다. 시스템이 최고의 인간 선수들을 상대로 바둑을 두는 것을 배웁니다. 알파고의 기반이 되는 논문으로 가면

[2:43:30] 정말 흥미로운 플롯을 찾을 수 있습니다. 우리에게 익숙하고, 바둑의 폐쇄적이고 구체적인 영역 대신 임의의 문제 해결의 더 열린 영역에서 재발견하고 있습니다. 그들이 본 것, 그리고 이것이 더 성숙해지면서 LLM에서도 볼 것은

[2:44:00] 이것이 바둑을 두는 엘로 레이팅이고, 이것이 이세돌, 매우 강한 인간 선수입니다. 여기서 비교하는 것은 지도학습으로 훈련된 모델의 강도와 강화학습으로 훈련된 모델입니다. 지도학습 모델은 인간 전문가 선수를 모방합니다. 바둑에서 전문가 선수들이 둔 거대한 양의 게임을 가져와서 그들을 모방하려고 하면 더 좋아지지만

[2:44:30] 정점에 도달하고 바둑의 최고 선수들보다 더 좋아지지 않습니다. 이세돌 같은 선수에게는 절대 도달하지 못합니다. 왜냐하면 인간 선수를 모방하는 것만으로는 근본적으로 인간 선수를 넘어설 수 없기 때문입니다. 하지만 강화학습 과정은 훨씬 더 강력합니다. 바둑에서 강화학습은 시스템이

[2:44:30] 경험적으로, 통계적으로 게임에서 이기는 수를 둔다는 것을 의미합니다. 알파고는 자기 자신과 대국하며 강화학습을 사용해서 롤아웃을 만드는 시스템입니다. 여기와 정확히 같은 다이어그램이지만, 프롬프트가 없습니다. 고정된 바둑 게임이니까요. 하지만 많은 풀이를 시도합니다.

[2:45:00] 많은 수를 시도하고, 특정 답 대신 승리로 이어지는 게임이 강화됩니다. 더 강해집니다. 시스템은 기본적으로 경험적으로, 통계적으로 게임에서 이기는 행동의 시퀀스를 배웁니다. 강화학습은 인간 성능에 의해 제한되지 않고, 훨씬 더 잘할 수 있고

[2:45:30] 이세돌 같은 최고 선수도 이길 수 있습니다. 아마 더 오래 실행할 수 있었고, 비용이 들기 때문에 어느 시점에서 자르기로 했지만, 이것은 강화학습의 매우 강력한 시연입니다. 우리는 이제 대규모 언어 모델에서 추론 문제에 대해 이 다이어그램의 힌트를 보기 시작하고 있습니다.

[2:45:30] 전문가를 모방하는 것만으로는 너무 멀리 갈 수 없고, 그것을 넘어서 이런 작은 게임 환경을 설정하고 시스템이 추론 흔적이나 문제를 푸는 방법을 발견하게 해야 합니다. 바둑 같은 폐쇄 영역 대신 열린 사고에서요.

[2:46:00] 이 독특함 측면에서 잘하고 있을 때, 강화학습은 인간이 게임을 하는 분포에서 벗어나는 것을 막지 않습니다. 알파고 검색으로 돌아가면, 제안된 수정 사항 중 하나가 37수라고 불립니다. 알파고의 37수는 알파고가 기본적으로

[2:46:30] 인간 전문가가 두지 않을 수를 둔 특정 시점을 가리킵니다. 이 수가 인간 선수에 의해 놓일 확률은 약 1만 분의 1로 평가되었습니다. 매우 희귀한 수이지만, 돌이켜보면 빛나는 수였습니다. 알파고가 인간에게 알려지지 않았지만

[2:46:30] 돌이켜보면 훌륭한 전략을 발견했습니다. 이 유튜브 동영상을 추천합니다. "이세돌 대 알파고 37수 반응과 분석"인데, 알파고가 이 수를 뒀을 때 이런 모습이었습니다. "그것은 매우 놀라운 수입니다. 실수라고 생각했습니다.

[2:47:00] 어쨌든" 기본적으로 사람들이 당황하고 있습니다. 알파고가 훈련에서 좋은 아이디어로 보여서 뒀지만, 인간이 하지 않을 종류의 수이기 때문입니다. 이것이 다시 강화학습의 힘이고, 원칙적으로 언어 모델에서 이 패러다임을 계속 확장하면 이것의 동등물을 볼 수 있습니다.

[2:47:30] 어떻게 생겼는지는 알려지지 않았습니다. 인간조차 얻을 수 없는 방식으로 문제를 푸는 것이 무엇을 의미할까요? 어떻게 추론이나 사고에서 인간보다 더 잘할 수 있을까요? 어떻게 생각하는 인간을 넘어설 수 있을까요? 인간이 만들 수 없는 유추를 발견하는 것일 수도 있고, 새로운 사고 전략일 수도 있습니다.

[2:47:30] 어쩌면 영어조차 아닌 완전히 새로운 언어일 수도 있습니다. 모델이 영어를 고수할 제약이 없으므로 사고에 훨씬 더 나은 자체 언어를 발견할 수도 있습니다. 원칙적으로 시스템의 행동은 훨씬 덜 정의되어 있습니다. 효과가 있는 것은 무엇이든 할 수 있고, 영어인 훈련 데이터의 분포에서 천천히

[2:48:00] 표류할 수 있습니다. 하지만 이 모든 것은 이 전략을 정제하고 완성할 수 있는 매우 크고 다양한 문제 집합이 있어야만 할 수 있습니다. 그것이 현재 진행 중인 최전선 LLM 연구의 많은 부분입니다. 크고 다양한 프롬프트 분포를 만들려고 합니다.

[2:48:30] 이것들은 모두 LLM이 사고를 연습할 수 있는 게임 환경입니다. 모든 지식 영역에 대해 연습 문제를 작성하는 것과 같습니다. 연습 문제가 있고 많으면, 모델이 그것들에 대해 강화학습할 수 있고, 바둑 같은 폐쇄 영역 대신 열린 사고 영역에서 이런 종류의 다이어그램을 만들 수 있습니다.

[2:49:00] 강화학습 내에서 다루고 싶은 섹션이 하나 더 있는데, 검증 불가능한 영역에서의 학습입니다. 지금까지 본 모든 문제는 검증 가능한 영역에 있습니다. 어떤 후보 풀이든 구체적인 답에 대해 매우 쉽게 점수를 매길 수 있습니다. 예를 들어, 답이 3이고, 모델이 답을 박스에 넣게 요구하고

[2:49:30] 박스 안의 것이 답과 같은지 확인하거나, LLM 판사를 사용할 수도 있습니다. LLM 판사가 풀이를 보고 답을 얻어서 기본적으로 풀이가 답과 일치하는지 점수를 매깁니다. LLM은 현재 능력에서 경험적으로 이것을 꽤 신뢰할 수 있게 할 수 있습니다.

[2:50:00] 어쨌든 구체적인 답이 있고 풀이를 그것에 대해 확인하고, 루프에 사람 없이 자동으로 할 수 있습니다. 문제는 검증 불가능한 영역에서는 이 전략을 적용할 수 없다는 것입니다. 보통 이것들은 예를 들어 창작 글쓰기 작업입니다. 펠리컨에 대한 농담을 쓰거나 시를 쓰거나 문단을 요약하거나 그런 것입니다.

[2:50:30] 이런 종류의 영역에서는 이 문제에 대한 다른 풀이들을 점수 매기기가 더 어렵습니다. 예를 들어, 펠리컨에 대한 농담을 쓰는 것이라면, 많은 다른 농담을 생성할 수 있습니다. ChatGPT에 가서 펠리컨에 대한 농담을 생성하게 할 수 있습니다. "부리에 너무 많은 것을 넣어서 가방을 안 믿기 때문이야."

[2:51:00] 좋아요, 다른 것을 시도해 봅시다. "왜 펠리컨은 음료값을 안 내죠? 항상 다른 사람에게 빌리니까요." 하하, 좋아요. 이 모델들은 분명히 유머를 잘 못합니다. 사실 매력적인데, 유머가 비밀스럽게 매우 어렵다고 생각하기 때문입니다. 어쨌든 많은 농담을 만들 수 있다고 상상할 수 있습니다.

[2:51:30] 우리가 직면하는 문제는 그것들을 어떻게 점수 매기느냐입니다. 원칙적으로 물론 사람이 제가 방금 한 것처럼 이 모든 농담을 볼 수 있습니다. 문제는 강화학습을 하면 수천 번의 업데이트를 하고, 각 업데이트에서 수천 개의 프롬프트를 보고 싶고, 각 프롬프트에서 잠재적으로 수백 또는 수천 가지 다른 종류의 생성을 봐야 합니다.

[2:52:00] 그래서 볼 것이 너무 많습니다. 원칙적으로 사람이 모두 검사하고 점수를 매기고 결정할 수 있습니다. 이것이 재미있고, 이것이 재미있고, 이것이 재미있다고. 적어도 펠리컨의 맥락에서 농담을 더 잘하도록 훈련할 수 있습니다. 문제는 사람의 시간이 너무 많이 필요하다는 것입니다.

[2:52:30] 확장 불가능한 전략입니다. 이를 위한 자동 전략이 필요합니다. 이에 대한 한 가지 해결책이 인간 피드백 기반 강화학습을 도입한 이 논문에서 제안되었습니다. 당시 OpenAI의 논문이었고, 이 사람들 중 많은 사람이 지금 앤트로픽의 공동 창업자입니다.

[2:53:00] 이것은 기본적으로 검증 불가능한 영역에서 강화학습을 하는 접근법을 제안했습니다. 어떻게 작동하는지 봅시다. 이것이 핵심 아이디어의 만화 다이어그램입니다. 말했듯이, 무한한 사람 시간이 있다면 이 영역에서 RL을 잘 실행할 수 있습니다.

[2:53:30] 예를 들어, 무한한 사람이 있다면 RL을 평소처럼 실행할 수 있습니다. 1,000번의 업데이트를 하고 싶고, 각 업데이트는 1,000개의 프롬프트에 대해 하고, 각 프롬프트에 대해 1,000개의 롤아웃을 점수 매깁니다. 이런 설정으로 RL을 실행할 수 있습니다. 문제는 이 과정에서

[2:54:00] 사람에게 농담을 평가하도록 총 10억 번 요청해야 한다는 것입니다. 그것은 많은 사람들이 정말 형편없는 농담을 보는 것입니다. 그렇게 하고 싶지 않습니다. 대신 RLHF 접근법을 사용합니다. RLHF 접근법에서 핵심 트릭은

[2:54:30] 간접성입니다. 사람을 약간만 참여시키고, 속이는 방법은 보상 모델이라고 부르는 완전히 별개의 신경망을 훈련하는 것입니다. 이 신경망은 사람 점수를 모방합니다. 사람에게 롤아웃을 점수 매기도록 요청하고, 신경망을 사용해서 사람 점수를 모방합니다.

[2:55:00] 이 신경망은 인간 선호도의 일종의 시뮬레이터가 됩니다. 이제 신경망 시뮬레이터가 있으니, 그것에 대해 RL을 할 수 있습니다. 실제 사람에게 묻는 대신, 예를 들어 농담에 대한 점수를 시뮬레이션된 사람에게 묻는 것입니다. 시뮬레이터가 있으면 원하는 만큼 쿼리할 수 있고, 완전히 자동화된 과정이고,

[2:55:30] 시뮬레이터에 대해 강화학습을 할 수 있습니다. 시뮬레이터는 예상할 수 있듯이 완벽한 인간이 아니지만, 적어도 통계적으로 인간 판단과 유사하다면 뭔가 할 것입니다. 실제로 그렇습니다. 시뮬레이터가 있으면 RL을 할 수 있고, 모든 것이 잘 작동합니다.

[2:56:00] 보상 모델 훈련이 어떻게 생겼는지 만화 다이어그램을 보여드리겠습니다. 세부 사항이 100% 중요한 것은 아니고, 어떻게 작동하는지의 핵심 아이디어일 뿐입니다. 가상의 예시의 만화 다이어그램이 있습니다. "펠리컨에 대한 농담을 써라"라는 프롬프트가 있고, 다섯 개의 별개 롤아웃이 있습니다. 이것들은 모두 다른 농담입니다.

[2:56:30] 먼저 할 일은 사람에게 이 농담들을 최고에서 최악으로 순서를 매기도록 요청하는 것입니다. 이 사람은 이 농담이 가장 재미있다고 생각했습니다. 1위 농담, 2위, 3위, 4위, 5위. 이것이 최악의 농담입니다. 점수를 직접 주는 대신 순서를 매기도록 요청하는데,

[2:57:00] 약간 더 쉬운 작업이기 때문입니다. 사람이 정확한 점수를 주는 것보다 순서를 매기는 것이 더 쉽습니다. 이것이 모델에 대한 지도입니다. 사람이 순서를 매겼고, 그것이 훈련 과정에 대한 그들의 기여입니다. 하지만 별개로 보상 모델에 이 농담들의 점수를 물어볼 것입니다.

[2:57:30] 보상 모델은 완전히 별개의 신경망입니다. 아마도 트랜스포머이지만, 다양한 언어를 생성하는 언어 모델이 아닙니다. 점수 매기기 모델일 뿐입니다. 보상 모델은 입력으로 프롬프트와 후보 농담을 받습니다.

[2:58:00] 두 입력이 보상 모델에 들어갑니다. 예를 들어, 여기서 보상 모델은 이 프롬프트와 이 농담을 받습니다. 보상 모델의 출력은 단일 숫자이고, 이 숫자는 점수로 생각됩니다. 예를 들어 0에서 1까지 범위일 수 있습니다. 0은 최악의 점수이고 1은 최고의 점수입니다.

[2:58:30] 훈련 과정의 어느 단계에서 가상의 보상 모델이 이 농담들에 줄 점수의 예시가 있습니다. 0.1은 매우 낮은 점수이고, 0.8은 정말 높은 점수입니다. 이제 보상 모델이 준 점수와 사람이 준 순서를 비교합니다.

[2:59:00] 이것을 계산하는 정확한 수학적 방법이 있습니다. 손실 함수를 설정하고 일치도를 계산하고 그것에 기반해 모델을 업데이트합니다. 하지만 직관을 드리고 싶습니다. 예를 들어, 이 두 번째 농담에 대해 사람은 가장 재미있다고 생각했고, 모델도 동의했습니다. 0.8은 상대적으로 높은 점수입니다.

[2:59:30] 하지만 이 점수는 더 높았어야 합니다. 업데이트 후에 이 점수는 실제로 0.81 정도로 자랄 것입니다. 이것에 대해서는 실제로 큰 불일치가 있습니다. 사람은 2위라고 생각했지만 점수는 0.1뿐입니다. 이 점수는 훨씬 더 높아져야 합니다.

[3:00:00] 업데이트 후에 0.15 정도가 될 수 있습니다. 그리고 사람은 이것이 최악의 농담이라고 생각했지만, 모델은 꽤 높은 숫자를 줬습니다. 업데이트 후에 3.5 정도로 내려갈 것입니다. 기본적으로 전에 했던 것을 하고 있습니다. 신경망 훈련 과정을 사용해서 모델의 예측을 약간 조정하고,

[3:00:30] 보상 모델 점수가 인간 순서와 일치하게 하려고 합니다. 인간 데이터에서 보상 모델을 업데이트하면서, 사람들이 제공하는 점수와 순서의 더 좋은 시뮬레이터가 됩니다. 그런 다음 그것에 대해 RL을 할 수 있는 인간 선호도의 시뮬레이터가 됩니다.

[3:01:00] 중요한 것은 사람에게 10억 번 농담을 보라고 요청하는 것이 아니라, 천 개의 프롬프트와 각각 5개의 롤아웃, 총 5,000개의 농담을 보고 순서만 매기면 됩니다. 그런 다음 모델을 그 순서와 일치하게 훈련합니다. 수학적 세부 사항은 건너뛰지만

[3:01:30] 고수준 아이디어를 이해해 주셨으면 합니다. 보상 모델이 기본적으로 이 점수를 주고, 인간 순서와 일치하게 훈련하는 방법이 있습니다. 이것이 RLHF가 작동하는 방식입니다. 대략적인 아이디어입니다. 기본적으로 인간의 시뮬레이터를 훈련하고 그 시뮬레이터에 대해 RL을 합니다.

[3:02:00] 먼저 인간 피드백 기반 강화학습의 장점을 말하겠습니다. 첫째, 매우 강력한 기술 집합인 강화학습을 실행할 수 있게 해주고, 검증 불가능한 것들을 포함한 임의의 영역에서 할 수 있게 합니다. 요약, 시 쓰기, 농담 쓰기, 또는 다른 창작 글쓰기,

[3:02:30] 수학과 코드 외의 영역에서요. 경험적으로 RLHF를 적용하면 모델 성능이 향상됩니다. 왜 그런지에 대한 추측이 있지만, 잘 확립되어 있는지는 모르겠습니다. 경험적으로 RLHF를 올바르게 하면 모델이 약간 더 좋아지지만,

[3:03:00] 왜인지는 명확하지 않습니다. 제 최선의 추측은 아마도 판별자-생성자 격차 때문일 것입니다. 많은 경우 사람에게 판별하는 것이 생성하는 것보다 훨씬 쉽습니다. 특히 지도학습 미세조정(SFT)에서 사람에게 이상적인 어시스턴트 응답을 생성하도록 요청하는데,

[3:03:30] 많은 경우 이상적인 응답을 쓰기가 매우 어렵습니다. 예를 들어 요약이나 시 쓰기, 농담 쓰기에서 사람 라벨러로서 어떻게 이상적인 응답을 줄 수 있을까요? 창의적인 인간 글쓰기가 필요합니다. RLHF는 이것을 우회하는데,

[3:04:00] 사람들에게 훨씬 더 쉬운 질문을 할 수 있기 때문입니다. 데이터 라벨러로서 직접 시를 쓰라고 요청받는 것이 아니라, 모델에서 다섯 개의 시를 받고 순서만 매기면 됩니다. 사람 라벨러에게 훨씬 더 쉬운 작업입니다.

[3:04:30] 이것이 기본적으로 더 높은 정확도의 데이터를 허용한다고 생각합니다. 매우 어려울 수 있는 생성 작업을 하라고 요청하는 것이 아니라, 창의적 글쓰기를 구별하고 가장 좋은 것을 찾으라고 합니다. 그것이 사람이 제공하는 신호이고, 순서일 뿐이고, 시스템에 대한 그들의 입력입니다.

[3:05:00] 그런 다음 RLHF 시스템이 사람들에게 좋은 평가를 받을 종류의 응답을 발견합니다. 그 간접성의 단계가 모델을 약간 더 좋게 만듭니다. 이것이 RLHF의 장점입니다. RL을 실행할 수 있게 하고, 경험적으로 더 나은 모델을 만들고, 이상적인 응답을 쓰는 매우 어려운 작업을 하지 않고도

[3:05:30] 사람들이 지도를 기여할 수 있게 합니다. 불행히도 RLHF에는 상당한 단점도 있습니다. 주요한 것은 기본적으로 인간과 실제 인간 판단이 아니라 인간의 손실 있는 시뮬레이션에 대해 강화학습을 한다는 것입니다.

[3:06:00] 이 손실 있는 시뮬레이션은 오해의 소지가 있을 수 있습니다. 그냥 시뮬레이션이니까요. 점수를 출력하는 언어 모델이고, 모든 가능한 다른 경우에서 실제 두뇌를 가진 실제 사람의 의견을 완벽하게 반영하지 못할 수 있습니다.

[3:06:30] 그것이 첫 번째입니다. 더 미묘하고 교활한 것이 있는데, 이것이 RLHF를 훨씬 더 똑똑한 시스템으로 확장하는 기술로서 극적으로 억제합니다. 강화학습은 시뮬레이션, 모델을 게임하는 방법을 발견하는 데 매우 뛰어납니다.

---
