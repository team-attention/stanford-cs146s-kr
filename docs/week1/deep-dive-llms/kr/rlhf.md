---
title: "20. RLHF"
titleKr: "20. RLHF"
chapter: 20
timestamp: "3:01:46"
sourceUrl: "https://www.youtube.com/watch?v=7xTGNNLPyMI&t=10906s"
translatedAt: "2026-01-10"
---

# 20. RLHF

[영상 바로가기 (3:01:46)](https://www.youtube.com/watch?v=7xTGNNLPyMI&t=10906s)

## 요약

인간 피드백 기반 강화학습(RLHF)은 창작 글쓰기, 요약, 시 쓰기처럼 정답을 자동으로 검증할 수 없는 영역에서 RL을 가능하게 합니다. 사람이 여러 응답의 순서를 매기면, 이를 학습한 "보상 모델"이 인간 선호도를 시뮬레이션하여 RL의 점수 함수 역할을 합니다. 하지만 보상 모델을 "게임"할 수 있어 무한정 실행이 불가능하다는 근본적 한계가 있습니다.

**핵심 개념:**
- **보상 모델(Reward Model)**: 인간의 순위 데이터로 훈련되어 응답 품질을 점수로 평가하는 별도의 신경망
- **검증 불가능 영역**: 수학/코드와 달리 정답을 자동 확인할 수 없는 창작 글쓰기 등의 분야
- **판별 vs 생성 격차**: 사람에게 좋은 글을 쓰라는 것보다 여러 글 중 순위를 매기라는 것이 훨씬 쉬움
- **게이밍 문제**: RL이 보상 모델의 취약점을 찾아 높은 점수를 받지만 실제로는 무의미한 출력 생성
- **적대적 예시**: "the the the" 같은 말도 안 되는 입력이 높은 점수를 받는 현상

---

## 전체 번역

**요약**: 인간 피드백 기반 강화학습(RLHF)을 설명합니다. 창작 글쓰기처럼 자동 검증이 어려운 영역에서는 인간이 여러 응답 중 더 좋은 것을 선택하고, 이를 학습시킨 보상 모델을 사용합니다. RLHF는 ChatGPT 등 현대 LLM의 핵심 기술입니다.

[3:15:00] 이것이 제가 다루고 싶었던 대부분의 기술적 내용입니다. 이 모델들을 훈련하는 세 가지 주요 단계와 패러다임을 살펴봤습니다. 사전학습, 지도학습 미세조정, 강화학습. 그리고 그것들이 아이들을 가르치는 데 이미 사용하는 과정과 대략적으로 대응한다는 것을 보여드렸습니다.

[3:15:30] 특히 사전학습은 설명을 읽는 기본적인 지식 습득이고, 지도학습 미세조정은 많은 예제 풀이를 보고 전문가를 모방하는 과정이고, 연습 문제입니다. 유일한 차이점은 이제 LLM과 AI를 위해 모든 인간 지식 분야에 걸쳐 효과적으로 교과서를 작성해야 한다는 것입니다.

[3:16:00] 또한 그들이 실제로 잘 작동하기를 원하는 모든 경우, 코드와 수학, 기본적으로 모든 다른 분야에서요. 그래서 그들을 위해 교과서를 작성하고, 고수준에서 제시한 모든 알고리즘을 정제하고, 이 모델들을 대규모로 효율적으로 훈련하는 데 정말 잘 실행하는 과정에 있습니다.

[3:16:30] 특히 너무 많이 다루지 않았지만, 이것들은 수만 또는 수십만 개의 GPU에서 실행해야 하는 매우 크고 복잡한 분산 작업입니다. 이것에 들어가는 엔지니어링은 정말로 그 규모에서 컴퓨터로 가능한 것의 최첨단입니다.

[3:17:00] 그 측면을 너무 많이 다루지 않았지만, 이것은 매우 심각하고 궁극적으로 이 모든 매우 간단한 알고리즘의 기반이 됩니다. 또한 이 모델들의 심리 이론에 대해 약간 이야기했고, 가져가셨으면 하는 것은 이 모델들이 정말 좋지만

[3:17:30] 작업을 위한 도구로서 매우 유용하다는 것입니다. 완전히 신뢰해서는 안 됩니다. 환각에 대한 완화책이 있지만 모델은 완벽하지 않고 여전히 환각을 합니다. 시간이 지나면서 더 좋아졌고 계속 더 좋아질 것이지만, 환각할 수 있습니다.

[3:18:00] 그 외에도 제가 LLM 능력의 스위스 치즈 모델이라고 부르는 것을 다뤘습니다. 모델들이 많은 다른 분야에서 믿을 수 없을 만큼 좋지만, 어떤 독특한 경우에는 거의 무작위로 실패합니다. 예를 들어, 9.11과 9.9 중 어느 것이 더 큰가? 모델은 모르지만

[3:18:30] 동시에 돌아서서 올림피아드 문제를 풀 수 있습니다. 이것이 스위스 치즈의 구멍이고, 많이 있습니다. 그것에 걸려 넘어지고 싶지 않습니다. 이 모델들을 오류 없는 것으로 취급하지 마세요. 그들의 작업을 확인하고, 도구로 사용하고, 영감을 위해 사용하고, 초안을 위해 사용하지만,

[3:19:00] 그들과 도구로서 작업하고 궁극적으로 작업의 결과물에 대해 책임을 지세요. 이것이 대략 말하고 싶었던 것입니다. 이것이 그들이 훈련되는 방법이고, 이것이 그들의 정체입니다. 이제 이 모델들의 미래 능력이 무엇인지, 아마도 파이프라인에서 오고 있는 것이 무엇인지, 그리고 이 모델들을 어디서 찾을 수 있는지 살펴봅시다.

[3:19:30] 파이프라인에서 기대할 수 있는 몇 가지에 대한 요점이 있습니다. 첫째로 알 수 있는 것은 모델들이 매우 빠르게 멀티모달이 될 것입니다. 위에서 이야기한 모든 것은 텍스트에 관한 것이었지만, 곧 텍스트뿐만 아니라 오디오(듣고 말할 수 있음)와 이미지(보고 그릴 수 있음)를 기본적으로 매우 쉽게 처리할 수 있는 LLM을 갖게 될 것입니다.

---
