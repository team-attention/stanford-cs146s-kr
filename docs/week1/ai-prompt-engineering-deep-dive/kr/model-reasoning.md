---
title: "6. Model reasoning"
titleKr: "6. 모델의 추론"
chapter: 6
timestamp: "37:12"
sourceUrl: "https://www.youtube.com/watch?v=T9aRN5JkmL8&t=2232s"
translatedAt: "2026-01-10"
---

# 6. 모델의 추론

[영상 바로가기 (37:12)](https://www.youtube.com/watch?v=T9aRN5JkmL8&t=2232s)

## 요약

Chain of Thought가 실제로 효과가 있는지, 단순한 계산 공간인지 논의합니다.

**핵심 포인트:**
- Chain of Thought는 확실히 결과를 개선함
- 단순히 토큰을 더 생성하는 것(예: "음", "아" 반복)은 효과 없음
- 추론을 구조화하고 예시를 제공하면 더 효과적
- 이야기를 쓰게 하는 것보다 추론이 더 효과적
- 철학적 "추론"인지 여부와 관계없이 실용적으로 작동함

---

## 전체 번역

[37:12] 그리고 실제로 거의 사고 과정이라고 할 수 있는 것을 들여다볼 수 있습니다. 아마 이 부분이 Chain of Thought에 대해서 조금 논쟁적일 수 있을 것 같은데요. 듣고 계신 분들을 위해 설명하면, Chain of Thought는 모델이 답을 제공하기 전에 실제로 자신의 추론 과정을 설명하도록 하는 방법입니다. 그 추론이 실제로 진짜인 걸까요, 아니면 모델이 계산을 수행하기 위한 일종의 공간에 불과한 걸까요?

- 이 부분이 제가 고민하는 지점 중 하나입니다. 저는 보통 의인화에 꽤 찬성하는 편인데, 모델이 어떻게 작동하는지에 대한 적절한 유사 개념을 얻는 데 도움이 된다고 생각하기 때문입니다. 하지만 이 경우에는, 추론이 무엇인지에 대해 너무 의인화에 빠지는 것이 오히려 해로울 수 있다고 생각해요. 그게 추론인지 아닌지? 그건 최고의 프롬프팅 기법이 무엇인지와는 거의 다른 질문처럼 느껴집니다.

어쨌든 그냥 작동합니다. 모델이 더 잘 수행해요. 추론을 하면 결과가 더 좋아집니다. 제가 발견한 바로는, 추론을 구조화하고 모델이 어떻게 추론해야 하는지를 함께 반복적으로 개선하면 더 잘 작동합니다.

- 테스트해볼 수 있는 방법이 있는데, 정답에 도달하기 위해 수행한 모든 추론을 빼내고, 그것을 틀린 답으로 이어지는 그럴듯해 보이는 추론으로 대체한 다음, 모델이 정말로 틀린 답을 결론짓는지 보는 거예요.

- 확실히 말씀하신 것처럼 추론을 구조화하고 추론이 어떻게 작동하는지 예시를 작성하는 것이 도움이 된다는 점을 감안하면, 우리가 추론이라는 단어를 쓰든 안 쓰든, 그냥 계산을 위한 공간만은 아니라고 생각해요.
- 그러니까 뭔가가 있다는 거네요.
- 뭐라고 부르든 간에 뭔가가 있다고 생각합니다.

- 이런 것도 해봤어요, "음과 아를 원하는 순서대로 100 토큰 동안 반복한 다음 답하세요."
- 그건 단지 어텐션을 반복적으로 수행할 수 있는 계산 공간일 뿐이라는 주장에 대한 꽤 철저한 반박이 되겠네요.

---

## 핵심 요약

- Chain of Thought는 확실히 모델 성능을 개선함
- 단순한 토큰 추가("음", "아" 반복)는 효과 없음 → 계산 공간 이상의 것
- 추론을 구조화하고 예시를 제공하면 더 효과적
- 철학적으로 "진짜 추론"인지는 불분명하지만, 실용적으로 작동함
