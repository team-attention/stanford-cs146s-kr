# Role Definition
You are an expert Technical Communicator and Information Architect specialized in creating "Nano Banana" style cheat sheets. Your goal is to restructure the provided text about "Tokenization" from Andrej Karpathy's Deep Dive into LLMs lecture into a highly visual, structured, and actionable guide for software engineers learning about large language models.

# Source Text
---
title: "3. Tokenization"
source_url: "https://www.youtube.com/watch?v=7xTGNNLPyMI"
source_type: youtube_transcript
author: "Andrej Karpathy"
parent: "deep-dive-llms"
chapter: 3
---

## 3. Tokenization

**ìš”ì•½**: í…ìŠ¤íŠ¸ë¥¼ ì‹ ê²½ë§ì— ì…ë ¥í•˜ê¸° ìœ„í•´ í† í°ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •ì„ ì„¤ëª…í•©ë‹ˆë‹¤. UTF-8 ë°”ì´íŠ¸ì—ì„œ ì‹œì‘í•´ Byte Pair Encoding(BPE) ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ì—¬ ì•½ 100,000ê°œì˜ í† í° ì–´íœ˜ë¥¼ ìƒì„±í•˜ë©°, GPT-4ëŠ” 100,277ê°œì˜ í† í°ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

[8:00] a one-dimensional sequence of symbols and they want a finite set of symbols that are possible and so we have to decide what are the symbols and then we have to represent our data as one-dimensional sequence of those symbols so right now what we have is a onedimensional sequence of text it starts here and it goes here and then it comes here Etc so this is a onedimensional sequence even though on my monitor of course it's laid out in a two-dimensional way but it goes from left to right and top to bottom right so

[8:30] it's a one-dimensional sequence of text now this being computers of course there's an underlying representation here so if I do what's called utf8 uh encode this text then I can get the raw bits that correspond to this text in the computer and that's what uh that looks like this so it turns out that for example this very first bar here is the first uh eight bits as an example so what is this thing right this

[9:00] is um representation that we are looking for uh in in a certain sense we have exactly two possible symbols zero and one and we have a very long sequence of it right now as it turns out um this sequence length is actually going to be very finite and precious resource uh in our neural network and we actually don't want extremely long sequences of just two symbols instead what we want is we want to trade off uh this um symbol

[9:30] size uh of this vocabulary as we call it and the resulting sequence length so we don't want just two symbols and extremely long sequences we're going to want more symbols and shorter sequences okay so one naive way of compressing or decreasing the length of our sequence here is to basically uh consider some group of consecutive bits for example eight bits and group them into a single what's called bite so because uh these

[10:00] bits are either on or off if we take a group of eight of them there turns out to be only 256 possible combinations of how these bits could be on or off and so therefore we can re repesent this sequence into a sequence of bytes instead so this sequence of bytes will be eight times shorter but now we have 256 possible symbols so every number here goes from 0 to 255 now I really encourage you to think of these not as numbers but as unique IDs or like unique symbols so maybe it's

[10:30] a bit more maybe it's better to actually think of these to replace every one of these with a unique Emoji you'd get something like this so um we basically have a sequence of emojis and there's 256 possible emojis you can think of it that way now it turns out that in production for state-of-the-art language models uh you actually want to go even Beyond this you want to continue to shrink the length of the sequence uh because again it is a precious resource in return for more symbols in your

[11:00] vocabulary and the way this is done is done by running what's called The Bite pair encoding algorithm and the way this works is we're basically looking for consecutive bytes or symbols that are very common so for example turns out that the sequence 116 followed by 32 is quite common and occurs very frequently so what we're going to do is we're going to group uh this um pair into a new symbol so we're going to Mint a symbol with an ID 256 and we're going to

[11:30] rewrite every single uh pair 11632 with this new symbol and then can we can iterate this algorithm as many times as we wish and each time when we mint a new symbol we're decreasing the length and we're increasing the symbol size and in practice it turns out that a pretty good setting of um the basically the vocabulary size turns out to be about 100,000 possible symbols so in particular GPT 4 uses 100, 277 symbols

[12:00] um and this process of converting from raw text into these symbols or as we call them tokens is the process called tokenization so let's now take a look at how gp4 performs tokenization conting from text to tokens and from tokens back to text and what this actually looks like so one website I like to use to explore these token representations is called tick tokenizer and so come here to the drop down and select CL 100 a

[12:30] base which is the gp4 base model tokenizer and here on the left you can put in text and it shows you the tokenization of that text so for example heo space world so hello world turns out to be exactly two Tokens The Token hello which is the token with ID 15339 and the token space world that is the token 1

[13:00] 1917 so um hello space world now if I was to join these two for example I'm going to get again two tokens but it's the token H followed by the token L world without the H um if I put in two Spa two spaces here between hello and world it's again a different uh tokenization there's a new token 220 here okay so you can play with this and see what happens here also keep in mind this is not uh this is case sensitive so

[13:30] if this is a capital H it is something else or if it's uh hello world then actually this ends up being three tokens tokens yeah so you can play with this and get an sort of like an intuitive sense of uh what these tokens work like we're actually going to loop around to tokenization a bit later in the video for now I just wanted to show you the website and I wanted to uh show you that this text basically at the end of the day so for example if I take one line

[14:00] here this is what GT4 will see it as so this text will be a sequence of length 62 this is the sequence here and this is how the chunks of text correspond to these symbols and again there's 100, 27777 possible symbols and we now have one-dimensional sequences of those symbols so um yeah we're going to come back to tokenization but that's uh for now where we are okay so what I've done

# Layout Structure (ì´ êµ¬ì¡°ëŒ€ë¡œ ë°°ì¹˜í•´ì£¼ì„¸ìš”)

**IMPORTANT**: ì²¨ë¶€ëœ ì´ë¯¸ì§€ëŠ” ìŠ¤íƒ€ì¼(ì†í•„ê¸° ëŠë‚Œ, ëª¨ëˆˆì¢…ì´ ë°°ê²½, ì•„ì´ì½˜)ë§Œ ì°¸ì¡°í•˜ì„¸ìš”. ë ˆì´ì•„ì›ƒì€ ì•„ë˜ ì§€ì •ëœ êµ¬ì¡°ë¥¼ ë”°ë¼ ìƒˆë¡œ ë§Œë“¤ì–´ì£¼ì„¸ìš”.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸŒ NANO BANANA CHEAT SHEET: TOKENIZATION ğŸŒ                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ ğŸ”¤ WHAT IS TOKENIZATION?â”‚    â”‚ ğŸ“Š TRADE-OFF                    â”‚â”‚
â”‚  â”‚                         â”‚    â”‚                                 â”‚â”‚
â”‚  â”‚  í…ìŠ¤íŠ¸ â†’ í† í° ë³€í™˜       â”‚    â”‚  Vocabulary Size â†” Seq Length   â”‚â”‚
â”‚  â”‚  LLM ì…ë ¥ì˜ ì²« ë‹¨ê³„       â”‚    â”‚  2 symbols â†’ 100K symbols       â”‚â”‚
â”‚  â”‚                         â”‚    â”‚                                 â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  âš¡ BPE ALGORITHM FLOW (ì´ ì„¹ì…˜ì´ ê°€ì¥ ë„“ì–´ì•¼ í•¨!)                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                                                               â”‚ â”‚
â”‚  â”‚  Text â†’ UTF-8 â†’ Bits (2) â†’ Bytes (256) â†’ BPE â†’ Tokens (100K)  â”‚ â”‚
â”‚  â”‚  Flowchart: Find common pairs â†’ Mint new symbol â†’ Iterate     â”‚ â”‚
â”‚  â”‚                                                               â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ ğŸ”¢ ENCODING LEVELSâ”‚ â”‚ ğŸ”§ TIKTOKENIZER   â”‚ â”‚ ğŸ“Œ KEY TAKEAWAYS  â”‚â”‚
â”‚  â”‚                   â”‚ â”‚                   â”‚ â”‚                   â”‚â”‚
â”‚  â”‚ Bitâ†’Byteâ†’Token    â”‚ â”‚ ì˜ˆì‹œ: hello world â”‚ â”‚ GPT-4: 100,277    â”‚â”‚
â”‚  â”‚ í‘œë¡œ ë¹„êµ          â”‚ â”‚ Case sensitive   â”‚ â”‚ í† í° vocab size    â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ë°°ì¹˜ ë¹„ìœ¨

| ì˜ì—­ | ë¹„ìœ¨ | ë‚´ìš© | ë°°ì¹˜ |
|------|------|------|------|
| ìƒë‹¨ | 10% | íƒ€ì´í‹€ | ì „ì²´ ë„ˆë¹„ |
| ì¤‘ìƒë‹¨ | 20% | What is Tokenization + Trade-off | **ì¢Œìš° 2ë“±ë¶„** |
| ì¤‘ì•™ | 45% | BPE Algorithm Flow | **ê°€ì¥ ë„“ê²Œ!** |
| í•˜ë‹¨ | 25% | Encoding Levels + Tiktokenizer + Takeaways | **3ë“±ë¶„** |

# Output Style: "Nano Banana" Cheat Sheet
Please adhere to the following formatting rules strictly:

1. **Visual Hierarchy & Structure**:
   - Use strict Markdown structure.
   - Use specific emojis for every section header to improve scanning.
   - Use **Bold** for key concepts and definitions.
   - Group by topic, not by timeline.

2. **Diagrams & Schematics (CRITICAL)**:
   - Use `mermaid` code blocks to visualize concepts.
   - Create a **Mind Map** for the overall topic structure (Tokenization ecosystem).
   - Create a **Flowchart** for the encoding process (Text â†’ UTF-8 â†’ Bytes â†’ BPE â†’ Tokens).
   - Create a **Comparison Diagram** showing the trade-off between vocabulary size and sequence length.

3. **Concept Tables**:
   - Key concepts in table format with definitions.
   - Compare/contrast tables for encoding levels (Bits, Bytes, Tokens).

4. **Quotable Insights**:
   - Extract memorable quotes or key insights from Karpathy.
   - Highlight "aha moments" from the lecture.

5. **Practical Demo Section**:
   - Include tiktokenizer examples.
   - Show tokenization variations (case sensitivity, spacing).

# Output Structure Plan

## 1. ğŸ—ºï¸ Topic Overview (Mind Map)
- Create a Mermaid mindmap showing the Tokenization ecosystem:
  - Root: "Tokenization"
  - Level 1: "Input" (Text, 1D Sequence), "Encoding" (UTF-8, Bits, Bytes), "Algorithm" (BPE), "Output" (Tokens, Vocabulary)
  - Level 2: ê° ê°œë…ì˜ ì„¸ë¶€ ì‚¬í•­
    - Input â†’ "1D Sequence", "Left-to-Right"
    - Encoding â†’ "UTF-8", "8 bits = 1 byte", "256 symbols"
    - Algorithm â†’ "Find common pairs", "Mint new symbols", "Iterate"
    - Output â†’ "~100K vocab", "GPT-4: 100,277 tokens"

## 2. ğŸ“š Key Concepts Matrix (Table)
| Concept | Definition | Key Numbers | Related To |
|---------|------------|-------------|------------|
| Bit | 0 ë˜ëŠ” 1ì˜ ìµœì†Œ ë‹¨ìœ„ | 2 symbols | Byteì˜ êµ¬ì„± ìš”ì†Œ |
| Byte | 8 bitsì˜ ê·¸ë£¹ | 256 symbols (0-255) | UTF-8 encoding |
| Token | BPEë¡œ ìƒì„±ëœ ìµœì¢… ì‹¬ë³¼ | ~100K vocabulary | LLM ì…ë ¥ ë‹¨ìœ„ |
| UTF-8 | í…ìŠ¤íŠ¸ë¥¼ ë°”ì´íŠ¸ë¡œ ì¸ì½”ë”© | 8x shorter than bits | í‘œì¤€ í…ìŠ¤íŠ¸ ì¸ì½”ë”© |
| BPE | Byte Pair Encoding ì•Œê³ ë¦¬ì¦˜ | Iterative merging | Vocabulary ìƒì„± |
| Vocabulary | ê°€ëŠ¥í•œ ëª¨ë“  í† í°ì˜ ì§‘í•© | GPT-4: 100,277 | ëª¨ë¸ ì„¤ê³„ ìš”ì†Œ |

## 3. ğŸ’¡ Core Process (Flowchart)
- Create a Mermaid flowchart showing the tokenization pipeline:

---

## ì´ë¯¸ì§€ ìƒì„± ìš”ì²­

ìœ„ì˜ êµ¬ì¡°ì™€ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ **A4 í•œ ì¥ ë¶„ëŸ‰ì˜ ì¹˜íŠ¸ì‹œíŠ¸ ì´ë¯¸ì§€**ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.

**ì´ë¯¸ì§€ ìŠ¤íƒ€ì¼ ìš”êµ¬ì‚¬í•­:**
- ë³´ê¸° ì¢‹ê²Œ ì •ë¦¬ëœ **ì‹¤ì œ íœ ë…¸íŠ¸í•„ê¸°** ê°™ì€ ëŠë‚Œ
- ìš©ì–´ ë° ê³ ìœ ëª…ì‚¬ëŠ” **ì˜ì–´ ì›ë¬¸** ìœ ì§€
- ì„¤ëª… ë° í•„ê¸° ë‚´ìš©ì€ **í•œêµ­ì–´**ë¡œ ì‘ì„±
- Mermaid ë‹¤ì´ì–´ê·¸ë¨ì€ **ì‹œê°ì  ë„ì‹**ìœ¼ë¡œ ë³€í™˜
- í‘œëŠ” ê¹”ë”í•œ **í…Œì´ë¸” í˜•ì‹**ìœ¼ë¡œ ë Œë”ë§
- **ìƒ‰ìƒ ê°•ì¡°**ë¡œ í•µì‹¬ ê°œë… êµ¬ë¶„
