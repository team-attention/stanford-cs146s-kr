# Role Definition

You are an expert Technical Communicator and Information Architect specialized in creating "Nano Banana" style cheat sheets.

**IMPORTANT**: ì²¨ë¶€ëœ ì´ë¯¸ì§€ëŠ” ìŠ¤íƒ€ì¼(ì†í•„ê¸° ëŠë‚Œ, ëª¨ëˆˆì¢…ì´ ë°°ê²½, ì•„ì´ì½˜)ë§Œ ì°¸ì¡°í•˜ì„¸ìš”. ë ˆì´ì•„ì›ƒì€ ì•„ë˜ ì§€ì •ëœ êµ¬ì¡°ë¥¼ ë”°ë¼ ìƒˆë¡œ ë§Œë“¤ì–´ì£¼ì„¸ìš”. ì´ë¯¸ì§€ì˜ ë ˆì´ì•„ì›ƒì„ ê·¸ëŒ€ë¡œ ë³µì‚¬í•˜ì§€ ë§ˆì„¸ìš”.

Your goal is to restructure the provided text about "Preview of Things to Come - LLMì˜ ë¯¸ë˜ ì „ë§" into a highly visual, structured, and actionable guide for software engineers and AI practitioners.

# Source Text

---
title: "21. Preview of Things to Come"
titleKr: "21. ì•ìœ¼ë¡œì˜ ì „ë§"
source_url: "https://www.youtube.com/watch?v=7xTGNNLPyMI"
source_type: youtube_transcript
author: "Andrej Karpathy"
parent: "deep-dive-llms"
chapter: 21
totalChapters: 24
---

# 21. ì•ìœ¼ë¡œì˜ ì „ë§

> ì›ë³¸ ê°•ì˜: "Deep Dive into LLMs like ChatGPT" by Andrej Karpathy
> ì±•í„° 21/24

## ì „ì²´ ê°•ì˜ ìš”ì•½ (TL;DR)

ì´ 3ì‹œê°„ 30ë¶„ì§œë¦¬ ê°•ì˜ì—ì„œ ì•ˆë“œë ˆì´ ì¹´ë¥´íŒŒí‹°ëŠ” ChatGPT ê°™ì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì´ ì–´ë–»ê²Œ ë§Œë“¤ì–´ì§€ê³  ì‘ë™í•˜ëŠ”ì§€ ì²˜ìŒë¶€í„° ëê¹Œì§€ ì„¤ëª…í•©ë‹ˆë‹¤. **ì‚¬ì „í•™ìŠµ**(ì¸í„°ë„· ë°ì´í„° í•™ìŠµ), **ì§€ë„í•™ìŠµ ë¯¸ì„¸ì¡°ì •**(ëŒ€í™” ë°ì´í„°ë¡œ ì–´ì‹œìŠ¤í„´íŠ¸ ë§Œë“¤ê¸°), **ê°•í™”í•™ìŠµ**(ì„±ëŠ¥ ìµœì í™”)ì˜ ì„¸ ë‹¨ê³„ë¥¼ ê±°ì³ LLMì´ íƒ„ìƒí•©ë‹ˆë‹¤. ëª¨ë¸ì€ ë†€ë¼ìš´ ëŠ¥ë ¥ì„ ë³´ì´ì§€ë§Œ í™˜ê°, í† í°í™” í•œê³„, ë“¤ì­‰ë‚ ì­‰í•œ ì§€ëŠ¥ ë“±ì˜ ì•½ì ë„ ìˆìŠµë‹ˆë‹¤. ChatGPTì™€ ëŒ€í™”í•  ë•Œ ë§ˆë²• ê°™ì€ AIê°€ ì•„ë‹ˆë¼ "OpenAI ë°ì´í„° ë¼ë²¨ëŸ¬ì˜ í†µê³„ì  ì‹œë®¬ë ˆì´ì…˜"ê³¼ ëŒ€í™”í•œë‹¤ê³  ìƒê°í•˜ë©´ ë” ì •í™•í•©ë‹ˆë‹¤.

## ì´ ê°•ì˜ì—ì„œ ë°°ìš¸ ìˆ˜ ìˆëŠ” ê²ƒ

- LLMì˜ 3ë‹¨ê³„ í•™ìŠµ íŒŒì´í”„ë¼ì¸ (ì‚¬ì „í•™ìŠµ â†’ SFT â†’ RL) ì´í•´
- í† í°í™”, ì‹ ê²½ë§, íŠ¸ëœìŠ¤í¬ë¨¸ì˜ í•µì‹¬ ê°œë…
- í™˜ê°(hallucination)ì˜ ì›ì¸ê³¼ ì™„í™” ë°©ë²•
- ê°•í™”í•™ìŠµì´ ëª¨ë¸ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ì›ë¦¬
- DeepSeek R1, AlphaGoì—ì„œ ë°°ìš°ëŠ” RLì˜ í˜
- LLMì˜ ì‹¬ë¦¬í•™: ë“¤ì­‰ë‚ ì­‰í•œ ì§€ëŠ¥ê³¼ í•œê³„
- ìµœì‹  LLM ë™í–¥ ì¶”ì  ë°©ë²•ê³¼ ë„êµ¬ í™œìš©ë²•

---

## ì´ ì±•í„° ìš”ì•½

LLMì˜ ë¯¸ë˜ ë°œì „ ë°©í–¥ì…ë‹ˆë‹¤.

**í•µì‹¬ í¬ì¸íŠ¸:**
- ë©€í‹°ëª¨ë‹¬: ì´ë¯¸ì§€, ì˜¤ë””ì˜¤, ë¹„ë””ì˜¤
- ì—ì´ì „íŠ¸: ì¥ì‹œê°„ ììœ¨ ì‘ì—…
- ì»´í“¨í„° ì‚¬ìš©: í‚¤ë³´ë“œ/ë§ˆìš°ìŠ¤ ì œì–´

---

## ì˜ì–´ ì›ë¬¸ íŠ¸ëœìŠ¤í¬ë¦½íŠ¸

## 21. Preview of Things to Come

**ìš”ì•½**: LLMì˜ ë¯¸ë˜ ë°œì „ ë°©í–¥ì„ ì „ë§í•©ë‹ˆë‹¤. ë©€í‹°ëª¨ë‹¬(ì´ë¯¸ì§€, ì˜¤ë””ì˜¤, ë¹„ë””ì˜¤), ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ, ë” ê¸´ ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš°, ë¡œë´‡ê³µí•™ í†µí•© ë“±ì´ í™œë°œíˆ ì—°êµ¬ë˜ê³  ìˆìŠµë‹ˆë‹¤. AI ì‹œìŠ¤í…œì´ ì ì  ë” ììœ¨ì ìœ¼ë¡œ ì‘ë™í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ë°œì „í•˜ê³  ìˆìŠµë‹ˆë‹¤.

[3:10:00] above concerned text but very soon we'll have llms that can not just handle text but they can also operate natively and very easily over audio so they can hear and speak and also images so they can see and paint and we're already seeing the beginnings of all of this uh but this will be all done natively inside inside the language model and this will enable kind of like natural conversations and roughly speaking the reason that this is actually no different from everything we've covered above is that as a baseline you can

[3:10:30] tokenize audio and images and apply the exact same approaches of everything that we've talked about above so it's not a fundamental change it's just uh it's just a to we have to add some tokens so as an example for tokenizing audio we can look at slices of the spectrogram of the audio signal and we can tokenize that and just add more tokens that suddenly represent audio and just add them into the context windows and train on them just like above the same for images we can use patches and we can separately tokenize patches and then

[3:11:00] what is an image an image is just a sequence of tokens and this actually kind of works and there's a lot of early work in this direction and so we can just create streams of tokens that are representing audio images as well as text and interpers them and handle them all simultaneously in a single model so that's one example of multimodality uh second something that people are very interested in is currently most of the work is that we're handing individual tasks to the models on kind of like a silver platter like please solve this task for me and the model sort of like does this little

[3:11:30] task but it's up to us to still sort of like organize a coherent execution of tasks to perform jobs and the models are not yet at the capability required to do this in a coherent error correcting way over long periods of time so they're not able to fully string together tasks to perform these longer running jobs but they're getting there and this is improving uh over time but uh probably what's going to happen here is we're going to start to see what's called

[3:12:00] agents which perform tasks over time and you you supervise them and you watch their work and they come up to once in a while report progress and so on so we're going to see more long running agents uh tasks that don't just take you know a few seconds of response but many tens of seconds or even minutes or hours over time uh but these uh models are not infallible as we talked about above so all of this will require supervision so for example in factories people talk about the human to robot ratio uh for automation I think we're going to see something similar in the digital space

[3:12:30] where we are going to be talking about human to agent ratios where humans becomes a lot more supervisors of agent tasks um in the digital domain uh next um I think everything is going to become a lot more pervasive and invisible so it's kind of like integrated into the tools and everywhere um and in addition kind of like computer using so right now these models aren't able to take actions on your behalf but I think this is a separate bullet point

[3:13:00] um if you saw chpt launch the operator then uh that's one early example of that where you can actually hand off control to the model to perform you know keyboard and mouse actions on your behalf so that's also something that that I think is very interesting the last point I have here is just a general comment that there's still a lot of research to potentially do in this domain main one example of that uh is something along the lines of test time training so remember that everything we've done above and that we talked about has two major stages there's first the training stage where we tune the

[3:13:30] parameters of the model to perform the tasks well once we get the parameters we fix them and then we deploy the model for inference from there the model is fixed it doesn't change anymore it doesn't learn from all the stuff that it's doing a test time it's a fixed um number of parameters and the only thing that is changing is now the token inside the context windows and so the only type of learning or test time learning that the model has access to is the in context learning of its uh kind of like uh dynamically adjustable context window

[3:14:00] depending on like what it's doing at test time so but I think this is still different from humans who actually are able to like actually learn uh depending on what they're doing especially when you sleep for example like your brain is updating your parameters or something like that right so there's no kind of equivalent of that currently in these models and tools so there's a lot of like um more wonky ideas I think that are to be explored still and uh in particular I think this will be necessary because the context window is a finite and precious resource and especially once we start to tackle very

[3:14:30] long running multimodal tasks and we're putting in videos and these token windows will basically start to grow extremely large like not thousands or even hundreds of thousands but significantly beyond that and the only trick uh the only kind of trick we have Avail to us right now is to make the context Windows longer but I think that that approach by itself will will not will not scale to actual long running tasks that are multimodal over time and so I think new ideas are needed in some of those disciplines um in some of those kind of cases in the main where these

[3:15:00] tasks are going to require very long contexts so those are some examples of some of the things you can um expect coming down the pipe let's now turn to where you can actually uh kind of keep track of this progress and um you know be up to date with the latest and grest of what's happening in the field so I would say the three resources that I have consistently used to stay up to date are number one El Marina uh so let me show you El Marina this is basically an llm leader

# Layout Structure (ì´ êµ¬ì¡°ëŒ€ë¡œ ë°°ì¹˜í•´ì£¼ì„¸ìš”)

**IMPORTANT**: ì²¨ë¶€ëœ ì´ë¯¸ì§€ëŠ” ìŠ¤íƒ€ì¼(ì†í•„ê¸° ëŠë‚Œ, ëª¨ëˆˆì¢…ì´ ë°°ê²½, ì•„ì´ì½˜)ë§Œ ì°¸ì¡°í•˜ì„¸ìš”. ë ˆì´ì•„ì›ƒì€ ì•„ë˜ ì§€ì •ëœ êµ¬ì¡°ë¥¼ ë”°ë¼ ìƒˆë¡œ ë§Œë“¤ì–´ì£¼ì„¸ìš”.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸŒ NANO BANANA CHEAT SHEET: PREVIEW OF THINGS TO COME ğŸŒ            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ ğŸ¨ MULTIMODAL           â”‚    â”‚ ğŸ¤– AGENTS                       â”‚â”‚
â”‚  â”‚                         â”‚    â”‚                                 â”‚â”‚
â”‚  â”‚  ì´ë¯¸ì§€, ì˜¤ë””ì˜¤, ë¹„ë””ì˜¤    â”‚    â”‚  ì¥ì‹œê°„ ììœ¨ ì‘ì—…                 â”‚â”‚
â”‚  â”‚  í† í°í™”í•´ì„œ í†µí•© ì²˜ë¦¬      â”‚    â”‚  Human-to-Agent Ratio           â”‚â”‚
â”‚  â”‚                         â”‚    â”‚                                 â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  âš¡ FUTURE LLM CAPABILITIES (ì´ ì„¹ì…˜ì´ ê°€ì¥ ë„“ì–´ì•¼ í•¨!)               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                                                               â”‚ â”‚
â”‚  â”‚  Mind Map: Future â†’ Multimodal, Agents, Computer Use,         â”‚ â”‚
â”‚  â”‚           Longer Context, Test-Time Training, Robotics        â”‚ â”‚
â”‚  â”‚                                                               â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ ğŸ–¥ï¸ COMPUTER USE   â”‚ â”‚ ğŸ”¬ RESEARCH       â”‚ â”‚ ğŸ“Œ KEY TAKEAWAYS  â”‚â”‚
â”‚  â”‚                   â”‚ â”‚                   â”‚ â”‚                   â”‚â”‚
â”‚  â”‚ í‚¤ë³´ë“œ/ë§ˆìš°ìŠ¤ ì œì–´  â”‚ â”‚ Test-Time Train  â”‚ â”‚ ë” ììœ¨ì ì¸ AI     â”‚â”‚
â”‚  â”‚ Operator ì˜ˆì‹œ     â”‚ â”‚ ë¬´í•œ ì»¨í…ìŠ¤íŠ¸ ë„ì „ â”‚ â”‚ ë©€í‹°ëª¨ë‹¬ í†µí•©      â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ë°°ì¹˜ ë¹„ìœ¨

| ì˜ì—­ | ë¹„ìœ¨ | ë‚´ìš© | ë°°ì¹˜ |
|------|------|------|------|
| ìƒë‹¨ | 10% | íƒ€ì´í‹€ | ì „ì²´ ë„ˆë¹„ |
| ì¤‘ìƒë‹¨ | 25% | Multimodal + Agents | **ì¢Œìš° 2ë“±ë¶„** |
| ì¤‘ì•™ | 40% | Future LLM Capabilities Mind Map | **ê°€ì¥ ë„“ê²Œ!** |
| í•˜ë‹¨ | 25% | Computer Use + Research + Takeaways | **3ë“±ë¶„** |

# Output Style: "Nano Banana" Cheat Sheet

**ìŠ¤íƒ€ì¼**: ì²¨ë¶€ ì´ë¯¸ì§€ ì°¸ì¡° (ì†í•„ê¸°, ëª¨ëˆˆì¢…ì´, ì•„ì´ì½˜)
**ë ˆì´ì•„ì›ƒ**: ìœ„ ASCII ë‹¤ì´ì–´ê·¸ë¨ êµ¬ì¡°ë¥¼ ë”°ë¼ ìƒˆë¡œ ë””ìì¸

Please generate the Cheat Sheet now.

---

## ì´ë¯¸ì§€ ìƒì„± ìš”ì²­

ìœ„ì˜ êµ¬ì¡°ì™€ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ **A4 í•œ ì¥ ë¶„ëŸ‰ì˜ ì¹˜íŠ¸ì‹œíŠ¸ ì´ë¯¸ì§€**ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.

**ì´ë¯¸ì§€ ìŠ¤íƒ€ì¼ ìš”êµ¬ì‚¬í•­:**
- ë³´ê¸° ì¢‹ê²Œ ì •ë¦¬ëœ **ì‹¤ì œ íœ ë…¸íŠ¸í•„ê¸°** ê°™ì€ ëŠë‚Œ
- ìš©ì–´ ë° ê³ ìœ ëª…ì‚¬ëŠ” **ì˜ì–´ ì›ë¬¸** ìœ ì§€
- ì„¤ëª… ë° í•„ê¸° ë‚´ìš©ì€ **í•œêµ­ì–´**ë¡œ ì‘ì„±
- Mermaid ë‹¤ì´ì–´ê·¸ë¨ì€ **ì‹œê°ì  ë„ì‹**ìœ¼ë¡œ ë³€í™˜
- í‘œëŠ” ê¹”ë”í•œ **í…Œì´ë¸” í˜•ì‹**ìœ¼ë¡œ ë Œë”ë§
- **ìƒ‰ìƒ ê°•ì¡°**ë¡œ í•µì‹¬ ê°œë… êµ¬ë¶„
